"""
EDU-Audit Streamlit ì±—ë´‡ ë°ëª¨
êµìˆ˜/ì—°êµ¬ìë¥¼ ìœ„í•œ ëŒ€í™”í˜• êµìœ¡ ì½˜í…ì¸  ê²€ìˆ˜ ì‹œìŠ¤í…œ
"""

import streamlit as st
import asyncio
import json
import os
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# í™˜ê²½ ì„¤ì •
from dotenv import load_dotenv
from pathlib import Path
env_path = Path(__file__).resolve().parents[1] / '.env.dev'
load_dotenv(env_path)

# ìš°ë¦¬ ì‹œìŠ¤í…œ import
import sys
sys.path.append('.')

from src.core.models import DocumentMeta, PageInfo, Issue, IssueType

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="EDU-Audit: êµìœ¡ ì½˜í…ì¸  ê²€ìˆ˜ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_document" not in st.session_state:
    st.session_state.current_document = None
if "analysis_results" not in st.session_state:
    st.session_state.analysis_results = {}
if "processing" not in st.session_state:
    st.session_state.processing = False

class EDUAuditChatbot:
    """EDU-Audit ì±—ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()
    
    async def analyze_document(self, file_path: str, analysis_type: str = "comprehensive") -> Dict[str, Any]:
        """ë¬¸ì„œ ë¶„ì„ ì‹¤í–‰"""
        try:
            # ê°„ë‹¨í•œ ë¶„ì„ (í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ ê¸°ëŠ¥ë§Œ ì‚¬ìš©)
            from src.core.models import DocumentMeta, PageInfo, Issue, IssueType, TextLocation, generate_doc_id, generate_issue_id
            import pdfplumber
            import re
            
            # 1. PDF íŒŒì‹±
            doc_id = generate_doc_id(file_path)
            pages_data = []
            
            with pdfplumber.open(file_path) as pdf:
                total_pages = len(pdf.pages)
                
                # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ placeholder
                progress_placeholder = st.empty()
                progress_bar = st.progress(0)
                
                for page_num, page in enumerate(pdf.pages, 1):
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    progress = page_num / total_pages
                    progress_bar.progress(progress)
                    progress_placeholder.text(f"í˜ì´ì§€ {page_num}/{total_pages} íŒŒì‹± ì¤‘...")
                    
                    text = page.extract_text() or ""
                    word_count = len(text.split()) if text else 0
                    
                    page_info = PageInfo(
                        page_id=f"p{page_num:03d}",
                        page_number=page_num,
                        raw_text=text,
                        word_count=word_count,
                        elements=[]
                    )
                    pages_data.append(page_info)
                    
                    # í˜ì´ì§€ê°€ ë§ì€ ê²½ìš° ì†ë„ ì¡°ì ˆ
                    if page_num % 5 == 0:
                        await asyncio.sleep(0.1)
                
                progress_placeholder.empty()
                progress_bar.empty()
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            doc_meta = DocumentMeta(
                doc_id=doc_id,
                title=Path(file_path).stem,
                doc_type="pdf",
                total_pages=len(pages_data),
                file_path=file_path
            )
            
            # 2. í’ˆì§ˆ ê²€ì‚¬ (ì˜¤íƒˆì íŒ¨í„´)
            st.write("ğŸ” í’ˆì§ˆ ê²€ì‚¬ ì§„í–‰ ì¤‘...")
            issues_found = await self._perform_quality_check(doc_meta, pages_data)
            
            # 3. ê°„ë‹¨í•œ í†µê³„ ìƒì„±
            total_words = sum(page.word_count for page in pages_data)
            
            results = {
                "document": {
                    "title": doc_meta.title,
                    "pages": len(pages_data),
                    "words": total_words,
                    "file_size": Path(file_path).stat().st_size / 1024 / 1024  # MB
                },
                "quality_issues": issues_found,
                "summary": {
                    "total_issues": len(issues_found),
                    "typo_issues": len([i for i in issues_found if i.issue_type == IssueType.TYPO]),
                    "pages_with_issues": len(set(i.page_id for i in issues_found))
                },
                "pages_data": pages_data,
                "doc_meta": doc_meta
            }
            
            return results
            
        except Exception as e:
            st.error(f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {}
    
    async def _perform_quality_check(self, doc_meta: DocumentMeta, pages_data: List[PageInfo]) -> List[Issue]:
        """í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰"""
        # í…ŒìŠ¤íŠ¸ì—ì„œ ê²€ì¦ëœ ì˜¤íƒˆì íŒ¨í„´ë“¤
        typo_patterns = [
            {"pattern": r"ì•Œê³ ë¦¬ë“¬", "correction": "ì•Œê³ ë¦¬ì¦˜", "description": "ì•Œê³ ë¦¬ë“¬ â†’ ì•Œê³ ë¦¬ì¦˜"},
            {"pattern": r"ë°ì´íƒ€", "correction": "ë°ì´í„°", "description": "ë°ì´íƒ€ â†’ ë°ì´í„°"},
            {"pattern": r"ì»´í“¨íƒ€", "correction": "ì»´í“¨í„°", "description": "ì»´í“¨íƒ€ â†’ ì»´í“¨í„°"},
            {"pattern": r"ì•¨ê³ ë¦¬ì¦˜", "correction": "ì•Œê³ ë¦¬ì¦˜", "description": "ì•¨ê³ ë¦¬ì¦˜ â†’ ì•Œê³ ë¦¬ì¦˜"},
            {"pattern": r"ë¨¸ì‹ ëŸ¬ë‹", "correction": "ë¨¸ì‹ ëŸ¬ë‹", "description": "ê¸°ê³„í•™ìŠµê³¼ í˜¼ìš© í™•ì¸"},
            {"pattern": r"ë”¥ëŸ¬ë‹", "correction": "ë”¥ëŸ¬ë‹", "description": "ì‹¬ì¸µí•™ìŠµê³¼ í˜¼ìš© í™•ì¸"},
        ]
        
        import re
        from src.core.models import TextLocation, generate_issue_id
        
        issues_found = []
        
        for page in pages_data:
            text = page.raw_text
            if not text:
                continue
            
            for pattern_info in typo_patterns:
                pattern = pattern_info["pattern"]
                matches = re.finditer(pattern, text, re.IGNORECASE)
                
                for match in matches:
                    location = TextLocation(start=match.start(), end=match.end())
                    
                    issue_id = generate_issue_id(
                        doc_meta.doc_id, page.page_id, location, IssueType.TYPO
                    )
                    
                    issue = Issue(
                        issue_id=issue_id,
                        doc_id=doc_meta.doc_id,
                        page_id=page.page_id,
                        issue_type=IssueType.TYPO,
                        text_location=location,
                        original_text=match.group(),
                        message=pattern_info["description"],
                        suggestion=pattern_info["correction"],
                        confidence=0.95,
                        confidence_level="high",
                        agent_name="quality_agent"
                    )
                    
                    issues_found.append(issue)
        
        return issues_found
    
    def format_chat_response(self, response_type: str, data: Any) -> str:
        """ì±—ë´‡ ì‘ë‹µ í¬ë§·íŒ…"""
        if response_type == "analysis_complete":
            doc_info = data["document"]
            summary = data["summary"]
            
            response = f"""
**ğŸ“‹ ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ!**

**ğŸ“„ ë¬¸ì„œ ì •ë³´:**
- ì œëª©: {doc_info['title']}
- í˜ì´ì§€ ìˆ˜: {doc_info['pages']}ê°œ
- ì´ ë‹¨ì–´ ìˆ˜: {doc_info['words']:,}ê°œ
- íŒŒì¼ í¬ê¸°: {doc_info['file_size']:.1f}MB

**ğŸ” í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:**
- ë°œê²¬ëœ ì´ìŠˆ: **{summary['total_issues']}ê°œ**
- ì˜¤íƒˆì ê´€ë ¨: {summary['typo_issues']}ê°œ
- ì´ìŠˆê°€ ìˆëŠ” í˜ì´ì§€: {summary['pages_with_issues']}ê°œ

ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ë‚´ìš©ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”!
ì˜ˆ: "3í˜ì´ì§€ì˜ ì˜¤íƒˆìë¥¼ ìì„¸íˆ ë³´ì—¬ì¤˜", "ê°€ì¥ ë§ì€ ì˜¤ë¥˜ê°€ ìˆëŠ” í˜ì´ì§€ëŠ” ì–´ë””ì•¼?"
"""
            return response
        
        elif response_type == "issue_details":
            issues = data
            if not issues:
                return "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” ì´ìŠˆê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            response = f"**ë°œê²¬ëœ ì´ìŠˆ ìƒì„¸ ({len(issues)}ê°œ):**\n\n"
            for i, issue in enumerate(issues, 1):
                response += f"{i}. **{issue.original_text}**\n"
                response += f"   - ìœ„ì¹˜: {issue.page_id}\n"
                response += f"   - ë¬¸ì œ: {issue.message}\n"
                response += f"   - ì œì•ˆ: {issue.suggestion}\n"
                response += f"   - ì‹ ë¢°ë„: {issue.confidence:.2f}\n\n"
            
            return response
        
        elif response_type == "page_analysis":
            page_num, issues, content_preview = data
            response = f"**ğŸ“„ í˜ì´ì§€ {page_num} ë¶„ì„ ê²°ê³¼:**\n\n"
            
            if issues:
                response += f"**ë°œê²¬ëœ ì´ìŠˆ ({len(issues)}ê°œ):**\n"
                for issue in issues:
                    response += f"- **{issue.original_text}**: {issue.message}\n"
                response += "\n"
            else:
                response += "ì´ í˜ì´ì§€ì—ì„œëŠ” ì´ìŠˆê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
            
            response += f"**ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:**\n{content_preview[:200]}..."
            return response
        
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
@st.cache_resource
def get_chatbot():
    return EDUAuditChatbot()

chatbot = get_chatbot()

def generate_simple_response(prompt: str, analysis_results: Dict) -> str:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
    prompt_lower = prompt.lower()
    
    # í˜ì´ì§€ë³„ ì§ˆë¬¸
    if "í˜ì´ì§€" in prompt and any(char.isdigit() for char in prompt):
        # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
        import re
        page_nums = re.findall(r'\d+', prompt)
        if page_nums:
            page_num = int(page_nums[0])
            
            # í•´ë‹¹ í˜ì´ì§€ ì´ìŠˆ ì°¾ê¸°
            page_issues = [
                issue for issue in analysis_results["quality_issues"]
                if issue.page_number == page_num
            ]
            
            # í•´ë‹¹ í˜ì´ì§€ ë‚´ìš© ì°¾ê¸°
            page_content = ""
            for page_data in analysis_results["pages_data"]:
                if page_data.page_number == page_num:
                    page_content = page_data.raw_text[:200]
                    break
            
            return chatbot.format_chat_response("page_analysis", (page_num, page_issues, page_content))
    
    # ì „ì²´ ì´ìŠˆ ìš”ì•½
    elif "ìš”ì•½" in prompt or "ì „ì²´" in prompt:
        return chatbot.format_chat_response("issue_details", analysis_results["quality_issues"][:5])
    
    # ì˜¤íƒˆì ê´€ë ¨ ì§ˆë¬¸
    elif "ì˜¤íƒˆì" in prompt or "ì˜¤ë¥˜" in prompt:
        typo_issues = [
            issue for issue in analysis_results["quality_issues"]
            if issue.issue_type == IssueType.TYPO
        ]
        return chatbot.format_chat_response("issue_details", typo_issues)
    
    # í†µê³„ ì§ˆë¬¸
    elif "í†µê³„" in prompt or "ì–¼ë§ˆë‚˜" in prompt:
        summary = analysis_results["summary"]
        return f"""
**ğŸ“Š ë¬¸ì„œ í†µê³„:**
- ì´ í˜ì´ì§€: {analysis_results['document']['pages']}ê°œ
- ì´ ë‹¨ì–´: {analysis_results['document']['words']:,}ê°œ
- ë°œê²¬ëœ ì´ìŠˆ: {summary['total_issues']}ê°œ
- ì´ìŠˆê°€ ìˆëŠ” í˜ì´ì§€: {summary['pages_with_issues']}ê°œ

í‰ê· ì ìœ¼ë¡œ í˜ì´ì§€ë‹¹ {summary['total_issues'] / analysis_results['document']['pages']:.1f}ê°œì˜ ì´ìŠˆê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.
"""
    
    # ê¸°ë³¸ ì‘ë‹µ
    else:
        return """
ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”:

- "3í˜ì´ì§€ì˜ ì˜¤íƒˆìë¥¼ ë³´ì—¬ì¤˜"
- "ì „ì²´ ì´ìŠˆë¥¼ ìš”ì•½í•´ì¤˜"  
- "í†µê³„ë¥¼ ì•Œë ¤ì¤˜"
- "ì˜¤íƒˆìê°€ ëª‡ ê°œì•¼?"

ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

# ë©”ì¸ UI
st.title("ğŸ“š EDU-Audit: êµìœ¡ ì½˜í…ì¸  ê²€ìˆ˜ ì‹œìŠ¤í…œ")
st.markdown("**AI ê¸°ë°˜ êµìœ¡ ìë£Œ í’ˆì§ˆ ê²€ì‚¬ ë° ì‚¬ì‹¤ ê²€ì¦ ë„êµ¬**")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="êµìœ¡ ìë£Œ PDF íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”"
    )
    
    if uploaded_file is not None:
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_dir = Path("temp_uploads")
        temp_dir.mkdir(exist_ok=True)
        
        temp_file_path = temp_dir / uploaded_file.name
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getvalue())
        
        st.success(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_file.name}")
        
        # ë¶„ì„ ì˜µì…˜
        st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
        
        analysis_type = st.selectbox(
            "ë¶„ì„ ìˆ˜ì¤€",
            ["ê¸°ë³¸ (ì˜¤íƒˆì ìœ„ì£¼)", "í‘œì¤€ (í’ˆì§ˆê²€ì‚¬ í¬í•¨)", "ì¢…í•© (ì‚¬ì‹¤ê²€ì¦ í¬í•¨)"],
            index=1
        )
        
        # ë¶„ì„ ì‹œì‘ ë²„íŠ¼
        if st.button("ğŸ” ë¶„ì„ ì‹œì‘", disabled=st.session_state.processing):
            if not st.session_state.processing:
                st.session_state.processing = True
                
                with st.spinner("ë¬¸ì„œ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
                    # ë¹„ë™ê¸° ë¶„ì„ ì‹¤í–‰
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    
                    results = loop.run_until_complete(
                        chatbot.analyze_document(str(temp_file_path), analysis_type)
                    )
                    
                    if results:
                        st.session_state.analysis_results = results
                        st.session_state.current_document = temp_file_path
                        
                        # ë¶„ì„ ì™„ë£Œ ë©”ì‹œì§€ ì¶”ê°€
                        response = chatbot.format_chat_response("analysis_complete", results)
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": response,
                            "timestamp": datetime.now()
                        })
                        
                        st.success("ë¶„ì„ ì™„ë£Œ!")
                    
                    loop.close()
                
                st.session_state.processing = False
                st.rerun()
    
    # í˜„ì¬ ë¬¸ì„œ ì •ë³´
    if st.session_state.analysis_results:
        st.header("ğŸ“Š í˜„ì¬ ë¬¸ì„œ")
        doc_info = st.session_state.analysis_results["document"]
        
        st.metric("í˜ì´ì§€ ìˆ˜", doc_info["pages"])
        st.metric("ì´ ë‹¨ì–´ ìˆ˜", f"{doc_info['words']:,}")
        st.metric("ë°œê²¬ëœ ì´ìŠˆ", st.session_state.analysis_results["summary"]["total_issues"])

# ë©”ì¸ ì±— ì¸í„°í˜ì´ìŠ¤
st.header("ğŸ’¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì™€ ëŒ€í™”")

# ì‚¬ìš© ì˜ˆì‹œ ì•ˆë‚´
if not st.session_state.messages and not st.session_state.analysis_results:
    st.info("""
    **ì‚¬ìš© ë°©ë²•:**
    1. ì™¼ìª½ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
    2. ë¶„ì„ ì™„ë£Œ í›„ ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”:
    
    - "3í˜ì´ì§€ì˜ ì˜¤íƒˆìë¥¼ ë³´ì—¬ì¤˜"
    - "ê°€ì¥ ë§ì€ ì˜¤ë¥˜ê°€ ìˆëŠ” í˜ì´ì§€ëŠ”?"
    - "ì „ì²´ ì´ìŠˆë¥¼ ìš”ì•½í•´ì¤˜"
    - "íŠ¹ì • ë‹¨ì–´ì˜ ì¼ê´€ì„±ì„ í™•ì¸í•´ì¤˜"
    """)

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ í‘œì‹œ
        st.caption(f"â° {message['timestamp'].strftime('%H:%M:%S')}")

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({
        "role": "user", 
        "content": prompt,
        "timestamp": datetime.now()
    })
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        if not st.session_state.analysis_results:
            response = "ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        else:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ë‹µ (ì‹¤ì œ LLM ëŒ€ì‹ )
            response = generate_simple_response(prompt, st.session_state.analysis_results)
        
        st.markdown(response)
        
        # AI ì‘ë‹µ ì €ì¥
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "timestamp": datetime.now()
        })

# í‘¸í„°
st.markdown("---")
st.markdown("**EDU-Audit v1.0** | êµìœ¡ ì½˜í…ì¸  í’ˆì§ˆ ê²€ìˆ˜ ì‹œìŠ¤í…œ")