"""
 DocumentAgent with Term/Symbol Dictionary
ìš©ì–´ ë° ì‹¬ë³¼ ë”•ì…”ë„ˆë¦¬ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ DocumentAgent
"""

import asyncio
import logging
import os
import base64
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
from io import BytesIO
from collections import defaultdict

# PDF/PPT íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬
import fitz  # PyMuPDF
from pptx import Presentation
from PIL import Image

# OpenAI API
import openai
from openai import AsyncOpenAI

# LlamaIndex ê´€ë ¨
from llama_index.core import Document, VectorStoreIndex
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI as LlamaOpenAI

# ê¸°ë³¸ ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©
from src.core.models import DocumentMeta, generate_doc_id

from dotenv import load_dotenv

env_path = Path(__file__).resolve().parents[2] / '.env.dev'
load_dotenv(env_path)

logger = logging.getLogger(__name__)

class TermDefinition:
    """ìš©ì–´ ì •ì˜ í´ë˜ìŠ¤"""
    def __init__(self, term: str, slide_id: str, context: str, definition_type: str = "explicit"):
        self.term = term
        self.slide_id = slide_id
        self.context = context
        self.definition_type = definition_type  # explicit, implicit, usage
        
    def to_dict(self) -> Dict[str, Any]:
        return {
            "term": self.term,
            "slide_id": self.slide_id,
            "context": self.context,
            "definition_type": self.definition_type
        }

class TermDictionary:
    """ìš©ì–´/ì‹¬ë³¼ ë”•ì…”ë„ˆë¦¬ í´ë˜ìŠ¤"""
    def __init__(self):
        self.symbols: Dict[str, List[TermDefinition]] = defaultdict(list)
        self.terms: Dict[str, List[TermDefinition]] = defaultdict(list)
        
    def add_symbol(self, symbol: str, slide_id: str, context: str, definition_type: str = "usage"):
        """ì‹¬ë³¼ ì¶”ê°€"""
        definition = TermDefinition(symbol, slide_id, context, definition_type)
        self.symbols[symbol].append(definition)
        
    def add_term(self, term: str, slide_id: str, context: str, definition_type: str = "usage"):
        """ìš©ì–´ ì¶”ê°€"""
        definition = TermDefinition(term, slide_id, context, definition_type)
        self.terms[term].append(definition)
    
    def get_symbol_history(self, symbol: str) -> List[TermDefinition]:
        """ì‹¬ë³¼ì˜ ì „ì²´ ë“±ì¥ ì´ë ¥"""
        return self.symbols.get(symbol, [])
    
    def get_term_history(self, term: str) -> List[TermDefinition]:
        """ìš©ì–´ì˜ ì „ì²´ ë“±ì¥ ì´ë ¥"""
        return self.terms.get(term, [])
    
    def get_last_definition(self, term_or_symbol: str) -> Optional[TermDefinition]:
        """ê°€ì¥ ìµœê·¼ ì •ì˜ ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¬ë³¼ì—ì„œ ë¨¼ì € ì°¾ê¸°
        if term_or_symbol in self.symbols:
            definitions = [d for d in self.symbols[term_or_symbol] if d.definition_type == "explicit"]
            if definitions:
                return definitions[-1]
        
        # ìš©ì–´ì—ì„œ ì°¾ê¸°
        if term_or_symbol in self.terms:
            definitions = [d for d in self.terms[term_or_symbol] if d.definition_type == "explicit"]
            if definitions:
                return definitions[-1]
        
        return None
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        return {
            "symbols": {k: [d.to_dict() for d in v] for k, v in self.symbols.items()},
            "terms": {k: [d.to_dict() for d in v] for k, v in self.terms.items()}
        }
    
    def get_context_for_slide(self, slide_id: str, terms_symbols: Set[str]) -> Dict[str, Any]:
        """íŠ¹ì • ìŠ¬ë¼ì´ë“œì˜ ìš©ì–´/ì‹¬ë³¼ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë°˜í™˜"""
        context = {}
        
        for item in terms_symbols:
            # ì‹¬ë³¼ ê²€ìƒ‰
            if item in self.symbols:
                relevant_defs = []
                for definition in self.symbols[item]:
                    if definition.slide_id != slide_id:  # í˜„ì¬ ìŠ¬ë¼ì´ë“œ ì œì™¸
                        relevant_defs.append({
                            "slide_id": definition.slide_id,
                            "context": definition.context,
                            "type": definition.definition_type
                        })
                if relevant_defs:
                    context[item] = {
                        "type": "symbol",
                        "definitions": relevant_defs
                    }
            
            # ìš©ì–´ ê²€ìƒ‰
            if item in self.terms:
                relevant_defs = []
                for definition in self.terms[item]:
                    if definition.slide_id != slide_id:  # í˜„ì¬ ìŠ¬ë¼ì´ë“œ ì œì™¸
                        relevant_defs.append({
                            "slide_id": definition.slide_id,
                            "context": definition.context,
                            "type": definition.definition_type
                        })
                if relevant_defs:
                    context[item] = {
                        "type": "term",
                        "definitions": relevant_defs
                    }
        
        return context

class DocumentAgent:
    """
    ìš©ì–´/ì‹¬ë³¼ ë”•ì…”ë„ˆë¦¬ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ DocumentAgent
    """
    
    def __init__(
        self, 
        openai_api_key: Optional[str] = None,
        vision_model: str = "gpt-5-nano",
        embedding_model: str = "text-embedding-3-small",
        image_quality: str = "high"
    ):
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        self.vision_model = vision_model
        self.embedding_model = embedding_model
        self.image_quality = image_quality
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.client = AsyncOpenAI(api_key=self.openai_api_key)
        
        # LlamaIndex ì»´í¬ë„ŒíŠ¸
        self.embeddings = OpenAIEmbedding(
            model=embedding_model,
            api_key=self.openai_api_key
        )
        
        # ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
        self.documents: Dict[str, DocumentMeta] = {}
        self.indexes: Dict[str, VectorStoreIndex] = {}
        self.slide_images: Dict[str, List[Dict[str, Any]]] = {}
        self.term_dictionaries: Dict[str, TermDictionary] = {}  # ìƒˆë¡œ ì¶”ê°€
        
        logger.info(f"DocumentAgent ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  Vision Model: {vision_model}")
        logger.info(f"  Embedding Model: {embedding_model}")
    
    async def process_document(self, file_path: str) -> DocumentMeta:
        """
        ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸ (ìš©ì–´ ë”•ì…”ë„ˆë¦¬ í¬í•¨)
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        logger.info(f" ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
        
        # 1. ë¬¸ì„œ ID ìƒì„±
        doc_id = generate_doc_id(str(file_path))
        
        # 2. ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ë³€í™˜
        slide_images = await self._convert_to_slide_images(file_path, doc_id)
        
        # 3. GPT ìº¡ì…˜ ìƒì„±
        captioned_slides = await self._generate_captions(slide_images)
        
        # 4. ìš©ì–´/ì‹¬ë³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶• (ìƒˆë¡œ ì¶”ê°€)
        term_dictionary = await self._build_term_dictionary(captioned_slides, doc_id)
        
        # 5. Document ê°ì²´ ìƒì„± ë° ì¸ë±ì‹±
        documents = await self._create_documents(captioned_slides, doc_id)
        
        # 6. ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        index = await self._build_vector_index(documents)
        
        # 7. ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±
        doc_meta = DocumentMeta(
            doc_id=doc_id,
            title=self._extract_title(file_path.stem, captioned_slides),
            doc_type=file_path.suffix.lower().replace('.', ''),
            total_pages=len(slide_images),
            file_path=str(file_path)
        )
        
        # 8. ë©”ëª¨ë¦¬ì— ì €ì¥
        self.documents[doc_id] = doc_meta
        self.indexes[doc_id] = index
        self.slide_images[doc_id] = captioned_slides
        self.term_dictionaries[doc_id] = term_dictionary  # ìƒˆë¡œ ì¶”ê°€
        
        logger.info(f" ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {doc_id}")
        logger.info(f"  ì´ {len(slide_images)} ìŠ¬ë¼ì´ë“œ ì²˜ë¦¬ë¨")
        logger.info(f"  ì‹¬ë³¼: {len(term_dictionary.symbols)}ê°œ, ìš©ì–´: {len(term_dictionary.terms)}ê°œ")
        
        return doc_meta
    
    async def _build_term_dictionary(self, captioned_slides: List[Dict[str, Any]], doc_id: str) -> TermDictionary:
        """ìš©ì–´/ì‹¬ë³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•"""
        logger.info(f"ìš©ì–´ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶• ì¤‘: {doc_id}")
        
        term_dict = TermDictionary()
        
        for slide_data in captioned_slides:
            try:
                logger.info(f"ìš©ì–´ ì¶”ì¶œ: {slide_data['page_id']}")
                
                # Vision LLMìœ¼ë¡œ ìš©ì–´/ì‹¬ë³¼ ì¶”ì¶œ
                extracted_items = await self._extract_terms_and_symbols(slide_data)
                
                # ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€
                for item in extracted_items:
                    if item["type"] == "symbol":
                        term_dict.add_symbol(
                            item["text"],
                            slide_data["page_id"],
                            item["context"],
                            item["definition_type"]
                        )
                    elif item["type"] == "term":
                        term_dict.add_term(
                            item["text"],
                            slide_data["page_id"],
                            item["context"],
                            item["definition_type"]
                        )
                
                # API ë ˆì´íŠ¸ ì œí•œ
                await asyncio.sleep(0.2)
                
            except Exception as e:
                logger.warning(f"ìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨ {slide_data['page_id']}: {str(e)}")
                continue
        
        logger.info(f"ìš©ì–´ ë”•ì…”ë„ˆë¦¬ êµ¬ì¶• ì™„ë£Œ: ì‹¬ë³¼ {len(term_dict.symbols)}ê°œ, ìš©ì–´ {len(term_dict.terms)}ê°œ")
        return term_dict
    
    async def _extract_terms_and_symbols(self, slide_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìŠ¬ë¼ì´ë“œì—ì„œ ìš©ì–´ì™€ ì‹¬ë³¼ ì¶”ì¶œ"""

        # ìº¡ì…˜ ì •ë³´ í¬í•¨
        context_info = ""
        if slide_data.get("caption"):
            context_info += f"\n\n[AI ìº¡ì…˜]\n{slide_data['caption']}"

        if slide_data.get("slide_text"):
            context_info += f"\n\n[ì›ë³¸ í…ìŠ¤íŠ¸]\n{slide_data['slide_text']}"

        extraction_prompt = f"""ì´ êµìœ¡ ìŠ¬ë¼ì´ë“œì—ì„œ ë‹¤ìŒì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

    **1. ìˆ˜í•™ ì‹¬ë³¼/ê¸°í˜¸ (Mathematical Symbols)**

    **2. í•µì‹¬ ìš©ì–´ (Key Terms)**

    **ì¶”ì¶œ ê·œì¹™:**
    - ê° í•­ëª©ì— ëŒ€í•´ ìŠ¬ë¼ì´ë“œì—ì„œ ì •ì˜ë˜ëŠ”ì§€ í™•ì¸
    - ì •ì˜ê°€ ëª…ì‹œì ì´ë©´ "explicit", ì•”ì‹œì ì´ë©´ "implicit", ë‹¨ìˆœ ì‚¬ìš©ì´ë©´ "usage"
    - í•´ë‹¹ í•­ëª©ì´ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ë§¥ ì œê³µ
    - ë§Œì•½, ë³„ ë‹¤ë¥¸ ìš©ì–´ê°€ ì—†ëŠ” ìŠ¬ë¼ì´ë“œë¼ë©´ **ë°˜ë“œì‹œ ë¹ˆ ë°°ì—´ []**ì„ ë°˜í™˜í•˜ì„¸ìš”.

    JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
    [
        {{
            "type": "symbol|term",
            "text": "ì¶”ì¶œëœ ì‹¬ë³¼/ìš©ì–´",
            "context": "í•´ë‹¹ í•­ëª©ì´ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ë§¥ (30ì ì´ë‚´)",
            "definition_type": "explicit|implicit|usage"
        }}
    ]

    ì°¸ê³  ì •ë³´:{context_info}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.vision_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": extraction_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{slide_data['image_base64']}",
                                    "detail": self.image_quality
                                }
                            }
                        ]
                    }
                ],
            )

            response_text = response.choices[0].message.content.strip()

            # JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
            if response_text.startswith("```json"):
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif response_text.startswith("```"):
                response_text = response_text.split("```")[1].split("```")[0].strip()

            # JSON íŒŒì‹±
            extracted_items = json.loads(response_text)

            # ëª¨ë¸ì´ ë¹ˆ ë°°ì—´ [] ì¤€ ê²½ìš° â†’ ì •ìƒ ì²˜ë¦¬
            if not extracted_items:
                logger.info(f"ìš©ì–´ ì—†ìŒ: {slide_data['page_id']}")
                return []

            # ìœ íš¨ì„± ê²€ì‚¬
            valid_items = []
            for item in extracted_items:
                if all(key in item for key in ["type", "text", "context", "definition_type"]):
                    if item["type"] in ["symbol", "term"] and item["text"].strip():
                        valid_items.append(item)

            return valid_items or []  # ìœ íš¨í•œ ê²Œ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜

        except json.JSONDecodeError as e:
            logger.warning(f"ìš©ì–´ ì¶”ì¶œ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:100]}... - {str(e)}")
            return []
        except Exception as e:
            logger.error(f"ìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨ {slide_data.get('page_id')}: {str(e)}")
            return []
    
    # ê¸°ì¡´ DocumentAgent ë©”ì„œë“œë“¤ (ë™ì¼)
    async def _convert_to_slide_images(self, file_path: Path, doc_id: str) -> List[Dict[str, Any]]:
        """íŒŒì¼ì„ ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        logger.info(f"ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘: {file_path}")
        
        slide_images = []
        
        if file_path.suffix.lower() == '.pdf':
            slide_images = await self._convert_pdf_to_images(file_path, doc_id)
        elif file_path.suffix.lower() in ['.ppt', '.pptx']:
            slide_images = await self._convert_ppt_to_images(file_path, doc_id)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}")
        
        logger.info(f"ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {len(slide_images)} ìŠ¬ë¼ì´ë“œ")
        return slide_images
    
    async def _convert_pdf_to_images(self, file_path: Path, doc_id: str) -> List[Dict[str, Any]]:
        """PDFë¥¼ í˜ì´ì§€ë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        slides = []
        
        try:
            pdf_doc = fitz.open(file_path)
            
            for page_num in range(len(pdf_doc)):
                page = pdf_doc[page_num]
                
                # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (300 DPI)
                mat = fitz.Matrix(300/72, 300/72)
                pix = page.get_pixmap(matrix=mat)
                
                # PIL Imageë¡œ ë³€í™˜
                img_data = pix.pil_tobytes(format="PNG")
                pil_image = Image.open(BytesIO(img_data))
                
                # Base64 ì¸ì½”ë”©
                buffered = BytesIO()
                pil_image.save(buffered, format="PNG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode()
                
                slide_data = {
                    "doc_id": doc_id,
                    "page_id": f"p{page_num + 1:03d}",
                    "page_number": page_num + 1,
                    "image_base64": img_base64,
                    "dimensions": pil_image.size,
                    "size_bytes": len(img_data),
                    "caption": None
                }
                
                slides.append(slide_data)
                pix = None
                
        except Exception as e:
            logger.error(f"PDF ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            if 'pdf_doc' in locals():
                pdf_doc.close()
        
        return slides
    
    async def _convert_ppt_to_images(self, file_path: Path, doc_id: str) -> List[Dict[str, Any]]:
        """PowerPointë¥¼ ìŠ¬ë¼ì´ë“œë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        slides = []
        
        try:
            prs = Presentation(file_path)
            
            for slide_num, slide in enumerate(prs.slides, 1):
                # ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                slide_text = ""
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text:
                        slide_text += shape.text + "\n"
                
                # ì„ì‹œ ì´ë¯¸ì§€ ìƒì„±
                dummy_image = Image.new('RGB', (1920, 1080), color='white')
                
                buffered = BytesIO()
                dummy_image.save(buffered, format="PNG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode()
                
                slide_data = {
                    "doc_id": doc_id,
                    "page_id": f"slide_{slide_num:03d}",
                    "page_number": slide_num,
                    "image_base64": img_base64,
                    "dimensions": (1920, 1080),
                    "size_bytes": len(buffered.getvalue()),
                    "slide_text": slide_text,
                    "caption": None
                }
                
                slides.append(slide_data)
        
        except Exception as e:
            logger.error(f"PPT ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            raise
        
        return slides
    
    async def _generate_captions(self, slide_images: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ GPT ìº¡ì…˜ ìƒì„±"""
        logger.info(f"GPT ìº¡ì…˜ ìƒì„± ì¤‘: {len(slide_images)} ìŠ¬ë¼ì´ë“œ")
        
        captioned_slides = []
        
        for i, slide_data in enumerate(slide_images):
            try:
                logger.info(f"ìº¡ì…˜ ìƒì„±: {slide_data['page_id']} ({i+1}/{len(slide_images)})")
                
                caption = await self._generate_single_caption(slide_data)
                
                slide_data_with_caption = slide_data.copy()
                slide_data_with_caption["caption"] = caption
                
                captioned_slides.append(slide_data_with_caption)
                
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.warning(f"ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨ {slide_data['page_id']}: {str(e)}")
                slide_data_with_caption = slide_data.copy()
                slide_data_with_caption["caption"] = f"ìŠ¬ë¼ì´ë“œ {slide_data['page_number']}"
                captioned_slides.append(slide_data_with_caption)
        
        logger.info(f"ìº¡ì…˜ ìƒì„± ì™„ë£Œ: {len(captioned_slides)} ìŠ¬ë¼ì´ë“œ")
        return captioned_slides
    
    async def _generate_single_caption(self, slide_data: Dict[str, Any]) -> str:
        """ë‹¨ì¼ ìŠ¬ë¼ì´ë“œì— ëŒ€í•œ GPT ìº¡ì…˜ ìƒì„±"""
        
        context = ""
        if "slide_text" in slide_data and slide_data["slide_text"].strip():
            context = f"\n\nìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸:\n{slide_data['slide_text']}"
        
        prompt = f"""ì´ êµìœ¡ìë£Œ ìŠ¬ë¼ì´ë“œë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”.

ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
- ìŠ¬ë¼ì´ë“œì˜ ì£¼ìš” ì£¼ì œ/ë‚´ìš©
- í•µì‹¬ ê°œë…ì´ë‚˜ ì •ë³´
- êµìœ¡ì  ëª©ì ì´ë‚˜ ì˜ë„

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.{context}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.vision_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{slide_data['image_base64']}",
                                    "detail": self.image_quality
                                }
                            }
                        ]
                    }
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            caption = response.choices[0].message.content.strip()
            return caption
            
        except Exception as e:
            logger.error(f"GPT ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ìŠ¬ë¼ì´ë“œ {slide_data['page_number']} - êµìœ¡ìë£Œ ë‚´ìš©"
    
    async def _create_documents(self, captioned_slides: List[Dict[str, Any]], doc_id: str) -> List[Document]:
        """ìº¡ì…˜ì´ ìˆëŠ” ìŠ¬ë¼ì´ë“œë“¤ì„ LlamaIndex Document ê°ì²´ë¡œ ë³€í™˜"""
        logger.info(f"Document ê°ì²´ ìƒì„± ì¤‘: {len(captioned_slides)} ìŠ¬ë¼ì´ë“œ")
        
        documents = []
        
        for slide_data in captioned_slides:
            text_content = slide_data["caption"]
            
            if "slide_text" in slide_data and slide_data["slide_text"].strip():
                text_content += f"\n\nì›ë³¸ í…ìŠ¤íŠ¸:\n{slide_data['slide_text']}"
            
            metadata = {
                "doc_id": doc_id,
                "page_id": slide_data["page_id"],
                "page_number": slide_data["page_number"],
                "image_path": None,
                "dimensions": slide_data["dimensions"],
                "size_bytes": slide_data["size_bytes"],
                "has_caption": bool(slide_data["caption"])
            }
            
            doc = Document(
                text=text_content,
                metadata=metadata
            )
            
            documents.append(doc)
        
        logger.info(f"Document ê°ì²´ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ")
        return documents
    
    async def _build_vector_index(self, documents: List[Document]) -> VectorStoreIndex:
        """Document ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        logger.info(f"ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘: {len(documents)} ë¬¸ì„œ")
        
        try:
            index = VectorStoreIndex.from_documents(
                documents,
                embed_model=self.embeddings
            )
            
            logger.info("ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            return index
            
        except Exception as e:
            logger.error(f"ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _extract_title(self, filename: str, slides: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œ ì œëª© ì¶”ì¶œ"""
        if slides and slides[0].get("caption"):
            first_caption = slides[0]["caption"]
            sentences = first_caption.split('.')
            if sentences and len(sentences[0]) < 100:
                return sentences[0].strip()
        
        return filename.replace('_', ' ').replace('-', ' ')
    
    # ê¸°ì¡´ ì¡°íšŒ ë©”ì„œë“œë“¤ + ìš©ì–´ ë”•ì…”ë„ˆë¦¬ ê´€ë ¨ ë©”ì„œë“œ ì¶”ê°€
    
    def get_document(self, doc_id: str) -> Optional[DocumentMeta]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        return self.documents.get(doc_id)
    
    def get_index(self, doc_id: str) -> Optional[VectorStoreIndex]:
        """ë¬¸ì„œì˜ ë²¡í„° ì¸ë±ìŠ¤ ì¡°íšŒ"""
        return self.indexes.get(doc_id)
    
    def get_slide_data(self, doc_id: str) -> List[Dict[str, Any]]:
        """ìŠ¬ë¼ì´ë“œ ì›ë³¸ ë°ì´í„° ì¡°íšŒ"""
        return self.slide_images.get(doc_id, [])
    
    def get_term_dictionary(self, doc_id: str) -> Optional[TermDictionary]:
        """ìš©ì–´ ë”•ì…”ë„ˆë¦¬ ì¡°íšŒ"""
        return self.term_dictionaries.get(doc_id)
    
    def get_slide_context(self, doc_id: str, page_id: str, terms_symbols: Set[str]) -> Dict[str, Any]:
        """íŠ¹ì • ìŠ¬ë¼ì´ë“œì˜ ìš©ì–´/ì‹¬ë³¼ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
        term_dict = self.get_term_dictionary(doc_id)
        if not term_dict:
            return {}
        
        return term_dict.get_context_for_slide(page_id, terms_symbols)
    
    def search_in_document(self, doc_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ë‚´ ì˜ë¯¸ì  ê²€ìƒ‰"""
        index = self.get_index(doc_id)
        if not index:
            return []
        
        try:
            query_engine = index.as_query_engine(similarity_top_k=top_k)
            response = query_engine.query(query)
            
            results = []
            for node in response.source_nodes:
                result = {
                    "text": node.text,
                    "score": getattr(node, 'score', 0.0),
                    "metadata": node.metadata,
                    "page_id": node.metadata.get("page_id"),
                    "page_number": node.metadata.get("page_number")
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_slide_by_page_id(self, doc_id: str, page_id: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ìŠ¬ë¼ì´ë“œ ë°ì´í„° ì¡°íšŒ"""
        slides = self.get_slide_data(doc_id)
        
        for slide in slides:
            if slide["page_id"] == page_id:
                return slide
        
        return None
    
    def list_documents(self) -> List[DocumentMeta]:
        """ëª¨ë“  ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        return list(self.documents.values())
    
    def get_document_stats(self, doc_id: str) -> Dict[str, Any]:
        """ë¬¸ì„œ í†µê³„ ì •ë³´ (ìš©ì–´ ë”•ì…”ë„ˆë¦¬ í¬í•¨)"""
        doc_meta = self.get_document(doc_id)
        slides = self.get_slide_data(doc_id)
        term_dict = self.get_term_dictionary(doc_id)
        
        if not doc_meta or not slides:
            return {}
        
        # ìº¡ì…˜ í†µê³„
        captioned_slides = sum(1 for slide in slides if slide.get("caption"))
        total_caption_chars = sum(len(slide.get("caption", "")) for slide in slides)
        
        # ì´ë¯¸ì§€ í†µê³„
        total_image_size = sum(slide.get("size_bytes", 0) for slide in slides)
        avg_dimensions = (
            sum(slide.get("dimensions", [0, 0])[0] for slide in slides) / len(slides),
            sum(slide.get("dimensions", [0, 0])[1] for slide in slides) / len(slides)
        ) if slides else (0, 0)
        
        # ìš©ì–´ ë”•ì…”ë„ˆë¦¬ í†µê³„
        term_stats = {}
        if term_dict:
            term_stats = {
                "total_symbols": len(term_dict.symbols),
                "total_terms": len(term_dict.terms),
                "explicit_definitions": sum(
                    len([d for d in defs if d.definition_type == "explicit"]) 
                    for defs in list(term_dict.symbols.values()) + list(term_dict.terms.values())
                ),
                "most_frequent_symbols": sorted(
                    term_dict.symbols.items(), 
                    key=lambda x: len(x[1]), 
                    reverse=True
                )[:5],
                "most_frequent_terms": sorted(
                    term_dict.terms.items(), 
                    key=lambda x: len(x[1]), 
                    reverse=True
                )[:5]
            }
        
        return {
            "doc_id": doc_id,
            "title": doc_meta.title,
            "doc_type": doc_meta.doc_type,
            "total_slides": len(slides),
            "captioned_slides": captioned_slides,
            "caption_coverage": captioned_slides / len(slides) if slides else 0,
            "total_caption_chars": total_caption_chars,
            "avg_caption_length": total_caption_chars / captioned_slides if captioned_slides else 0,
            "total_image_size_mb": total_image_size / (1024 * 1024),
            "avg_dimensions": avg_dimensions,
            "has_index": doc_id in self.indexes,
            "term_dictionary": term_stats
        }
    
    def export_term_dictionary(self, doc_id: str, output_path: Optional[str] = None) -> Optional[str]:
        """ìš©ì–´ ë”•ì…”ë„ˆë¦¬ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        term_dict = self.get_term_dictionary(doc_id)
        if not term_dict:
            return None
        
        if not output_path:
            output_path = f"{doc_id}_term_dictionary.json"
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(term_dict.to_dict(), f, ensure_ascii=False, indent=2)
            
            logger.info(f"ìš©ì–´ ë”•ì…”ë„ˆë¦¬ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"ìš©ì–´ ë”•ì…”ë„ˆë¦¬ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")
            return None


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_document_agent():
    """DocumentAgent í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª  DocumentAgent í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ì°¾ê¸°
    test_files = [
        "sample_docs/sample.pdf", 
    ]
    
    test_file = None
    for file_name in test_files:
        if Path(file_name).exists():
            test_file = file_name
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ë‹¤ìŒ íŒŒì¼ ì¤‘ í•˜ë‚˜ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”: {', '.join(test_files)}")
        return
    
    print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_file}")
    
    try:
        #  DocumentAgent ìƒì„±
        agent = DocumentAgent(
            openai_api_key=api_key,
            vision_model="gpt-5-nano",
            embedding_model="text-embedding-3-small"
        )
        
        print(f"ğŸ“–  ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {test_file}")
        doc_meta = await agent.process_document(test_file)
        
        print(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ë¬¸ì„œ ID: {doc_meta.doc_id}")
        print(f"   ì œëª©: {doc_meta.title}")
        print(f"   ìŠ¬ë¼ì´ë“œ ìˆ˜: {doc_meta.total_pages}")
        
        # ìš©ì–´ ë”•ì…”ë„ˆë¦¬ í™•ì¸
        term_dict = agent.get_term_dictionary(doc_meta.doc_id)
        if term_dict:
            print(f"\nğŸ“š ìš©ì–´ ë”•ì…”ë„ˆë¦¬:")
            print(f"   ì‹¬ë³¼: {len(term_dict.symbols)}ê°œ")
            print(f"   ìš©ì–´: {len(term_dict.terms)}ê°œ")
            
            # ìƒìœ„ 5ê°œ ì‹¬ë³¼ ì¶œë ¥
            symbol_counts = [(symbol, len(defs)) for symbol, defs in term_dict.symbols.items()]
            symbol_counts.sort(key=lambda x: x[1], reverse=True)
            
            if symbol_counts:
                print(f"   ì£¼ìš” ì‹¬ë³¼:")
                for symbol, count in symbol_counts[:5]:
                    print(f"     {symbol}: {count}íšŒ ë“±ì¥")
            
            # ìƒìœ„ 5ê°œ ìš©ì–´ ì¶œë ¥
            term_counts = [(term, len(defs)) for term, defs in term_dict.terms.items()]
            term_counts.sort(key=lambda x: x[1], reverse=True)
            
            if term_counts:
                print(f"   ì£¼ìš” ìš©ì–´:")
                for term, count in term_counts[:5]:
                    print(f"     {term}: {count}íšŒ ë“±ì¥")
        
        # ë¬¸ì„œ í†µê³„
        stats = agent.get_document_stats(doc_meta.doc_id)
        print(f"\nğŸ“Š ë¬¸ì„œ í†µê³„:")
        print(f"   ìº¡ì…˜ ìƒì„±ë¥ : {stats['caption_coverage']:.1%}")
        print(f"   í‰ê·  ìº¡ì…˜ ê¸¸ì´: {stats['avg_caption_length']:.0f}ì")
        print(f"   ì´ ì´ë¯¸ì§€ í¬ê¸°: {stats['total_image_size_mb']:.1f}MB")
        
        if 'term_dictionary' in stats and stats['term_dictionary']:
            td_stats = stats['term_dictionary']
            print(f"   ìš©ì–´ í†µê³„:")
            print(f"     ì´ ì‹¬ë³¼: {td_stats['total_symbols']}ê°œ")
            print(f"     ì´ ìš©ì–´: {td_stats['total_terms']}ê°œ") 
            print(f"     ëª…ì‹œì  ì •ì˜: {td_stats['explicit_definitions']}ê°œ")
        
        # ìŠ¬ë¼ì´ë“œë³„ ì»¨í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        slides = agent.get_slide_data(doc_meta.doc_id)
        if slides and len(slides) > 2:
            # 3ë²ˆì§¸ ìŠ¬ë¼ì´ë“œì—ì„œ ìš©ì–´ ì»¨í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
            test_slide = slides[2]
            test_terms = {"Î¸", "Î·", "learning", "gradient"}  # ì˜ˆì‹œ ìš©ì–´ë“¤
            
            context = agent.get_slide_context(doc_meta.doc_id, test_slide["page_id"], test_terms)
            if context:
                print(f"\nğŸ” {test_slide['page_id']} ì»¨í…ìŠ¤íŠ¸ ì˜ˆì‹œ:")
                for item, info in context.items():
                    print(f"   {item} ({info['type']}):")
                    for def_info in info['definitions'][:2]:  # ìµœëŒ€ 2ê°œë§Œ ì¶œë ¥
                        print(f"     - {def_info['slide_id']}: {def_info['context'][:50]}...")
        
        # ìš©ì–´ ë”•ì…”ë„ˆë¦¬ ë‚´ë³´ë‚´ê¸°
        export_path = agent.export_term_dictionary(doc_meta.doc_id)
        if export_path:
            print(f"\nğŸ’¾ ìš©ì–´ ë”•ì…”ë„ˆë¦¬ ë‚´ë³´ë‚´ê¸°: {export_path}")
        
        print("\nğŸ‰  DocumentAgent í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_document_agent())