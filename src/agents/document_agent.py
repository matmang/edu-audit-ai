"""
EDU-Audit Document Agent - Simplified
ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ â†’ GPT ìº¡ì…˜ â†’ ì„ë² ë”© â†’ ë¼ë§ˆì¸ë±ìŠ¤ íŒŒì´í”„ë¼ì¸
"""

import asyncio
import logging
import os
import base64
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
from io import BytesIO
from collections import defaultdict

# PDF/PPT íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬
import fitz  # PyMuPDF
from pptx import Presentation
from PIL import Image

# OpenAI API
import openai
from openai import AsyncOpenAI

# LlamaIndex ê´€ë ¨
from llama_index.core import Document, VectorStoreIndex
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI as LlamaOpenAI

# ê¸°ë³¸ ëª¨ë¸ë“¤ë§Œ ì‚¬ìš©
from src.core.models import DocumentMeta, generate_doc_id

from dotenv import load_dotenv

env_path = Path(__file__).resolve().parents[2] / '.env.dev'
load_dotenv(env_path)

logger = logging.getLogger(__name__)

class DocumentAgent:
    """
    ìš©ì–´/ì‹¬ë³¼ ë”•ì…”ë„ˆë¦¬ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ DocumentAgent
    """
    
    def __init__(
        self, 
        openai_api_key: Optional[str] = None,
        vision_model: str = "gpt-5-nano",
        embedding_model: str = "text-embedding-3-small",
        image_quality: str = "high"
    ):
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        self.vision_model = vision_model
        self.embedding_model = embedding_model
        self.image_quality = image_quality
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.client = AsyncOpenAI(api_key=self.openai_api_key)
        
        # LlamaIndex ì»´í¬ë„ŒíŠ¸
        self.embeddings = OpenAIEmbedding(
            model=embedding_model,
            api_key=self.openai_api_key
        )
        
        # ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
        self.documents: Dict[str, DocumentMeta] = {}
        self.indexes: Dict[str, VectorStoreIndex] = {}
        self.slide_images: Dict[str, List[Dict[str, Any]]] = {}
        
        logger.info(f"DocumentAgent ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  Vision Model: {vision_model}")
        logger.info(f"  Embedding Model: {embedding_model}")
    
    async def process_document(self, file_path: str) -> DocumentMeta:
        """
        ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸ (ìš©ì–´ ë”•ì…”ë„ˆë¦¬ í¬í•¨)
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        logger.info(f" ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
        
        # 1. ë¬¸ì„œ ID ìƒì„±
        doc_id = generate_doc_id(str(file_path))
        
        # 2. ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ë³€í™˜
        slide_images = await self._convert_to_slide_images(file_path, doc_id)
        
        # 3. GPT ìº¡ì…˜ ìƒì„±
        captioned_slides = await self._generate_captions(slide_images)
        
        # 4. Document ê°ì²´ ìƒì„± ë° ì¸ë±ì‹±
        documents = await self._create_documents(captioned_slides, doc_id)
        
        # 5. ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•
        index = await self._build_vector_index(documents)
        
        # 6. ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±
        doc_meta = DocumentMeta(
            doc_id=doc_id,
            title=self._extract_title(file_path.stem, captioned_slides),
            doc_type=file_path.suffix.lower().replace('.', ''),
            total_pages=len(slide_images),
            file_path=str(file_path)
        )
        
        # 7. ë©”ëª¨ë¦¬ì— ì €ì¥
        self.documents[doc_id] = doc_meta
        self.indexes[doc_id] = index
        self.slide_images[doc_id] = captioned_slides
        
        logger.info(f" ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {doc_id}")
        logger.info(f"  ì´ {len(slide_images)} ìŠ¬ë¼ì´ë“œ ì²˜ë¦¬ë¨")
        
        return doc_meta
    
    # ê¸°ì¡´ DocumentAgent ë©”ì„œë“œë“¤ (ë™ì¼)
    async def _convert_to_slide_images(self, file_path: Path, doc_id: str) -> List[Dict[str, Any]]:
        """íŒŒì¼ì„ ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        logger.info(f"ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ë³€í™˜ ì¤‘: {file_path}")
        
        slide_images = []
        
        if file_path.suffix.lower() == '.pdf':
            slide_images = await self._convert_pdf_to_images(file_path, doc_id)
        elif file_path.suffix.lower() in ['.ppt', '.pptx']:
            slide_images = await self._convert_ppt_to_images(file_path, doc_id)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}")
        
        logger.info(f"ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ: {len(slide_images)} ìŠ¬ë¼ì´ë“œ")
        return slide_images
    
    async def _convert_pdf_to_images(self, file_path: Path, doc_id: str) -> List[Dict[str, Any]]:
        """PDFë¥¼ í˜ì´ì§€ë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        slides = []
        
        try:
            pdf_doc = fitz.open(file_path)
            
            for page_num in range(len(pdf_doc)):
                page = pdf_doc[page_num]
                
                # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (300 DPI)
                mat = fitz.Matrix(300/72, 300/72)
                pix = page.get_pixmap(matrix=mat)
                
                # PIL Imageë¡œ ë³€í™˜
                img_data = pix.pil_tobytes(format="PNG")
                pil_image = Image.open(BytesIO(img_data))
                
                # Base64 ì¸ì½”ë”©
                buffered = BytesIO()
                pil_image.save(buffered, format="PNG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode()
                
                slide_data = {
                    "doc_id": doc_id,
                    "page_id": f"p{page_num + 1:03d}",
                    "page_number": page_num + 1,
                    "image_base64": img_base64,
                    "dimensions": pil_image.size,
                    "size_bytes": len(img_data),
                    "caption": None
                }
                
                slides.append(slide_data)
                pix = None
                
        except Exception as e:
            logger.error(f"PDF ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            if 'pdf_doc' in locals():
                pdf_doc.close()
        
        return slides
    
    async def _convert_ppt_to_images(self, file_path: Path, doc_id: str) -> List[Dict[str, Any]]:
        """PowerPointë¥¼ ìŠ¬ë¼ì´ë“œë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        slides = []
        
        try:
            prs = Presentation(file_path)
            
            for slide_num, slide in enumerate(prs.slides, 1):
                # ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                slide_text = ""
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text:
                        slide_text += shape.text + "\n"
                
                # ì„ì‹œ ì´ë¯¸ì§€ ìƒì„±
                dummy_image = Image.new('RGB', (1920, 1080), color='white')
                
                buffered = BytesIO()
                dummy_image.save(buffered, format="PNG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode()
                
                slide_data = {
                    "doc_id": doc_id,
                    "page_id": f"slide_{slide_num:03d}",
                    "page_number": slide_num,
                    "image_base64": img_base64,
                    "dimensions": (1920, 1080),
                    "size_bytes": len(buffered.getvalue()),
                    "slide_text": slide_text,
                    "caption": None
                }
                
                slides.append(slide_data)
        
        except Exception as e:
            logger.error(f"PPT ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            raise
        
        return slides
    
    async def _generate_captions(self, slide_images: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ GPT ìº¡ì…˜ ìƒì„±"""
        logger.info(f"GPT ìº¡ì…˜ ìƒì„± ì¤‘: {len(slide_images)} ìŠ¬ë¼ì´ë“œ")
        
        captioned_slides = []
        
        for i, slide_data in enumerate(slide_images):
            try:
                logger.info(f"ìº¡ì…˜ ìƒì„±: {slide_data['page_id']} ({i+1}/{len(slide_images)})")
                
                caption = await self._generate_single_caption(slide_data)
                
                slide_data_with_caption = slide_data.copy()
                slide_data_with_caption["caption"] = caption
                
                captioned_slides.append(slide_data_with_caption)
                
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.warning(f"ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨ {slide_data['page_id']}: {str(e)}")
                slide_data_with_caption = slide_data.copy()
                slide_data_with_caption["caption"] = f"ìŠ¬ë¼ì´ë“œ {slide_data['page_number']}"
                captioned_slides.append(slide_data_with_caption)
        
        logger.info(f"ìº¡ì…˜ ìƒì„± ì™„ë£Œ: {len(captioned_slides)} ìŠ¬ë¼ì´ë“œ")
        return captioned_slides
    
    async def _generate_single_caption(self, slide_data: Dict[str, Any]) -> str:
        """ë‹¨ì¼ ìŠ¬ë¼ì´ë“œì— ëŒ€í•œ GPT ìº¡ì…˜ ìƒì„±"""
        
        context = ""
        if "slide_text" in slide_data and slide_data["slide_text"].strip():
            context = f"\n\nìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸:\n{slide_data['slide_text']}"
        
        prompt = f"""ì´ êµìœ¡ìë£Œ ìŠ¬ë¼ì´ë“œë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”.

ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
- ìŠ¬ë¼ì´ë“œì˜ ì£¼ìš” ì£¼ì œ/ë‚´ìš©
- í•µì‹¬ ê°œë…ì´ë‚˜ ì •ë³´
- êµìœ¡ì  ëª©ì ì´ë‚˜ ì˜ë„

ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.{context}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.vision_model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{slide_data['image_base64']}",
                                    "detail": self.image_quality
                                }
                            }
                        ]
                    }
                ],
            )
            
            caption = response.choices[0].message.content.strip()

            return caption
            
        except Exception as e:
            logger.error(f"GPT ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ìŠ¬ë¼ì´ë“œ {slide_data['page_number']} - êµìœ¡ìë£Œ ë‚´ìš©"
    
    async def _create_documents(self, captioned_slides: List[Dict[str, Any]], doc_id: str) -> List[Document]:
        """ìº¡ì…˜ì´ ìˆëŠ” ìŠ¬ë¼ì´ë“œë“¤ì„ LlamaIndex Document ê°ì²´ë¡œ ë³€í™˜"""
        logger.info(f"Document ê°ì²´ ìƒì„± ì¤‘: {len(captioned_slides)} ìŠ¬ë¼ì´ë“œ")
        
        documents = []
        
        for slide_data in captioned_slides:
            text_content = slide_data["caption"]
            
            if "slide_text" in slide_data and slide_data["slide_text"].strip():
                text_content += f"\n\nì›ë³¸ í…ìŠ¤íŠ¸:\n{slide_data['slide_text']}"
            
            metadata = {
                "doc_id": doc_id,
                "page_id": slide_data["page_id"],
                "page_number": slide_data["page_number"],
                "image_path": None,
                "dimensions": slide_data["dimensions"],
                "size_bytes": slide_data["size_bytes"],
                "has_caption": bool(slide_data["caption"])
            }
            
            doc = Document(
                text=text_content,
                metadata=metadata
            )
            
            documents.append(doc)
        
        logger.info(f"Document ê°ì²´ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ")
        return documents
    
    async def _build_vector_index(self, documents: List[Document]) -> VectorStoreIndex:
        """Document ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        logger.info(f"ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘: {len(documents)} ë¬¸ì„œ")
        
        try:
            index = VectorStoreIndex.from_documents(
                documents,
                embed_model=self.embeddings
            )
            
            logger.info("ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
            return index
            
        except Exception as e:
            logger.error(f"ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _extract_title(self, filename: str, slides: List[Dict[str, Any]]) -> str:
        """ë¬¸ì„œ ì œëª© ì¶”ì¶œ"""
        if slides and slides[0].get("caption"):
            first_caption = slides[0]["caption"]
            sentences = first_caption.split('.')
            if sentences and len(sentences[0]) < 100:
                return sentences[0].strip()
        
        return filename.replace('_', ' ').replace('-', ' ')
    
    # ê¸°ì¡´ ì¡°íšŒ ë©”ì„œë“œë“¤ + ìš©ì–´ ë”•ì…”ë„ˆë¦¬ ê´€ë ¨ ë©”ì„œë“œ ì¶”ê°€
    
    def get_document(self, doc_id: str) -> Optional[DocumentMeta]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        return self.documents.get(doc_id)
    
    def get_index(self, doc_id: str) -> Optional[VectorStoreIndex]:
        """ë¬¸ì„œì˜ ë²¡í„° ì¸ë±ìŠ¤ ì¡°íšŒ"""
        return self.indexes.get(doc_id)
    
    def get_slide_data(self, doc_id: str) -> List[Dict[str, Any]]:
        """ìŠ¬ë¼ì´ë“œ ì›ë³¸ ë°ì´í„° ì¡°íšŒ"""
        return self.slide_images.get(doc_id, [])
    
    def search_in_document(self, doc_id: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ë‚´ ì˜ë¯¸ì  ê²€ìƒ‰"""
        index = self.get_index(doc_id)
        if not index:
            return []
        
        try:
            query_engine = index.as_query_engine(similarity_top_k=top_k)
            response = query_engine.query(query)
            
            results = []
            for node in response.source_nodes:
                result = {
                    "text": node.text,
                    "score": getattr(node, 'score', 0.0),
                    "metadata": node.metadata,
                    "page_id": node.metadata.get("page_id"),
                    "page_number": node.metadata.get("page_number")
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_slide_by_page_id(self, doc_id: str, page_id: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ìŠ¬ë¼ì´ë“œ ë°ì´í„° ì¡°íšŒ"""
        slides = self.get_slide_data(doc_id)
        
        for slide in slides:
            if slide["page_id"] == page_id:
                return slide
        
        return None
    
    def list_documents(self) -> List[DocumentMeta]:
        """ëª¨ë“  ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        return list(self.documents.values())
    
    def get_document_stats(self, doc_id: str) -> Dict[str, Any]:
        """ë¬¸ì„œ í†µê³„ ì •ë³´"""
        doc_meta = self.get_document(doc_id)
        slides = self.get_slide_data(doc_id)
        
        if not doc_meta or not slides:
            return {}
        
        # ìº¡ì…˜ í†µê³„
        captioned_slides = sum(1 for slide in slides if slide.get("caption"))
        total_caption_chars = sum(len(slide.get("caption", "")) for slide in slides)
        
        # ì´ë¯¸ì§€ í†µê³„
        total_image_size = sum(slide.get("size_bytes", 0) for slide in slides)
        avg_dimensions = (
            sum(slide.get("dimensions", [0, 0])[0] for slide in slides) / len(slides),
            sum(slide.get("dimensions", [0, 0])[1] for slide in slides) / len(slides)
        ) if slides else (0, 0)
        
        return {
            "doc_id": doc_id,
            "title": doc_meta.title,
            "doc_type": doc_meta.doc_type,
            "total_slides": len(slides),
            "captioned_slides": captioned_slides,
            "caption_coverage": captioned_slides / len(slides) if slides else 0,
            "total_caption_chars": total_caption_chars,
            "avg_caption_length": total_caption_chars / captioned_slides if captioned_slides else 0,
            "total_image_size_mb": total_image_size / (1024 * 1024),
            "avg_dimensions": avg_dimensions,
            "has_index": doc_id in self.indexes,
        }


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_document_agent():
    """DocumentAgent í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª  DocumentAgent í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ì°¾ê¸°
    test_files = [
        "sample_docs/sample.pdf", 
    ]
    
    test_file = None
    for file_name in test_files:
        if Path(file_name).exists():
            test_file = file_name
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ë‹¤ìŒ íŒŒì¼ ì¤‘ í•˜ë‚˜ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”: {', '.join(test_files)}")
        return
    
    print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_file}")
    
    try:
        #  DocumentAgent ìƒì„±
        agent = DocumentAgent(
            openai_api_key=api_key,
            vision_model="gpt-5-nano",
            embedding_model="text-embedding-3-small"
        )
        
        print(f"ğŸ“–  ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {test_file}")
        doc_meta = await agent.process_document(test_file)
        
        print(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ë¬¸ì„œ ID: {doc_meta.doc_id}")
        print(f"   ì œëª©: {doc_meta.title}")
        print(f"   ìŠ¬ë¼ì´ë“œ ìˆ˜: {doc_meta.total_pages}")
        
        
        # ë¬¸ì„œ í†µê³„
        stats = agent.get_document_stats(doc_meta.doc_id)
        print(f"\nğŸ“Š ë¬¸ì„œ í†µê³„:")
        print(f"   ìº¡ì…˜ ìƒì„±ë¥ : {stats['caption_coverage']:.1%}")
        print(f"   í‰ê·  ìº¡ì…˜ ê¸¸ì´: {stats['avg_caption_length']:.0f}ì")
        print(f"   ì´ ì´ë¯¸ì§€ í¬ê¸°: {stats['total_image_size_mb']:.1f}MB")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        if stats['has_index']:
            search_results = agent.search_in_document(
                doc_meta.doc_id,
                "ê°œìš”",
                top_k=3
            )
            print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            
            for result in search_results[:2]:
                print(f"     - í˜ì´ì§€ {result['page_number']}: {result['text'][:50]}...")
        
        # ìŠ¬ë¼ì´ë“œ ë°ì´í„° í™•ì¸
        slides = agent.get_slide_data(doc_meta.doc_id)
        if slides:
            first_slide = slides[0]
            print(f"   ì²« ìŠ¬ë¼ì´ë“œ ìº¡ì…˜: {first_slide.get('caption', 'None')[:100]}...")
        
        print("\nğŸ‰  DocumentAgent í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_document_agent())