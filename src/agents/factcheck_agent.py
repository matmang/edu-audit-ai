"""
EDU-Audit Fact Check Agent
ì‚¬ì‹¤ ê²€ì¦ ë° ì •ë³´ ìµœì‹ ì„± ê²€ì‚¬ ì—ì´ì „íŠ¸
"""

import asyncio
import logging
import re
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass
from urllib.parse import quote

import aiohttp
from llama_index.llms.openai import OpenAI

from src.core.models import (
    DocumentMeta, PageInfo, PageElement, ElementType, Issue, IssueType,
    FactCheckRequest, FactCheckResult, TextLocation, BoundingBox,
    generate_issue_id
)

logger = logging.getLogger(__name__)


@dataclass
class FactCheckRule:
    """ì‚¬ì‹¤ ê²€ì¦ ê·œì¹™"""
    domains: List[str]  # ì ìš© ë„ë©”ì¸ (science, technology, history, etc.)
    claim_patterns: List[str]  # ê²€ì¦ì´ í•„ìš”í•œ ë¬¸ì¥ íŒ¨í„´
    confidence_threshold: float = 0.7  # ì‹ ë¢°ë„ ì„ê³„ê°’
    require_sources: bool = True  # ì¶œì²˜ í•„ìš” ì—¬ë¶€


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    title: str
    url: str
    snippet: str
    published_date: Optional[str] = None
    source_domain: str = ""
    relevance_score: float = 0.0


@dataclass
class FactVerification:
    """ì‚¬ì‹¤ ê²€ì¦ ê²°ê³¼"""
    claim: str
    is_factual: bool
    confidence: float
    evidence: List[SearchResult]
    reasoning: str
    is_outdated: bool = False
    last_updated: Optional[str] = None
    contradictory_info: Optional[str] = None


class FactCheckAgent:
    """ì‚¬ì‹¤ ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    def __init__(
        self,
        openai_api_key: Optional[str] = None,
        serpapi_key: Optional[str] = None,
        llm_model: str = "gpt-5-nano",
        max_search_results: int = 5,
        fact_check_timeout: int = 30
    ):
        self.openai_api_key = openai_api_key
        self.serpapi_key = serpapi_key
        self.max_search_results = max_search_results
        self.fact_check_timeout = fact_check_timeout
        
        # LLM ì´ˆê¸°í™”
        self.llm = None
        if openai_api_key:
            self.llm = OpenAI(
                model=llm_model,
                temperature=0.1,  # ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •
                api_key=openai_api_key
            )
        
        # HTTP í´ë¼ì´ì–¸íŠ¸
        self.session = None
        
        # ì‚¬ì‹¤ ê²€ì¦ ê·œì¹™ ë¡œë“œ
        self._load_fact_check_rules()
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ ëª©ë¡
        self._load_trusted_sources()
        
        logger.info("FactCheckAgent ì´ˆê¸°í™” ì™„ë£Œ")
        if not openai_api_key:
            logger.warning("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. LLM ë¶„ì„ì´ ì œí•œë©ë‹ˆë‹¤.")
        if not serpapi_key:
            logger.warning("SerpAPI í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì´ ì œí•œë©ë‹ˆë‹¤.")
    
    def _load_fact_check_rules(self):
        """ì‚¬ì‹¤ ê²€ì¦ ê·œì¹™ ë¡œë“œ"""
        self.fact_check_rules = [
            FactCheckRule(
                domains=["science", "technology", "research"],
                claim_patterns=[
                    r"ì—°êµ¬ì— ë”°ë¥´ë©´",
                    r"ìµœì‹  ì—°êµ¬",
                    r"[0-9]{4}ë…„.*ì—°êµ¬",
                    r"ì‹¤í—˜ ê²°ê³¼",
                    r"ê³¼í•™ìë“¤ì€.*ë°œê²¬",
                    r"í†µê³„ì— ì˜í•˜ë©´",
                    r"ë°ì´í„°ì— ë”°ë¥´ë©´"
                ],
                confidence_threshold=0.8,
                require_sources=True
            ),
            FactCheckRule(
                domains=["statistics", "data", "numbers"],
                claim_patterns=[
                    r"[0-9]+%",
                    r"[0-9,]+ëª…",
                    r"[0-9,]+ê°œ",
                    r"ìˆœìœ„.*[0-9]+ìœ„",
                    r"ì¦ê°€.*[0-9]+%",
                    r"ê°ì†Œ.*[0-9]+%"
                ],
                confidence_threshold=0.7,
                require_sources=True
            ),
            FactCheckRule(
                domains=["technology", "software", "ai"],
                claim_patterns=[
                    r"ìµœì‹  ë²„ì „",
                    r"ìƒˆë¡œìš´ ê¸°ëŠ¥",
                    r"ì—…ë°ì´íŠ¸",
                    r"[0-9]{4}ë…„.*ì¶œì‹œ",
                    r"í˜„ì¬.*ì§€ì›",
                    r"GPT-[0-9]+",
                    r"ChatGPT",
                    r"ìµœì‹ .*ëª¨ë¸"
                ],
                confidence_threshold=0.6,
                require_sources=False
            ),
            FactCheckRule(
                domains=["events", "news", "current"],
                claim_patterns=[
                    r"ìµœê·¼.*ë°œí‘œ",
                    r"ì˜¬í•´",
                    r"ì´ë²ˆ ë‹¬",
                    r"í˜„ì¬.*ìƒí™©",
                    r"ì‘ë…„",
                    r"[0-9]{4}ë…„.*ì›”"
                ],
                confidence_threshold=0.8,
                require_sources=True
            )
        ]
    
    def _load_trusted_sources(self):
        """ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ ëª©ë¡ ë¡œë“œ"""
        self.trusted_sources = {
            # í•™ìˆ /ì—°êµ¬ ì†ŒìŠ¤
            "academic": [
                "arxiv.org", "pubmed.ncbi.nlm.nih.gov", "scholar.google.com",
                "ieee.org", "acm.org", "nature.com", "science.org",
                "springer.com", "elsevier.com"
            ],
            # ë‰´ìŠ¤/ë¯¸ë””ì–´ ì†ŒìŠ¤
            "news": [
                "bbc.com", "reuters.com", "ap.org", "nytimes.com",
                "washingtonpost.com", "cnn.com", "npr.org"
            ],
            # ê¸°ìˆ  ì†ŒìŠ¤
            "technology": [
                "github.com", "stackoverflow.com", "developer.mozilla.org",
                "techcrunch.com", "arstechnica.com", "wired.com"
            ],
            # ì •ë¶€/ê³µì‹ ì†ŒìŠ¤
            "official": [
                ".gov", ".edu", "who.int", "unesco.org", "un.org"
            ]
        }
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.fact_check_timeout)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def check_document_facts(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """
        ë¬¸ì„œ ì „ì²´ ì‚¬ì‹¤ ê²€ì¦
        
        Args:
            doc_meta: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            pages: í˜ì´ì§€ ëª©ë¡
            
        Returns:
            List[Issue]: ë°œê²¬ëœ ì‚¬ì‹¤ ì˜¤ë¥˜ ì´ìŠˆë“¤
        """
        logger.info(f"ë¬¸ì„œ ì‚¬ì‹¤ ê²€ì¦ ì‹œì‘: {doc_meta.doc_id}")
        
        all_issues = []
        
        async with self:
            for page in pages:
                # 1. í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ê²€ì¦ ëŒ€ìƒ ì¶”ì¶œ
                text_claims = await self._extract_fact_claims(page.raw_text)
                
                for claim in text_claims:
                    page_issues = await self._verify_claim_in_page(
                        doc_meta, page, claim, page.raw_text
                    )
                    all_issues.extend(page_issues)
                
                # 2. ë©€í‹°ëª¨ë‹¬ ìš”ì†Œì—ì„œ ê²€ì¦ ëŒ€ìƒ ì¶”ì¶œ
                for element in page.elements:
                    element_issues = await self._verify_multimodal_element(
                        doc_meta, page, element
                    )
                    all_issues.extend(element_issues)
                
                # API í˜¸ì¶œ ì œí•œ
                await asyncio.sleep(0.5)
        
        logger.info(f"ì‚¬ì‹¤ ê²€ì¦ ì™„ë£Œ: {len(all_issues)}ê°œ ì´ìŠˆ ë°œê²¬")
        return all_issues
    
    async def verify_single_claim(self, claim: str, context: str = None) -> FactCheckResult:
        """
        ë‹¨ì¼ ì‚¬ì‹¤ ì£¼ì¥ ê²€ì¦
        
        Args:
            claim: ê²€ì¦í•  ì£¼ì¥
            context: ë¬¸ë§¥ ì •ë³´
            
        Returns:
            FactCheckResult: ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"ë‹¨ì¼ ì‚¬ì‹¤ ê²€ì¦: {claim[:50]}...")
        
        async with self:
            # 1. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
            search_results = await self._search_for_claim(claim)
            
            # 2. LLMì„ í†µí•œ ì‚¬ì‹¤ ê²€ì¦
            verification = await self._verify_with_llm(claim, search_results, context)
            
            # 3. ìµœì‹ ì„± ê²€ì‚¬
            is_outdated, last_updated = await self._check_if_outdated(claim, search_results)
            
            result = FactCheckResult(
                sentence=claim,
                is_factual=verification.is_factual,
                confidence=verification.confidence,
                explanation=verification.reasoning,
                sources=[result.url for result in verification.evidence],
                checked_at=datetime.now()
            )
            
            # ì¶”ê°€ ì •ë³´
            if is_outdated:
                result.explanation += f"\n\nâš ï¸ ì£¼ì˜: ì´ ì •ë³´ëŠ” ì˜¤ë˜ëœ ê²ƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_updated})"
            
            if verification.contradictory_info:
                result.explanation += f"\n\nğŸ”„ ìƒì¶© ì •ë³´: {verification.contradictory_info}"
            
            return result
    
    async def _extract_fact_claims(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì‹¤ ê²€ì¦ì´ í•„ìš”í•œ ì£¼ì¥ë“¤ ì¶”ì¶œ"""
        claims = []
        
        if not text.strip():
            return claims
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ì œì™¸
                continue
            
            # ê° ê·œì¹™ì— ëŒ€í•´ íŒ¨í„´ ë§¤ì¹­
            for rule in self.fact_check_rules:
                for pattern in rule.claim_patterns:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        claims.append(sentence)
                        break
        
        # ì¤‘ë³µ ì œê±°
        return list(set(claims))
    
    async def _verify_claim_in_page(
        self, 
        doc_meta: DocumentMeta, 
        page: PageInfo, 
        claim: str,
        full_text: str
    ) -> List[Issue]:
        """í˜ì´ì§€ ë‚´ íŠ¹ì • ì£¼ì¥ ê²€ì¦"""
        issues = []
        
        try:
            # ì£¼ì¥ ê²€ì¦ ìˆ˜í–‰
            verification = await self._perform_fact_verification(claim)
            
            if not verification.is_factual or verification.is_outdated:
                # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ ì°¾ê¸°
                pos = full_text.find(claim)
                if pos != -1:
                    location = TextLocation(start=pos, end=pos + len(claim))
                else:
                    # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                    words = claim.split()[:5]  # ì²˜ìŒ 5ë‹¨ì–´ë¡œ ê²€ìƒ‰
                    partial_claim = " ".join(words)
                    pos = full_text.find(partial_claim)
                    if pos != -1:
                        location = TextLocation(start=pos, end=pos + len(partial_claim))
                    else:
                        location = TextLocation(start=0, end=len(claim))
                
                # ì´ìŠˆ íƒ€ì… ê²°ì •
                if not verification.is_factual:
                    issue_message = f"ì‚¬ì‹¤ ì˜¤ë¥˜ ê°€ëŠ¥ì„±: {verification.reasoning}"
                    suggestion = "ì¶œì²˜ í™•ì¸ ë° ìµœì‹  ì •ë³´ ì—…ë°ì´íŠ¸ í•„ìš”"
                else:  # is_outdated
                    issue_message = f"ì •ë³´ê°€ ì˜¤ë˜ë˜ì—ˆì„ ìˆ˜ ìˆìŒ: {verification.last_updated}"
                    suggestion = "ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸ ê¶Œì¥"
                
                issue_id = generate_issue_id(
                    doc_meta.doc_id,
                    page.page_id,
                    location,
                    IssueType.FACT
                )
                
                issue = Issue(
                    issue_id=issue_id,
                    doc_id=doc_meta.doc_id,
                    page_id=page.page_id,
                    issue_type=IssueType.FACT,
                    text_location=location,
                    original_text=claim,
                    message=issue_message,
                    suggestion=suggestion,
                    confidence=verification.confidence,
                    confidence_level="high",  # Pydanticì´ ìë™ ê³„ì‚°
                    agent_name="fact_check_agent"
                )
                
                issues.append(issue)
        
        except Exception as e:
            logger.warning(f"ì£¼ì¥ ê²€ì¦ ì‹¤íŒ¨ '{claim[:30]}...': {str(e)}")
        
        return issues
    
    async def _verify_multimodal_element(
        self, 
        doc_meta: DocumentMeta, 
        page: PageInfo, 
        element: PageElement
    ) -> List[Issue]:
        """ë©€í‹°ëª¨ë‹¬ ìš”ì†Œì˜ ì‚¬ì‹¤ ê²€ì¦"""
        issues = []
        
        try:
            text_to_verify = None
            
            # ìš”ì†Œ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if element.element_type == ElementType.IMAGE and element.image_data:
                # OCR í…ìŠ¤íŠ¸ì™€ AI ì„¤ëª…ì—ì„œ ê²€ì¦ ëŒ€ìƒ ì°¾ê¸°
                if element.image_data.ocr_text:
                    claims = await self._extract_fact_claims(element.image_data.ocr_text)
                    for claim in claims:
                        verification = await self._perform_fact_verification(claim)
                        if not verification.is_factual or verification.is_outdated:
                            issue = self._create_multimodal_fact_issue(
                                doc_meta, page, element, claim, verification
                            )
                            issues.append(issue)
                
                if element.image_data.description:
                    claims = await self._extract_fact_claims(element.image_data.description)
                    for claim in claims:
                        verification = await self._perform_fact_verification(claim)
                        if not verification.is_factual or verification.is_outdated:
                            issue = self._create_multimodal_fact_issue(
                                doc_meta, page, element, claim, verification
                            )
                            issues.append(issue)
            
            elif element.element_type == ElementType.TABLE and element.table_data:
                # í‘œ ë‚´ìš©ì—ì„œ ìˆ«ì ë°ì´í„° ê²€ì¦
                table_text = self._extract_table_text(element.table_data)
                claims = await self._extract_fact_claims(table_text)
                for claim in claims:
                    verification = await self._perform_fact_verification(claim)
                    if not verification.is_factual or verification.is_outdated:
                        issue = self._create_multimodal_fact_issue(
                            doc_meta, page, element, claim, verification
                        )
                        issues.append(issue)
            
            elif element.element_type == ElementType.CHART and element.chart_data:
                # ì°¨íŠ¸ ì„¤ëª…ì—ì„œ ë°ì´í„° ê´€ë ¨ ì£¼ì¥ ê²€ì¦
                if element.chart_data.description:
                    claims = await self._extract_fact_claims(element.chart_data.description)
                    for claim in claims:
                        verification = await self._perform_fact_verification(claim)
                        if not verification.is_factual or verification.is_outdated:
                            issue = self._create_multimodal_fact_issue(
                                doc_meta, page, element, claim, verification
                            )
                            issues.append(issue)
        
        except Exception as e:
            logger.warning(f"ë©€í‹°ëª¨ë‹¬ ìš”ì†Œ ê²€ì¦ ì‹¤íŒ¨ {element.element_id}: {str(e)}")
        
        return issues
    
    def _extract_table_text(self, table_data) -> str:
        """í‘œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        texts = []
        
        if table_data.headers:
            texts.append(" ".join(table_data.headers))
        
        for row in table_data.rows:
            texts.append(" ".join(row))
        
        return " ".join(texts)
    
    def _create_multimodal_fact_issue(
        self, 
        doc_meta: DocumentMeta, 
        page: PageInfo, 
        element: PageElement,
        claim: str, 
        verification: FactVerification
    ) -> Issue:
        """ë©€í‹°ëª¨ë‹¬ ìš”ì†Œì˜ ì‚¬ì‹¤ ì˜¤ë¥˜ ì´ìŠˆ ìƒì„±"""
        
        if not verification.is_factual:
            message = f"ë©€í‹°ëª¨ë‹¬ ìš”ì†Œ ë‚´ ì‚¬ì‹¤ ì˜¤ë¥˜: {verification.reasoning}"
            suggestion = "ì¶œì²˜ í™•ì¸ ë° ì •ë³´ ê²€ì¦ í•„ìš”"
        else:  # is_outdated
            message = f"ë©€í‹°ëª¨ë‹¬ ìš”ì†Œ ë‚´ ì •ë³´ ê³¼ì‹œ: {verification.last_updated}"
            suggestion = "ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ ê¶Œì¥"
        
        issue_id = generate_issue_id(
            doc_meta.doc_id,
            page.page_id,
            element.bbox or BoundingBox(x=0, y=0, width=100, height=100),
            IssueType.FACT
        )
        
        return Issue(
            issue_id=issue_id,
            doc_id=doc_meta.doc_id,
            page_id=page.page_id,
            issue_type=IssueType.FACT,
            bbox_location=element.bbox,
            element_id=element.element_id,
            original_text=claim,
            message=message,
            suggestion=suggestion,
            confidence=verification.confidence,
            confidence_level="high",
            agent_name="fact_check_agent"
        )
    
    async def _perform_fact_verification(self, claim: str) -> FactVerification:
        """ì‹¤ì œ ì‚¬ì‹¤ ê²€ì¦ ìˆ˜í–‰"""
        try:
            # 1. ì›¹ ê²€ìƒ‰
            search_results = await self._search_for_claim(claim)
            
            # 2. LLM ê²€ì¦
            verification = await self._verify_with_llm(claim, search_results)
            
            # 3. ìµœì‹ ì„± ê²€ì‚¬
            is_outdated, last_updated = await self._check_if_outdated(claim, search_results)
            verification.is_outdated = is_outdated
            verification.last_updated = last_updated
            
            return verification
            
        except Exception as e:
            logger.warning(f"ì‚¬ì‹¤ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return FactVerification(
                claim=claim,
                is_factual=True,  # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                confidence=0.1,
                evidence=[],
                reasoning="ê²€ì¦ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ",
                is_outdated=False
            )
    
    async def _search_for_claim(self, claim: str) -> List[SearchResult]:
        """ì£¼ì¥ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        search_results = []
        
        try:
            if self.serpapi_key:
                # SerpAPIë¥¼ í†µí•œ Google ê²€ìƒ‰
                results = await self._search_with_serpapi(claim)
                search_results.extend(results)
            else:
                # ëŒ€ì²´ ê²€ìƒ‰ ë°©ë²• ë˜ëŠ” ë”ë¯¸ ê²°ê³¼
                logger.warning("SerpAPI í‚¤ê°€ ì—†ì–´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                search_results = self._generate_dummy_search_results(claim)
        
        except Exception as e:
            logger.warning(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            search_results = self._generate_dummy_search_results(claim)
        
        return search_results[:self.max_search_results]
    
    async def _search_with_serpapi(self, query: str) -> List[SearchResult]:
        """SerpAPIë¥¼ í†µí•œ Google ê²€ìƒ‰"""
        if not self.session:
            return []
        
        try:
            url = "https://serpapi.com/search"
            params = {
                "q": query,
                "api_key": self.serpapi_key,
                "engine": "google",
                "num": self.max_search_results
            }
            
            async with self.session.get(url, params=params) as response:
                data = await response.json()
                
                results = []
                for item in data.get("organic_results", []):
                    result = SearchResult(
                        title=item.get("title", ""),
                        url=item.get("link", ""),
                        snippet=item.get("snippet", ""),
                        source_domain=self._extract_domain(item.get("link", "")),
                        relevance_score=0.8  # ê¸°ë³¸ê°’
                    )
                    results.append(result)
                
                return results
        
        except Exception as e:
            logger.warning(f"SerpAPI ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _generate_dummy_search_results(self, claim: str) -> List[SearchResult]:
        """ë”ë¯¸ ê²€ìƒ‰ ê²°ê³¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        return [
            SearchResult(
                title=f"Fact-checking: {claim[:30]}...",
                url="https://example.com/fact-check",
                snippet="ì´ ì£¼ì¥ì— ëŒ€í•œ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                source_domain="example.com",
                relevance_score=0.5
            )
        ]
    
    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc
        except:
            return ""
    
    async def _verify_with_llm(
        self, 
        claim: str, 
        search_results: List[SearchResult], 
        context: str = None
    ) -> FactVerification:
        """LLMì„ í†µí•œ ì‚¬ì‹¤ ê²€ì¦"""
        if not self.llm:
            return FactVerification(
                claim=claim,
                is_factual=True,
                confidence=0.1,
                evidence=search_results,
                reasoning="LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê²€ì¦í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"
            )
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ë§¥ìœ¼ë¡œ ì •ë¦¬
            evidence_text = "\n".join([
                f"ì¶œì²˜ {i+1}: {result.title}\n{result.snippet}\n"
                for i, result in enumerate(search_results)
            ])
            
            prompt = self._create_fact_verification_prompt(claim, evidence_text, context)
            
            response = await self.llm.acomplete(prompt)
            
            # LLM ì‘ë‹µ íŒŒì‹±
            verification = self._parse_llm_verification_response(
                response.text, claim, search_results
            )
            
            return verification
            
        except Exception as e:
            logger.warning(f"LLM ì‚¬ì‹¤ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return FactVerification(
                claim=claim,
                is_factual=True,
                confidence=0.1,
                evidence=search_results,
                reasoning="LLM ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            )
    
    def _create_fact_verification_prompt(self, claim: str, evidence: str, context: str = None) -> str:
        """ì‚¬ì‹¤ ê²€ì¦ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        context_part = f"\n\në¬¸ë§¥ ì •ë³´:\n{context}" if context else ""
        
        return f"""ë‹¤ìŒ ì£¼ì¥ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•´ì£¼ì„¸ìš”.

ê²€ì¦í•  ì£¼ì¥: {claim}

ì°¸ê³  ìë£Œ:
{evidence}{context_part}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
íŒì •: [ì‚¬ì‹¤/ê±°ì§“/ë¶ˆë¶„ëª…]
ì‹ ë¢°ë„: [0.0-1.0]
ê·¼ê±°: [íŒì • ì´ìœ ë¥¼ ìì„¸íˆ ì„¤ëª…]
ìƒì¶©ì •ë³´: [ìˆë‹¤ë©´ ì–¸ê¸‰, ì—†ìœ¼ë©´ "ì—†ìŒ"]

ì£¼ì˜ì‚¬í•­:
- ê²€ìƒ‰ ê²°ê³¼ì™€ ì£¼ì¥ì„ ì‹ ì¤‘íˆ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”
- ì¶œì²˜ì˜ ì‹ ë¢°ì„±ë„ ê³ ë ¤í•˜ì„¸ìš”
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ë¶ˆë¶„ëª…"ìœ¼ë¡œ íŒì •í•˜ì„¸ìš”
- êµìœ¡ ìë£Œì˜ ì •í™•ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ì—„ê²©í•˜ê²Œ ê²€ì¦í•˜ì„¸ìš”"""
    
    def _parse_llm_verification_response(
        self, 
        response: str, 
        claim: str, 
        search_results: List[SearchResult]
    ) -> FactVerification:
        """LLM ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
        try:
            lines = response.strip().split('\n')
            
            # ê¸°ë³¸ê°’
            is_factual = True
            confidence = 0.5
            reasoning = "ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
            contradictory_info = None
            
            for line in lines:
                line = line.strip()
                
                if line.startswith("íŒì •:"):
                    judgment = line.split(":", 1)[1].strip().lower()
                    if "ê±°ì§“" in judgment or "false" in judgment:
                        is_factual = False
                    elif "ë¶ˆë¶„ëª…" in judgment or "unclear" in judgment:
                        is_factual = False
                        confidence = 0.3
                
                elif line.startswith("ì‹ ë¢°ë„:"):
                    try:
                        confidence = float(line.split(":", 1)[1].strip())
                    except:
                        pass
                
                elif line.startswith("ê·¼ê±°:"):
                    reasoning = line.split(":", 1)[1].strip()
                
                elif line.startswith("ìƒì¶©ì •ë³´:"):
                    contradictory_part = line.split(":", 1)[1].strip()
                    if contradictory_part and contradictory_part != "ì—†ìŒ":
                        contradictory_info = contradictory_part
            
            return FactVerification(
                claim=claim,
                is_factual=is_factual,
                confidence=confidence,
                evidence=search_results,
                reasoning=reasoning,
                contradictory_info=contradictory_info
            )
            
        except Exception as e:
            logger.warning(f"LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return FactVerification(
                claim=claim,
                is_factual=True,
                confidence=0.1,
                evidence=search_results,
                reasoning="ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            )
    
    async def _check_if_outdated(
        self, 
        claim: str, 
        search_results: List[SearchResult]
    ) -> Tuple[bool, Optional[str]]:
        """ì •ë³´ì˜ ìµœì‹ ì„± ê²€ì‚¬"""
        if not self.llm:
            return False, None
        
        try:
            # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ì‚¬
            time_keywords = ["ìµœì‹ ", "í˜„ì¬", "ì˜¬í•´", "ì‘ë…„", "ìµœê·¼", "ìƒˆë¡œìš´", "ì—…ë°ì´íŠ¸"]
            has_time_reference = any(keyword in claim for keyword in time_keywords)
            
            if not has_time_reference:
                return False, None
            
            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‚ ì§œ ì •ë³´ ìˆ˜ì§‘
            recent_info = []
            for result in search_results:
                if result.published_date:
                    recent_info.append(f"{result.title}: {result.published_date}")
            
            if not recent_info:
                return False, None
            
            # LLMìœ¼ë¡œ ìµœì‹ ì„± ë¶„ì„
            prompt = f"""ë‹¤ìŒ ì£¼ì¥ì´ í˜„ì¬ ì‹œì ì—ì„œ ìµœì‹  ì •ë³´ì¸ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì£¼ì¥: {claim}

ìµœê·¼ ì •ë³´:
{chr(10).join(recent_info)}

í˜„ì¬ ë‚ ì§œ: {datetime.now().strftime('%Yë…„ %mì›”')}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
ìµœì‹ ì„±: [ìµœì‹ /êµ¬ì‹/ë¶ˆë¶„ëª…]
ë§ˆì§€ë§‰ì—…ë°ì´íŠ¸: [ì˜ˆìƒ ë‚ ì§œ ë˜ëŠ” "ë¶ˆëª…"]
ì´ìœ : [íŒë‹¨ ê·¼ê±°]"""
            
            response = await self.llm.acomplete(prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            lines = response.text.strip().split('\n')
            is_outdated = False
            last_updated = None
            
            for line in lines:
                if line.startswith("ìµœì‹ ì„±:"):
                    status = line.split(":", 1)[1].strip().lower()
                    if "êµ¬ì‹" in status or "outdated" in status:
                        is_outdated = True
                
                elif line.startswith("ë§ˆì§€ë§‰ì—…ë°ì´íŠ¸:"):
                    last_updated = line.split(":", 1)[1].strip()
                    if last_updated == "ë¶ˆëª…":
                        last_updated = None
            
            return is_outdated, last_updated
            
        except Exception as e:
            logger.warning(f"ìµœì‹ ì„± ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}")
            return False, None
    
    def get_source_credibility_score(self, url: str) -> float:
        """ì†ŒìŠ¤ì˜ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        domain = self._extract_domain(url)
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ ì²´í¬
        for category, sources in self.trusted_sources.items():
            for trusted_source in sources:
                if trusted_source in domain:
                    if category == "academic":
                        return 0.9
                    elif category == "official":
                        return 0.85
                    elif category == "news":
                        return 0.8
                    elif category == "technology":
                        return 0.75
        
        # ì¼ë°˜ ì›¹ì‚¬ì´íŠ¸
        if domain.endswith('.edu') or domain.endswith('.gov'):
            return 0.85
        elif domain.endswith('.org'):
            return 0.7
        else:
            return 0.5


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_fact_check_agent():
    """FactCheckAgent í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª FactCheckAgent í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    import os
    from dotenv import load_dotenv
    from pathlib import Path
    env_path = Path(__file__).resolve().parents[2] / '.env.dev'
    load_dotenv(env_path)
    
    openai_key = os.getenv("OPENAI_API_KEY")
    serpapi_key = os.getenv("SERPAPI_API_KEY")  # í•„ìš” ì‹œ ì¶”ê°€
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = FactCheckAgent(
        openai_api_key=openai_key,
        serpapi_key=serpapi_key
    )
    
    # í…ŒìŠ¤íŠ¸ ì£¼ì¥ë“¤
    test_claims = [
        "GPT-4ëŠ” 2023ë…„ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤",
        "í•œêµ­ì˜ ì¸êµ¬ëŠ” ì•½ 5ì²œë§Œëª…ì…ë‹ˆë‹¤",
        "ìµœì‹  ì—°êµ¬ì— ë”°ë¥´ë©´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì •í™•ë„ê°€ 95%ë¥¼ ë„˜ì—ˆìŠµë‹ˆë‹¤",
        "ChatGPTëŠ” í˜„ì¬ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "ì‘ë…„ AI ì‹œì¥ ê·œëª¨ëŠ” 100ì¡°ì›ì„ ë„˜ì—ˆìŠµë‹ˆë‹¤"
    ]
    
    print(f"\nğŸ” {len(test_claims)}ê°œ ì£¼ì¥ ê²€ì¦ ì¤‘...")
    
    for i, claim in enumerate(test_claims, 1):
        print(f"\n[{i}] ê²€ì¦ ì¤‘: {claim}")
        
        try:
            result = await agent.verify_single_claim(claim)
            
            print(f"    ê²°ê³¼: {'âœ… ì‚¬ì‹¤' if result.is_factual else 'âŒ ê±°ì§“/ì˜ì‹¬'}")
            print(f"    ì‹ ë¢°ë„: {result.confidence:.2f}")
            print(f"    ê·¼ê±°: {result.explanation[:100]}...")
            if result.sources:
                print(f"    ì¶œì²˜: {len(result.sources)}ê°œ")
        
        except Exception as e:
            print(f"    âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
    
    # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ í…ŒìŠ¤íŠ¸ (ë”ë¯¸)
    print(f"\nğŸ“„ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì‚¬ì‹¤ ê²€ì¦ í…ŒìŠ¤íŠ¸...")
    
    from src.core.models import DocumentMeta, PageInfo, PageElement, ElementType, ImageElement, generate_doc_id
    
    # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±
    doc_meta = DocumentMeta(
        doc_id=generate_doc_id("fact_test.pdf"),
        title="ì‚¬ì‹¤ ê²€ì¦ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ",
        doc_type="pdf",
        total_pages=1,
        file_path="fact_test.pdf"
    )
    
    # ì‚¬ì‹¤ ê²€ì¦ ëŒ€ìƒì´ í¬í•¨ëœ í˜ì´ì§€
    test_page = PageInfo(
        page_id="p001",
        page_number=1,
        raw_text="ìµœì‹  ì—°êµ¬ì— ë”°ë¥´ë©´ GPT-4ì˜ ì„±ëŠ¥ì´ ì´ì „ ëª¨ë¸ë³´ë‹¤ 40% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. 2024ë…„ í˜„ì¬ AI ì‹œì¥ì€ ê¸‰ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        word_count=25,
        elements=[]
    )
    
    print("    ë¬¸ì„œ ë‚´ ì‚¬ì‹¤ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
    
    try:
        issues = await agent.check_document_facts(doc_meta, [test_page])
        
        print(f"    ë°œê²¬ëœ ì‚¬ì‹¤ ì´ìŠˆ: {len(issues)}ê°œ")
        
        for issue in issues:
            print(f"      - {issue.message[:80]}...")
            print(f"        ì‹ ë¢°ë„: {issue.confidence:.2f}")
            print(f"        ì œì•ˆ: {issue.suggestion[:60]}...")
    
    except Exception as e:
        print(f"    âŒ ë¬¸ì„œ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
    
    print("\nğŸ‰ FactCheckAgent í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


async def test_fact_check_integration():
    """FactCheckAgentì™€ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª FactCheckAgent í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    import os
    from dotenv import load_dotenv
    from pathlib import Path
    env_path = Path(__file__).resolve().parents[2] / '.env.dev'
    load_dotenv(env_path)
    
    # ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”
    from src.agents.document_agent import MultimodalDocumentAgent
    from src.agents.quality_agent import MultimodalQualityAgent
    
    openai_key = os.getenv("OPENAI_API_KEY")
    
    document_agent = MultimodalDocumentAgent(openai_api_key=openai_key)
    quality_agent = MultimodalQualityAgent(openai_api_key=openai_key)
    fact_agent = FactCheckAgent(openai_api_key=openai_key)
    
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë¬¸ì„œ ìƒì„± (ì‹¤ì œ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°)
    from src.core.models import (
        DocumentMeta, PageInfo, PageElement, ElementType, 
        ImageElement, BoundingBox, generate_doc_id
    )
    
    doc_meta = DocumentMeta(
        doc_id=generate_doc_id("integrated_test.pdf"),
        title="í†µí•© í…ŒìŠ¤íŠ¸ ë¬¸ì„œ",
        doc_type="pdf", 
        total_pages=1,
        file_path="integrated_test.pdf"
    )
    
    # ì‚¬ì‹¤ ê²€ì¦ì´ í•„ìš”í•œ ë‚´ìš©ì„ í¬í•¨í•œ í˜ì´ì§€
    image_element = PageElement(
        element_id="p001_image_001",
        element_type=ElementType.IMAGE,
        bbox=BoundingBox(x=100, y=200, width=300, height=200),
        image_data=ImageElement(
            element_id="p001_image_001",
            bbox=BoundingBox(x=100, y=200, width=300, height=200),
            format="png",
            size_bytes=12345,
            dimensions=(300, 200),
            ocr_text="2023ë…„ ì—°êµ¬ì— ë”°ë¥´ë©´ ChatGPT ì‚¬ìš©ìëŠ” 1ì–µëª…ì„ ë„˜ì—ˆìŠµë‹ˆë‹¤",
            description="AI ì‚¬ìš©ì í†µê³„ ì°¨íŠ¸"
        )
    )
    
    test_page = PageInfo(
        page_id="p001",
        page_number=1,
        raw_text="ìµœì‹  ì—°êµ¬ì— ë”°ë¥´ë©´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì •í™•ë„ê°€ 99%ì— ë‹¬í•©ë‹ˆë‹¤. GPT-4ëŠ” í˜„ì¬ ê°€ì¥ ê°•ë ¥í•œ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.",
        word_count=20,
        elements=[image_element]
    )
    
    print("\nğŸ” í†µí•© ë¶„ì„ ì‹¤í–‰...")
    
    try:
        # 1. í’ˆì§ˆ ê²€ì‚¬
        print("1. í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰...")
        quality_issues = await quality_agent.check_document(doc_meta, [test_page])
        print(f"   í’ˆì§ˆ ì´ìŠˆ: {len(quality_issues)}ê°œ")
        
        # 2. ì‚¬ì‹¤ ê²€ì¦
        print("2. ì‚¬ì‹¤ ê²€ì¦ ì‹¤í–‰...")
        fact_issues = await fact_agent.check_document_facts(doc_meta, [test_page])
        print(f"   ì‚¬ì‹¤ ì´ìŠˆ: {len(fact_issues)}ê°œ")
        
        # 3. í†µí•© ê²°ê³¼ ë¶„ì„
        all_issues = quality_issues + fact_issues
        
        print(f"\nğŸ“Š í†µí•© ê²°ê³¼:")
        print(f"   ì´ ì´ìŠˆ: {len(all_issues)}ê°œ")
        
        issue_by_type = {}
        for issue in all_issues:
            issue_type = issue.issue_type.value
            if issue_type not in issue_by_type:
                issue_by_type[issue_type] = []
            issue_by_type[issue_type].append(issue)
        
        for issue_type, issues in issue_by_type.items():
            print(f"   {issue_type}: {len(issues)}ê°œ")
            for issue in issues[:2]:  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                print(f"     - {issue.message[:60]}...")
        
        # 4. ë©€í‹°ëª¨ë‹¬ + ì‚¬ì‹¤ê²€ì¦ íŠ¹í™” ë¶„ì„
        multimodal_fact_issues = [
            issue for issue in fact_issues 
            if issue.element_id is not None
        ]
        
        print(f"\nğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬ ì‚¬ì‹¤ ê²€ì¦:")
        print(f"   ë©€í‹°ëª¨ë‹¬ ìš”ì†Œì˜ ì‚¬ì‹¤ ì´ìŠˆ: {len(multimodal_fact_issues)}ê°œ")
        
        for issue in multimodal_fact_issues:
            print(f"     ìš”ì†Œ {issue.element_id}: {issue.message[:50]}...")
    
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ‰ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "integration":
        asyncio.run(test_fact_check_integration())
    else:
        asyncio.run(test_fact_check_agent())