"""
EDU-Audit Fact Check Agent - Efficient & Selective
ì„ íƒì  íŒ©íŠ¸ì²´í‚¹ ì—ì´ì „íŠ¸ (LLM í•„í„° â†’ ê²€ìƒ‰ â†’ ëŒ€ì¡° â†’ ì´ìŠˆí™”)
"""

import asyncio
import logging
import json
import re
from datetime import datetime
from typing import List, Dict, Optional, Any, Set
from dataclasses import dataclass

import aiohttp
from openai import AsyncOpenAI

from src.core.models import (
    DocumentMeta, Issue, IssueType, TextLocation,
    generate_issue_id
)

logger = logging.getLogger(__name__)

@dataclass
class FactCheckTrigger:
    """íŒ©íŠ¸ì²´í¬ í•„ìš” ì—¬ë¶€ íŒë‹¨ ê²°ê³¼"""
    factcheck_required: bool
    reason: str
    keywords: List[str]
    confidence: float

@dataclass
class SearchResult:
    """ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼"""
    title: str
    url: str
    snippet: str
    source_domain: str

@dataclass
class FactVerification:
    """íŒ©íŠ¸ì²´í¬ ê²€ì¦ ê²°ê³¼"""
    claim: str
    is_accurate: bool
    is_outdated: bool
    confidence: float
    reasoning: str
    search_results: List[SearchResult]

class FactCheckAgent:
    """
    ì„ íƒì  íŒ©íŠ¸ì²´í‚¹ ì—ì´ì „íŠ¸
    
    íŒŒì´í”„ë¼ì¸:
    1. LLM í•„í„°: íŒ©íŠ¸ì²´í¬ í•„ìš” ì—¬ë¶€ íŒë‹¨
    2. ê²€ìƒ‰ ì‹¤í–‰: í•„ìš”í•œ ê²½ìš°ë§Œ ì™¸ë¶€ ê²€ìƒ‰
    3. ëŒ€ì¡° ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ vs ìŠ¬ë¼ì´ë“œ ë‚´ìš© ë¹„êµ
    4. ì´ìŠˆí™”: ë¬¸ì œê°€ ìžˆëŠ” ê²½ìš°ë§Œ Issue ìƒì„±
    """
    
    def __init__(
        self,
        openai_api_key: Optional[str] = None,
        serpapi_key: Optional[str] = None,
        model: str = "gpt-5-nano",
        max_search_results: int = 5,
        search_timeout: int = 10
    ):
        self.openai_api_key = openai_api_key
        self.serpapi_key = serpapi_key
        self.model = model
        self.max_search_results = max_search_results
        self.search_timeout = search_timeout
        
        if not openai_api_key:
            raise ValueError("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.client = AsyncOpenAI(api_key=openai_api_key)
        
        # HTTP ì„¸ì…˜ (ê²€ìƒ‰ìš©)
        self.session = None
        
        # ìºì‹œ (ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€)
        self.search_cache: Dict[str, List[SearchResult]] = {}
        self.verification_cache: Dict[str, FactVerification] = {}
        
        logger.info(f"FactCheckAgent ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  ëª¨ë¸: {model}")
        logger.info(f"  ê²€ìƒ‰ API: {'í™œì„±í™”' if serpapi_key else 'ë¹„í™œì„±í™”'}")
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.search_timeout)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def analyze_document(self, document_agent, doc_id: str) -> List[Issue]:
        """
        DocumentAgentì—ì„œ ì²˜ë¦¬ëœ ë¬¸ì„œì˜ íŒ©íŠ¸ì²´í‚¹ ë¶„ì„
        
        Args:
            document_agent: DocumentAgent ì¸ìŠ¤í„´ìŠ¤
            doc_id: ë¬¸ì„œ ID
            
        Returns:
            List[Issue]: ë°œê²¬ëœ íŒ©íŠ¸ì²´í‚¹ ì´ìŠˆë“¤
        """
        logger.info(f"íŒ©íŠ¸ì²´í‚¹ ë¶„ì„ ì‹œìž‘: {doc_id}")
        
        # DocumentAgentì—ì„œ ìŠ¬ë¼ì´ë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        doc_meta = document_agent.get_document(doc_id)
        slide_data_list = document_agent.get_slide_data(doc_id)
        
        if not doc_meta or not slide_data_list:
            logger.warning(f"ë¬¸ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
            return []
        
        all_issues = []
        
        async with self:
            # 1ë‹¨ê³„: ê° ìŠ¬ë¼ì´ë“œë³„ íŒ©íŠ¸ì²´í¬ í•„ìš” ì—¬ë¶€ íŒë‹¨
            factcheck_candidates = []
            
            for slide_data in slide_data_list:
                try:
                    trigger = await self._check_factcheck_trigger(slide_data)
                    
                    if trigger.factcheck_required:
                        factcheck_candidates.append({
                            "slide_data": slide_data,
                            "trigger": trigger
                        })
                        logger.info(f"íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ: {slide_data['page_id']} - {trigger.reason}")
                    
                    # API ë ˆì´íŠ¸ ì œí•œ
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.warning(f"íŠ¸ë¦¬ê±° íŒë‹¨ ì‹¤íŒ¨ {slide_data['page_id']}: {str(e)}")
                    continue
            
            logger.info(f"íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ: {len(factcheck_candidates)}/{len(slide_data_list)} ìŠ¬ë¼ì´ë“œ")
            
            # 2ë‹¨ê³„: íŒ©íŠ¸ì²´í¬ í•„ìš”í•œ ìŠ¬ë¼ì´ë“œë§Œ ì²˜ë¦¬
            for candidate in factcheck_candidates:
                try:
                    slide_issues = await self._verify_slide_facts(
                        candidate["slide_data"], 
                        candidate["trigger"],
                        doc_meta
                    )
                    all_issues.extend(slide_issues)
                    
                    # ê²€ìƒ‰ API ë ˆì´íŠ¸ ì œí•œ
                    await asyncio.sleep(0.5)
                    
                except Exception as e:
                    logger.error(f"ìŠ¬ë¼ì´ë“œ íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨ {candidate['slide_data']['page_id']}: {str(e)}")
                    continue
        
        logger.info(f"íŒ©íŠ¸ì²´í‚¹ ë¶„ì„ ì™„ë£Œ: {len(all_issues)}ê°œ ì´ìŠˆ ë°œê²¬")
        return all_issues
    
    async def _check_factcheck_trigger(self, slide_data: Dict[str, Any]) -> FactCheckTrigger:
        """1ë‹¨ê³„: LLMìœ¼ë¡œ íŒ©íŠ¸ì²´í¬ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        
        # ë¶„ì„í•  í…ìŠ¤íŠ¸ ì¤€ë¹„
        analysis_text = ""
        if slide_data.get("caption"):
            analysis_text += f"[ìº¡ì…˜] {slide_data['caption']}"
        
        if slide_data.get("slide_text"):
            analysis_text += f"\n[í…ìŠ¤íŠ¸] {slide_data['slide_text']}"
        
        if not analysis_text.strip():
            return FactCheckTrigger(
                factcheck_required=False,
                reason="ë¶„ì„í•  í…ìŠ¤íŠ¸ ì—†ìŒ",
                keywords=[],
                confidence=1.0
            )
        
        trigger_prompt = f"""ë‹¤ìŒ êµìœ¡ ìŠ¬ë¼ì´ë“œ ë‚´ìš©ì„ ë³´ê³ , ì™¸ë¶€ ê²€ìƒ‰ì„ í†µí•œ ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ìŠ¬ë¼ì´ë“œ ë‚´ìš©:
{analysis_text}

**íŒ©íŠ¸ì²´í¬ê°€ í•„ìš”í•œ ê²½ìš°:**
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, í†µê³„, ë°ì´í„° (ì˜ˆ: "ì‚¬ìš©ìž ìˆ˜ 1ì–µëª…", "ì •í™•ë„ 95%")
- ìµœì‹  ì—°êµ¬ ê²°ê³¼, ë°œí‘œ ë‚´ìš© (ì˜ˆ: "2024ë…„ ì—°êµ¬", "ìµœê·¼ ë°œí‘œ")
- íšŒì‚¬/ì œí’ˆ ì •ë³´, ì¶œì‹œì¼ (ì˜ˆ: "GPT-4 ì¶œì‹œ", "ìƒˆë¡œìš´ ê¸°ëŠ¥")
- ë²•ê·œ, ì •ì±…, í˜„í™© (ì˜ˆ: "í˜„ìž¬ ê·œì œ", "ì •ë¶€ ì •ì±…")
- ì‹œì˜ì„± ìžˆëŠ” ì‚¬ê±´, ë™í–¥ (ì˜ˆ: "ì˜¬í•´ íŠ¸ë Œë“œ", "ìµœê·¼ ë³€í™”")

**íŒ©íŠ¸ì²´í¬ê°€ ë¶ˆí•„ìš”í•œ ê²½ìš°:**
- ìˆ˜í•™ ê³µì‹, ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… (ì˜ˆ: "Î¸ = Î¸ - Î·âˆ‡J(Î¸)")
- ê¸°ë³¸ ê°œë… ì •ì˜ (ì˜ˆ: "ë¨¸ì‹ ëŸ¬ë‹ì´ëž€", "ë¶„ë¥˜ì™€ íšŒê·€")
- ì¼ë°˜ì ì¸ ì„¤ëª…, ì›ë¦¬ (ì˜ˆ: "ì‹ ê²½ë§ êµ¬ì¡°", "í•™ìŠµ ê³¼ì •")
- ì˜ˆì‹œ, ë¹„ìœ , êµìœ¡ì  ì„¤ëª…

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "factcheck_required": true|false,
    "reason": "íŒë‹¨ ì´ìœ  (í•œ ë¬¸ìž¥)",
    "keywords": ["ê²€ìƒ‰í• ", "í•µì‹¬", "í‚¤ì›Œë“œ"],
    "confidence": 0.0-1.0
}}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": trigger_prompt}
                ],
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹±
            if response_text.startswith("```json"):
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif response_text.startswith("```"):
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            
            return FactCheckTrigger(
                factcheck_required=result.get("factcheck_required", False),
                reason=result.get("reason", "ì´ìœ  ì—†ìŒ"),
                keywords=result.get("keywords", []),
                confidence=result.get("confidence", 0.5)
            )
            
        except json.JSONDecodeError as e:
            logger.warning(f"íŠ¸ë¦¬ê±° íŒë‹¨ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:100]}... - {str(e)}")
            return FactCheckTrigger(
                factcheck_required=False,
                reason="ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨",
                keywords=[],
                confidence=0.1
            )
        except Exception as e:
            logger.error(f"íŠ¸ë¦¬ê±° íŒë‹¨ ì‹¤íŒ¨: {str(e)}")
            return FactCheckTrigger(
                factcheck_required=False,
                reason="íŒë‹¨ ê³¼ì • ì˜¤ë¥˜",
                keywords=[],
                confidence=0.1
            )
    
    async def _verify_slide_facts(
        self, 
        slide_data: Dict[str, Any], 
        trigger: FactCheckTrigger,
        doc_meta: DocumentMeta
    ) -> List[Issue]:
        """2-4ë‹¨ê³„: ê²€ìƒ‰ â†’ ëŒ€ì¡° â†’ ì´ìŠˆí™”"""
        
        issues = []
        
        # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¤€ë¹„
        search_queries = self._prepare_search_queries(slide_data, trigger)
        
        for query in search_queries:
            try:
                # ìºì‹œ í™•ì¸
                cache_key = f"{query}:{slide_data['page_id']}"
                if cache_key in self.verification_cache:
                    verification = self.verification_cache[cache_key]
                    logger.info(f"ìºì‹œì—ì„œ ê²€ì¦ ê²°ê³¼ ì‚¬ìš©: {query}")
                else:
                    # 2ë‹¨ê³„: ì™¸ë¶€ ê²€ìƒ‰
                    search_results = await self._search_external(query)
                    
                    # 3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ì™€ ìŠ¬ë¼ì´ë“œ ë‚´ìš© ëŒ€ì¡°
                    verification = await self._compare_with_search_results(
                        slide_data, query, search_results
                    )
                    
                    # ìºì‹œì— ì €ìž¥
                    self.verification_cache[cache_key] = verification
                
                # 4ë‹¨ê³„: ë¬¸ì œê°€ ìžˆëŠ” ê²½ìš° ì´ìŠˆ ìƒì„±
                if not verification.is_accurate or verification.is_outdated:
                    issue = self._create_fact_issue(
                        slide_data, verification, doc_meta
                    )
                    issues.append(issue)
                
            except Exception as e:
                logger.warning(f"íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨ '{query}': {str(e)}")
                continue
        
        return issues
    
    def _prepare_search_queries(self, slide_data: Dict[str, Any], trigger: FactCheckTrigger) -> List[str]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„"""
        queries = []
        
        # íŠ¸ë¦¬ê±°ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œ ì‚¬ìš©
        if trigger.keywords:
            # í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            main_keywords = " ".join(trigger.keywords[:3])  # ìƒìœ„ 3ê°œë§Œ
            queries.append(main_keywords)
        
        # ìº¡ì…˜ì—ì„œ ìˆ«ìžë‚˜ êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ
        text_content = ""
        if slide_data.get("caption"):
            text_content += slide_data["caption"]
        if slide_data.get("slide_text"):
            text_content += " " + slide_data["slide_text"]
        
        # ìˆ«ìžê°€ í¬í•¨ëœ ë¬¸ìž¥ ì¶”ì¶œ
        sentences = re.split(r'[.!?]\s+', text_content)
        for sentence in sentences:
            if re.search(r'\d', sentence) and len(sentence) > 10:
                # ë¬¸ìž¥ì—ì„œ í•µì‹¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì²˜ìŒ 50ìž)
                clean_sentence = sentence.strip()[:50]
                if clean_sentence not in [q[:50] for q in queries]:
                    queries.append(clean_sentence)
        
        return queries[:2]  # ìµœëŒ€ 2ê°œ ì¿¼ë¦¬ë§Œ
    
    async def _search_external(self, query: str) -> List[SearchResult]:
        """2ë‹¨ê³„: ì™¸ë¶€ ê²€ìƒ‰ ì‹¤í–‰"""
        
        # ìºì‹œ í™•ì¸
        if query in self.search_cache:
            logger.info(f"ê²€ìƒ‰ ìºì‹œ ì‚¬ìš©: {query}")
            return self.search_cache[query]
        
        search_results = []
        
        try:
            if self.serpapi_key and self.session:
                search_results = await self._search_with_serpapi(query)
            else:
                # ê²€ìƒ‰ APIê°€ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ê²°ê³¼
                logger.warning("ê²€ìƒ‰ API í‚¤ê°€ ì—†ì–´ ë”ë¯¸ ê²°ê³¼ ì‚¬ìš©")
                search_results = self._generate_dummy_results(query)
            
            # ìºì‹œì— ì €ìž¥
            self.search_cache[query] = search_results
            
        except Exception as e:
            logger.warning(f"ì™¸ë¶€ ê²€ìƒ‰ ì‹¤íŒ¨ '{query}': {str(e)}")
            search_results = self._generate_dummy_results(query)
        
        return search_results
    
    async def _search_with_serpapi(self, query: str) -> List[SearchResult]:
        """SerpAPIë¥¼ í†µí•œ Google ê²€ìƒ‰"""
        
        try:
            url = "https://serpapi.com/search"
            params = {
                "q": query,
                "api_key": self.serpapi_key,
                "engine": "google",
                "num": self.max_search_results,
                "gl": "kr",  # í•œêµ­ ê²°ê³¼
                "hl": "ko"   # í•œêµ­ì–´
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status != 200:
                    logger.warning(f"SerpAPI ì‘ë‹µ ì˜¤ë¥˜: {response.status}")
                    return []
                
                data = await response.json()
                results = []
                
                for item in data.get("organic_results", []):
                    result = SearchResult(
                        title=item.get("title", ""),
                        url=item.get("link", ""),
                        snippet=item.get("snippet", ""),
                        source_domain=self._extract_domain(item.get("link", ""))
                    )
                    results.append(result)
                
                logger.info(f"SerpAPI ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
                return results
                
        except Exception as e:
            logger.warning(f"SerpAPI ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _generate_dummy_results(self, query: str) -> List[SearchResult]:
        """ë”ë¯¸ ê²€ìƒ‰ ê²°ê³¼ (í…ŒìŠ¤íŠ¸/ë°ëª¨ìš©)"""
        return [
            SearchResult(
                title=f"Search result for: {query}",
                url="https://example.com/search",
                snippet="ì´ ì •ë³´ëŠ” ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                source_domain="example.com"
            )
        ]
    
    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse
            return urlparse(url).netloc.replace("www.", "")
        except:
            return ""
    
    async def _compare_with_search_results(
        self, 
        slide_data: Dict[str, Any], 
        query: str,
        search_results: List[SearchResult]
    ) -> FactVerification:
        """3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ì™€ ìŠ¬ë¼ì´ë“œ ë‚´ìš© ëŒ€ì¡°"""
        
        if not search_results:
            return FactVerification(
                claim=query,
                is_accurate=True,  # ê²€ìƒ‰ ê²°ê³¼ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
                is_outdated=False,
                confidence=0.1,
                reasoning="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê²€ì¦í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                search_results=[]
            )
        
        # ìŠ¬ë¼ì´ë“œ ë‚´ìš© ì •ë¦¬
        slide_content = ""
        if slide_data.get("caption"):
            slide_content += f"ìº¡ì…˜: {slide_data['caption']}\n"
        if slide_data.get("slide_text"):
            slide_content += f"í…ìŠ¤íŠ¸: {slide_data['slide_text']}\n"
        
        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        search_summary = "\n".join([
            f"ì¶œì²˜ {i+1} ({result.source_domain}): {result.title}\n{result.snippet}"
            for i, result in enumerate(search_results[:3])
        ])
        
        comparison_prompt = f"""ë‹¤ìŒ ìŠ¬ë¼ì´ë“œ ë‚´ìš©ê³¼ ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì‚¬ì‹¤ ì •í™•ì„±ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.

ìŠ¬ë¼ì´ë“œ ë‚´ìš©:
{slide_content}

ê²€ìƒ‰ ê²°ê³¼:
{search_summary}

í˜„ìž¬ ë‚ ì§œ: {datetime.now().strftime('%Yë…„ %mì›”')}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”:
1. ì •í™•ì„±: ìŠ¬ë¼ì´ë“œì˜ ì •ë³´ê°€ ê²€ìƒ‰ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
2. ìµœì‹ ì„±: ìŠ¬ë¼ì´ë“œì˜ ì •ë³´ê°€ í˜„ìž¬ ì‹œì ì—ì„œ ìµœì‹ ì¸ê°€?

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "is_accurate": true|false,
    "is_outdated": true|false,
    "confidence": 0.0-1.0,
    "reasoning": "íŒë‹¨ ê·¼ê±° (2-3ë¬¸ìž¥)"
}}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": comparison_prompt}
                ],
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹±
            if response_text.startswith("```json"):
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif response_text.startswith("```"):
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            
            return FactVerification(
                claim=query,
                is_accurate=result.get("is_accurate", True),
                is_outdated=result.get("is_outdated", False),
                confidence=result.get("confidence", 0.5),
                reasoning=result.get("reasoning", "ê²€ì¦ ì™„ë£Œ"),
                search_results=search_results
            )
            
        except json.JSONDecodeError as e:
            logger.warning(f"ëŒ€ì¡° ê²°ê³¼ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:100]}... - {str(e)}")
            return FactVerification(
                claim=query,
                is_accurate=True,
                is_outdated=False,
                confidence=0.1,
                reasoning="ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨",
                search_results=search_results
            )
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê²°ê³¼ ëŒ€ì¡° ì‹¤íŒ¨: {str(e)}")
            return FactVerification(
                claim=query,
                is_accurate=True,
                is_outdated=False,
                confidence=0.1,
                reasoning="ëŒ€ì¡° ê³¼ì • ì˜¤ë¥˜",
                search_results=search_results
            )
    
    def _create_fact_issue(
        self, 
        slide_data: Dict[str, Any], 
        verification: FactVerification,
        doc_meta: DocumentMeta
    ) -> Issue:
        """4ë‹¨ê³„: íŒ©íŠ¸ì²´í‚¹ ì´ìŠˆ ìƒì„±"""
        
        # ì´ìŠˆ ë©”ì‹œì§€ êµ¬ì„±
        if not verification.is_accurate:
            message = f"ì‚¬ì‹¤ ì •í™•ì„± ì˜ì‹¬: {verification.reasoning}"
            suggestion = "ì™¸ë¶€ ì¶œì²˜ë¥¼ í™•ì¸í•˜ì—¬ ì •ë³´ë¥¼ ê²€ì¦í•˜ì„¸ìš”."
        else:  # is_outdated
            message = f"ì •ë³´ ìµœì‹ ì„± ë¬¸ì œ: {verification.reasoning}"
            suggestion = "ìµœì‹  ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
        
        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ëŠ” ë”ë¯¸ë¡œ ì„¤ì • (ìº¡ì…˜ ê¸°ë°˜ì´ë¯€ë¡œ ì •í™•í•œ ìœ„ì¹˜ íŒŒì•… ì–´ë ¤ì›€)
        text_location = TextLocation(start=0, end=len(verification.claim))
        
        issue_id = generate_issue_id(
            doc_meta.doc_id,
            slide_data["page_id"],
            text_location,
            IssueType.FACT
        )
        
        return Issue(
            issue_id=issue_id,
            doc_id=doc_meta.doc_id,
            page_id=slide_data["page_id"],
            issue_type=IssueType.FACT,
            text_location=text_location,
            bbox_location=None,
            element_id=None,
            original_text=verification.claim,
            message=message,
            suggestion=suggestion,
            confidence=verification.confidence,
            confidence_level="medium",
            agent_name="fact_check_agent"
        )
    
    def get_factcheck_summary(self, issues: List[Issue]) -> Dict[str, Any]:
        """íŒ©íŠ¸ì²´í‚¹ ê²°ê³¼ ìš”ì•½"""
        if not issues:
            return {
                "total_fact_issues": 0,
                "accuracy_issues": 0,
                "outdated_issues": 0,
                "avg_confidence": 0.0,
                "recommendations": ["íŒ©íŠ¸ì²´í‚¹ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."]
            }
        
        # ì´ìŠˆ ë¶„ë¥˜
        accuracy_issues = 0
        outdated_issues = 0
        total_confidence = 0
        
        for issue in issues:
            if "ì •í™•ì„±" in issue.message:
                accuracy_issues += 1
            elif "ìµœì‹ ì„±" in issue.message:
                outdated_issues += 1
            
            total_confidence += issue.confidence
        
        avg_confidence = total_confidence / len(issues) if issues else 0
        
        # ê¶Œìž¥ì‚¬í•­
        recommendations = []
        if accuracy_issues > 0:
            recommendations.append(f"ì‚¬ì‹¤ ì •í™•ì„± ê²€í† ê°€ í•„ìš”í•œ í•­ëª©ì´ {accuracy_issues}ê°œ ìžˆìŠµë‹ˆë‹¤.")
        if outdated_issues > 0:
            recommendations.append(f"ì •ë³´ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ í•­ëª©ì´ {outdated_issues}ê°œ ìžˆìŠµë‹ˆë‹¤.")
        
        return {
            "total_fact_issues": len(issues),
            "accuracy_issues": accuracy_issues,
            "outdated_issues": outdated_issues,
            "avg_confidence": avg_confidence,
            "recommendations": recommendations or ["íŒ©íŠ¸ì²´í‚¹ ì™„ë£Œ"]
        }


# E2E í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_fact_check_agent_e2e():
    """FactCheckAgent E2E í…ŒìŠ¤íŠ¸"""
    print("ðŸ§ª FactCheckAgent E2E í…ŒìŠ¤íŠ¸ ì‹œìž‘...")
    
    import os
    from dotenv import load_dotenv
    from pathlib import Path
    
    env_path = Path(__file__).resolve().parents[2] / '.env.dev'
    load_dotenv(env_path)
    
    openai_key = os.getenv("OPENAI_API_KEY")
    serpapi_key = os.getenv("SERPAPI_API_KEY")  # ì„ íƒì‚¬í•­
    
    if not openai_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # 1. DocumentAgentë¡œ ë¬¸ì„œ ì²˜ë¦¬
        print("ðŸ“– DocumentAgentë¡œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
        from src.agents.document_agent import DocumentAgent
        
        document_agent = DocumentAgent(
            openai_api_key=openai_key,
            vision_model="gpt-5-nano"
        )
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°
        test_files = ["sample_docs/sample.pdf"]
        test_file = None
        
        for file_name in test_files:
            if Path(file_name).exists():
                test_file = file_name
                break
        
        if not test_file:
            print("âŒ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   Mock ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # Mock DocumentAgent
            class MockDocumentAgent:
                def get_document(self, doc_id):
                    from src.core.models import DocumentMeta
                    return DocumentMeta(
                        doc_id=doc_id,
                        title="íŒ©íŠ¸ì²´í¬ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ",
                        doc_type="pdf",
                        total_pages=3,
                        file_path="test.pdf"
                    )
                
                def get_slide_data(self, doc_id):
                    return [
                        {
                            "doc_id": doc_id,
                            "page_id": "p001",
                            "page_number": 1,
                            "caption": "GPT-4ëŠ” 2023ë…„ì— OpenAIì—ì„œ ì¶œì‹œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ìž…ë‹ˆë‹¤. ì‚¬ìš©ìž ìˆ˜ëŠ” 1ì–µëª…ì„ ë„˜ì—ˆìŠµë‹ˆë‹¤.",
                            "slide_text": "GPT-4 ì†Œê°œ\n- ì¶œì‹œ: 2023ë…„\n- ê°œë°œì‚¬: OpenAI",
                            "dimensions": (1920, 1080),
                            "size_bytes": 123456
                        },
                        {
                            "doc_id": doc_id,
                            "page_id": "p002",
                            "page_number": 2,
                            "caption": "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ê²½ì‚¬í•˜ê°•ë²•ì€ Î¸ = Î¸ - Î·âˆ‡J(Î¸) ê³µì‹ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.",
                            "slide_text": "ê²½ì‚¬í•˜ê°•ë²•\n- ìˆ˜ì‹: Î¸ = Î¸ - Î·âˆ‡J(Î¸)\n- Î·: í•™ìŠµë¥ ",
                            "dimensions": (1920, 1080),
                            "size_bytes": 98765
                        },
                        {
                            "doc_id": doc_id,
                            "page_id": "p003",
                            "page_number": 3,
                            "caption": "2024ë…„ í•œêµ­ì˜ AI ì‹œìž¥ ê·œëª¨ëŠ” 5ì¡°ì›ì— ë‹¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. ì •ë¶€ ì •ì±…ì— ë”°ë¼ ë³€ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                            "slide_text": "AI ì‹œìž¥ ì „ë§\n- 2024ë…„ ì˜ˆìƒ: 5ì¡°ì›\n- ì •ë¶€ ì •ì±… ì˜í–¥",
                            "dimensions": (1920, 1080),
                            "size_bytes": 87654
                        }
                    ]
            
            document_agent = MockDocumentAgent()
            doc_meta = document_agent.get_document("mock_doc_001")
            
        else:
            # ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬
            print(f"   íŒŒì¼: {test_file}")
            doc_meta = await document_agent.process_document(test_file)
        
        print(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {doc_meta.doc_id}")
        
        # 2. FactCheckAgentë¡œ íŒ©íŠ¸ì²´í‚¹
        print("\nðŸ” FactCheckAgentë¡œ íŒ©íŠ¸ì²´í‚¹ ì¤‘...")
        
        fact_agent = FactCheckAgent(
            openai_api_key=openai_key,
            serpapi_key=serpapi_key,
            model="gpt-5-nano"
        )
        
        issues = await fact_agent.analyze_document(document_agent, doc_meta.doc_id)
        
        print(f"âœ… íŒ©íŠ¸ì²´í‚¹ ì™„ë£Œ!")
        print(f"   ë°œê²¬ëœ ì´ìŠˆ: {len(issues)}ê°œ")
        
        # 3. ê²°ê³¼ ë¶„ì„
        if issues:
            print(f"\nðŸ“‹ íŒ©íŠ¸ì²´í‚¹ ì´ìŠˆë“¤:")
            
            for i, issue in enumerate(issues, 1):
                print(f"\n{i}. [{issue.issue_type.value.upper()}] {issue.page_id}")
                print(f"   ì›ë³¸: {issue.original_text[:60]}...")
                print(f"   ë¬¸ì œ: {issue.message}")
                print(f"   ì œì•ˆ: {issue.suggestion}")
                print(f"   ì‹ ë¢°ë„: {issue.confidence:.2f}")
        else:
            print("\nâœ… íŒ©íŠ¸ì²´í‚¹ ì´ìŠˆê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # 4. ìš”ì•½ ì •ë³´
        summary = fact_agent.get_factcheck_summary(issues)
        print(f"\nðŸ“Š íŒ©íŠ¸ì²´í‚¹ ìš”ì•½:")
        print(f"   ì´ ì´ìŠˆ: {summary['total_fact_issues']}ê°œ")
        print(f"   ì •í™•ì„± ë¬¸ì œ: {summary['accuracy_issues']}ê°œ")
        print(f"   ìµœì‹ ì„± ë¬¸ì œ: {summary['outdated_issues']}ê°œ")
        print(f"   í‰ê·  ì‹ ë¢°ë„: {summary['avg_confidence']:.2f}")
        
        print(f"\nðŸŽ¯ ê¶Œìž¥ì‚¬í•­:")
        for rec in summary['recommendations']:
            print(f"   - {rec}")
        
        # 5. ìºì‹œ ì •ë³´
        print(f"\nðŸ’¾ ìºì‹œ ìƒíƒœ:")
        print(f"   ê²€ìƒ‰ ìºì‹œ: {len(fact_agent.search_cache)}ê°œ")
        print(f"   ê²€ì¦ ìºì‹œ: {len(fact_agent.verification_cache)}ê°œ")
        
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ ìž„í¬íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print("   DocumentAgent í´ëž˜ìŠ¤ì˜ ìž„í¬íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    except Exception as e:
        print(f"âŒ E2E í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\nðŸŽ‰ FactCheckAgent E2E í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


# í†µí•© í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ (DocumentAgent + QualityAgent + FactCheckAgent)"""
    print("ðŸ§ª ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œìž‘...")
    
    import os
    from dotenv import load_dotenv
    from pathlib import Path
    
    env_path = Path(__file__).resolve().parents[2] / '.env.dev'
    load_dotenv(env_path)
    
    openai_key = os.getenv("OPENAI_API_KEY")
    serpapi_key = os.getenv("SERPAPI_API_KEY")
    
    if not openai_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # í…ŒìŠ¤íŠ¸ìš© Mock DocumentAgent (ê°„ë‹¨í•œ ë°ì´í„°)
        class MockDocumentAgent:
            def get_document(self, doc_id):
                from src.core.models import DocumentMeta
                return DocumentMeta(
                    doc_id=doc_id,
                    title="EDU-Audit í†µí•© í…ŒìŠ¤íŠ¸",
                    doc_type="pdf",
                    total_pages=2,
                    file_path="integration_test.pdf"
                )
            
            def get_slide_data(self, doc_id):
                return [
                    {
                        "doc_id": doc_id,
                        "page_id": "p001",
                        "page_number": 1,
                        "caption": "ChatGPTëŠ” 2022ë…„ì— ì¶œì‹œë˜ì—ˆê³  í˜„ìž¬ ì‚¬ìš©ìž ìˆ˜ê°€ 1ì–µëª…ì„ ë„˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ìµœì‹  í†µê³„ìž…ë‹ˆë‹¤.",
                        "slide_text": "ChatGPT í˜„í™©\n- ì¶œì‹œ: 2022ë…„\n- ì‚¬ìš©ìž: 1ì–µëª…+",
                        "image_base64": "dummy_base64_data",
                        "dimensions": (1920, 1080),
                        "size_bytes": 123456
                    },
                    {
                        "doc_id": doc_id,
                        "page_id": "p002", 
                        "page_number": 2,
                        "caption": "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ê²½ì‚¬í•˜ê°•ë²•ì€ Î¸ = Î¸ - Î±âˆ‡J(Î¸) ê³µì‹ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ Î±ëŠ” í•™ìŠµë¥ ìž…ë‹ˆë‹¤.",
                        "slide_text": "ê²½ì‚¬í•˜ê°•ë²• ê³µì‹\nÎ¸ = Î¸ - Î±âˆ‡J(Î¸)",
                        "image_base64": "dummy_base64_data",
                        "dimensions": (1920, 1080),
                        "size_bytes": 98765
                    }
                ]
        
        document_agent = MockDocumentAgent()
        doc_id = "integration_test_001"
        
        print("ðŸ“– Mock ë¬¸ì„œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        
        # 1. QualityAgent ì‹¤í–‰
        print("\nðŸ” QualityAgent ì‹¤í–‰ ì¤‘...")
        try:
            from src.agents.quality_agent import QualityAgent, QualityConfig
            
            quality_config = QualityConfig(
                max_issues_per_slide=2,
                confidence_threshold=0.7,
                issue_severity_filter="medium"
            )
            
            quality_agent = QualityAgent(
                openai_api_key=openai_key,
                vision_model="gpt-5-nano",
                config=quality_config
            )
            
            quality_issues = await quality_agent.analyze_document(document_agent, doc_id)
            print(f"   í’ˆì§ˆ ì´ìŠˆ: {len(quality_issues)}ê°œ")
            
        except ImportError:
            print("   âš ï¸ QualityAgent ìž„í¬íŠ¸ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
            quality_issues = []
        except Exception as e:
            print(f"   âŒ QualityAgent ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            quality_issues = []
        
        # 2. FactCheckAgent ì‹¤í–‰  
        print("\nðŸ” FactCheckAgent ì‹¤í–‰ ì¤‘...")
        
        fact_agent = FactCheckAgent(
            openai_api_key=openai_key,
            serpapi_key=serpapi_key,
            model="gpt-5-nano"
        )
        
        fact_issues = await fact_agent.analyze_document(document_agent, doc_id)
        print(f"   íŒ©íŠ¸ì²´í‚¹ ì´ìŠˆ: {len(fact_issues)}ê°œ")
        
        # 3. í†µí•© ê²°ê³¼ ë¶„ì„
        all_issues = quality_issues + fact_issues
        
        print(f"\nðŸ“Š í†µí•© ë¶„ì„ ê²°ê³¼:")
        print(f"   ì´ ì´ìŠˆ: {len(all_issues)}ê°œ")
        print(f"   í’ˆì§ˆ ì´ìŠˆ: {len(quality_issues)}ê°œ")
        print(f"   íŒ©íŠ¸ì²´í‚¹ ì´ìŠˆ: {len(fact_issues)}ê°œ")
        
        # ì´ìŠˆ íƒ€ìž…ë³„ ë¶„ë¥˜
        issue_by_type = {}
        for issue in all_issues:
            issue_type = issue.issue_type.value
            if issue_type not in issue_by_type:
                issue_by_type[issue_type] = []
            issue_by_type[issue_type].append(issue)
        
        print(f"\nðŸ“ˆ ì´ìŠˆ íƒ€ìž…ë³„ ë¶„í¬:")
        for issue_type, issues in issue_by_type.items():
            print(f"   {issue_type}: {len(issues)}ê°œ")
            
            # ê° íƒ€ìž…ì—ì„œ ëŒ€í‘œ ì´ìŠˆ 1ê°œì”© ì¶œë ¥
            if issues:
                sample_issue = issues[0]
                print(f"     ì˜ˆì‹œ: {sample_issue.message[:50]}...")
        
        # 4. ì—ì´ì „íŠ¸ë³„ ìš”ì•½
        if quality_issues:
            quality_summary = quality_agent.get_quality_summary(quality_issues)
            print(f"\nðŸŽ¯ í’ˆì§ˆ ìš”ì•½:")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {quality_summary['quality_score']:.2f}/1.0")
            for rec in quality_summary['recommendations'][:2]:
                print(f"   - {rec}")
        
        if fact_issues:
            fact_summary = fact_agent.get_factcheck_summary(fact_issues)
            print(f"\nðŸŽ¯ íŒ©íŠ¸ì²´í‚¹ ìš”ì•½:")
            print(f"   í‰ê·  ì‹ ë¢°ë„: {fact_summary['avg_confidence']:.2f}")
            for rec in fact_summary['recommendations'][:2]:
                print(f"   - {rec}")
        
        # 5. ìµœì¢… ê¶Œìž¥ì‚¬í•­
        print(f"\nâœ… ìµœì¢… ê¶Œìž¥ì‚¬í•­:")
        if len(all_issues) == 0:
            print("   ë¬¸ì„œ í’ˆì§ˆê³¼ íŒ©íŠ¸ì²´í‚¹ ëª¨ë‘ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        elif len(quality_issues) > len(fact_issues):
            print("   í’ˆì§ˆ ê°œì„ ì— ìš°ì„  ì§‘ì¤‘í•˜ì‹œê¸° ë°”ëžë‹ˆë‹¤.")
        else:
            print("   ì‚¬ì‹¤ í™•ì¸ ë° ì •ë³´ ì—…ë°ì´íŠ¸ê°€ ìš°ì„  í•„ìš”í•©ë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\nðŸŽ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_fact_check_agent():
    """FactCheckAgent ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
    print("ðŸ§ª FactCheckAgent ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹œìž‘...")
    
    import os
    from dotenv import load_dotenv
    from pathlib import Path
    
    env_path = Path(__file__).resolve().parents[2] / '.env.dev'
    load_dotenv(env_path)
    
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("âŒ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = FactCheckAgent(
        openai_api_key=openai_key,
        serpapi_key=None,  # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ê²€ìƒ‰ ë¹„í™œì„±í™”
        model="gpt-5-nano"
    )
    
    # í…ŒìŠ¤íŠ¸ ìŠ¬ë¼ì´ë“œ ë°ì´í„°
    test_slides = [
        {
            "page_id": "p001",
            "page_number": 1,
            "caption": "GPT-4ëŠ” 2023ë…„ 3ì›”ì— OpenAIê°€ ì¶œì‹œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ìž…ë‹ˆë‹¤.",
            "slide_text": "GPT-4 ì¶œì‹œì¼: 2023ë…„ 3ì›”"
        },
        {
            "page_id": "p002", 
            "page_number": 2,
            "caption": "ë”¥ëŸ¬ë‹ì—ì„œ ì—­ì „íŒŒëŠ” âˆ‚L/âˆ‚w = Î´x ê³µì‹ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.",
            "slide_text": "ì—­ì „íŒŒ ê³µì‹: âˆ‚L/âˆ‚w = Î´x"
        },
        {
            "page_id": "p003",
            "page_number": 3, 
            "caption": "2024ë…„ í•œêµ­ AI íˆ¬ìž ê·œëª¨ëŠ” 10ì¡°ì›ì„ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤.",
            "slide_text": "AI íˆ¬ìž: 10ì¡°ì› ëŒíŒŒ"
        }
    ]
    
    print(f"ðŸ“‹ {len(test_slides)}ê°œ ìŠ¬ë¼ì´ë“œ íŠ¸ë¦¬ê±° í…ŒìŠ¤íŠ¸...")
    
    # 1. íŠ¸ë¦¬ê±° í…ŒìŠ¤íŠ¸
    async with agent:
        for slide in test_slides:
            trigger = await agent._check_factcheck_trigger(slide)
            
            print(f"\nìŠ¬ë¼ì´ë“œ {slide['page_id']}:")
            print(f"   íŒ©íŠ¸ì²´í¬ í•„ìš”: {'âœ…' if trigger.factcheck_required else 'âŒ'}")
            print(f"   ì´ìœ : {trigger.reason}")
            print(f"   í‚¤ì›Œë“œ: {trigger.keywords}")
            print(f"   ì‹ ë¢°ë„: {trigger.confidence:.2f}")
    
    print("\nðŸŽ‰ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "e2e":
            asyncio.run(test_fact_check_agent_e2e())
        elif sys.argv[1] == "pipeline":
            asyncio.run(test_full_pipeline()) 
        elif sys.argv[1] == "unit":
            asyncio.run(test_fact_check_agent())
        else:
            print("ì‚¬ìš©ë²•: python fact_check_agent.py [e2e|pipeline|unit]")
    else:
        asyncio.run(test_fact_check_agent())