"""
EDU-Audit Quality Agent - Simplified
ìŠ¬ë¼ì´ë“œ ìº¡ì…˜ ê¸°ë°˜ êµìœ¡ í’ˆì§ˆ ê²€ìˆ˜ ì—ì´ì „íŠ¸
"""

import asyncio
import logging
import json
import re
from typing import List, Dict, Optional, Any
from dataclasses import dataclass

from openai import AsyncOpenAI

from src.core.models import (
    DocumentMeta, Issue, IssueType, TextLocation,
    generate_issue_id
)

logger = logging.getLogger(__name__)


@dataclass
class QualityConfig:
    """í’ˆì§ˆ ê²€ìˆ˜ ì„¤ì •"""
    max_issues_per_slide: int = 3
    confidence_threshold: float = 0.7
    enable_vision_analysis: bool = False
    issue_severity_filter: str = "medium"  # low, medium, high
    
    # í•„í„°ë§í•  ì´ìŠˆ íƒ€ì…ë“¤ (ë„ˆë¬´ ì‚¬ì†Œí•œ ê²ƒë“¤)
    exclude_minor_issues: List[str] = None
    
    def __post_init__(self):
        if self.exclude_minor_issues is None:
            self.exclude_minor_issues = [
                "missing_period",  # ë¬¸ì¥ ë ë§ˆì¹¨í‘œ ì—†ìŒ
                "whitespace_issues",  # ê³µë°± ë¬¸ì œ
                "minor_formatting"  # ì‚¬ì†Œí•œ í˜•ì‹ ë¬¸ì œ
            ]

class QualityAgent:
    """
    êµìœ¡ìë£Œ í’ˆì§ˆ ê²€ìˆ˜ ì—ì´ì „íŠ¸
    DocumentAgentê°€ ìƒì„±í•œ ìŠ¬ë¼ì´ë“œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 
    ì‹¤ì œ í•™ìŠµì— ì˜í–¥ì„ ì£¼ëŠ” ì¤‘ìš”í•œ í’ˆì§ˆ ì´ìŠˆë§Œ ì„ ë³„í•˜ì—¬ ê²€ì¶œ
    """
    
    def __init__(
        self,
        openai_api_key: Optional[str] = None,
        model: str = "gpt-5-nano",
        vision_model: str = "gpt-5-nano",
        config: Optional[QualityConfig] = None
    ):
        self.openai_api_key = openai_api_key
        if not openai_api_key:
            raise ValueError("OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        self.model = model
        self.vision_model = vision_model
        self.config = config or QualityConfig()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.client = AsyncOpenAI(api_key=openai_api_key)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self._setup_system_prompts()
        
        logger.info(f"QualityAgent ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  Vision ëª¨ë¸: {vision_model} (í•„ìˆ˜ ì‚¬ìš©)")
        logger.info(f"  ìŠ¬ë¼ì´ë“œë‹¹ ìµœëŒ€ ì´ìŠˆ: {self.config.max_issues_per_slide}ê°œ")
        logger.info(f"  ì‹¬ê°ë„ í•„í„°: {self.config.issue_severity_filter}")

    def _passes_severity_filter(self, severity: str) -> bool:
        """ì‹¬ê°ë„ í•„í„° í†µê³¼ í™•ì¸"""
        severity_levels = {"low": 1, "medium": 2, "high": 3}
        filter_level = severity_levels.get(self.config.issue_severity_filter, 2)
        issue_level = severity_levels.get(severity, 2)

        return issue_level >= filter_level
    
    def _setup_system_prompts(self):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        self.system_prompt = """ë‹¹ì‹ ì€ êµìœ¡ìë£Œ í’ˆì§ˆ ê²€ìˆ˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì„ í•¨ê»˜ ë¶„ì„í•˜ì—¬ ì‹¤ì œ í•™ìŠµì— ë°©í•´ê°€ ë  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ ë¬¸ì œë§Œ ì°¾ì•„ë‚´ì„¸ìš”.

**ë¶„ì„ ìš°ì„ ìˆœìœ„:**
1. ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ì§ì ‘ ë¶„ì„ (ë©”ì¸ ì†ŒìŠ¤)
2. ìº¡ì…˜ ì •ë³´ ì°¸ê³  (ë³´ì¡° ì •ë³´)

**ì¤‘ìš”í•œ ì›ì¹™:**
1. ì‚¬ì†Œí•˜ê³  trivialí•œ ë¬¸ì œëŠ” ë¬´ì‹œí•˜ì„¸ìš”
2. í•™ìŠµìì˜ ì´í•´ì— ì‹¤ì§ˆì ìœ¼ë¡œ ì˜í–¥ì„ ì£¼ëŠ” ë¬¸ì œì— ì§‘ì¤‘í•˜ì„¸ìš”
3. í•œ ìŠ¬ë¼ì´ë“œë‹¹ ìµœëŒ€ 3ê°œì˜ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì œë§Œ ì„ ë³„í•˜ì„¸ìš”

**ê²€ì¶œ ëŒ€ìƒ ì´ìŠˆ ìœ í˜•:**
- typo: ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ì˜ ëª…ë°±í•œ ì˜¤íƒˆì
- grammar: ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ì˜ ë¬¸ë²• ì˜¤ë¥˜
- fact: ëª…ë°±í•œ ì‚¬ì‹¤ ì˜¤ë¥˜ë‚˜ ì˜ëª»ëœ ì •ë³´
- image_quality: ì´ë¯¸ì§€ í•´ìƒë„, ì„ ëª…ë„, ê°€ë…ì„± ë¬¸ì œ
- content_clarity: ë‚´ìš© ì „ë‹¬ì˜ ëª…ë£Œì„± ë¬¸ì œ
- layout: ë ˆì´ì•„ì›ƒì´ë‚˜ ë””ìì¸ìœ¼ë¡œ ì¸í•œ ì´í•´ ë°©í•´

**ì¤‘ì  ê²€ì‚¬ í•­ëª©:**
1. ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ì˜ ì˜¤íƒˆì ë° ë¬¸ë²• ì˜¤ë¥˜
2. ë‹¤ì´ì–´ê·¸ë¨, ì°¨íŠ¸, í‘œì˜ ì •í™•ì„±
3. í…ìŠ¤íŠ¸ ê°€ë…ì„± (í¬ê¸°, ëŒ€ë¹„, ë°°ì¹˜)
4. ì •ë³´ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±
5. ì‹œê°ì  ìš”ì†Œì˜ êµìœ¡ì  íš¨ê³¼

**ë¬´ì‹œí•  ë¬¸ì œë“¤:**
- ë‹¨ìˆœ ë„ì–´ì“°ê¸°, ë¬¸ì¥ ë¶€í˜¸ ëˆ„ë½
- ì‚¬ì†Œí•œ í‘œí˜„ ì°¨ì´
- ê°œì¸ì  ì„ í˜¸ë„ ë¬¸ì œ
- ê·¹íˆ ê²½ë¯¸í•œ í˜•ì‹ ë¬¸ì œ

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON ë°°ì—´ í˜•íƒœë¡œ í•´ì£¼ì„¸ìš”:
[
    {
        "issue_type": "typo|grammar|fact|image_quality|content_clarity|layout",
        "original_text": "ë¬¸ì œê°€ ìˆëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ (ì´ë¯¸ì§€ì—ì„œ ë°œê²¬ëœ)",
        "message": "êµ¬ì²´ì ì¸ ë¬¸ì œì  ì„¤ëª…",
        "suggestion": "ìˆ˜ì • ì œì•ˆ",
        "severity": "high|medium|low",
        "confidence": 0.0-1.0,
        "location": "ì´ë¯¸ì§€ ë‚´ ìœ„ì¹˜ ì„¤ëª… (ì˜ˆ: ì œëª©, ë³¸ë¬¸, ì°¨íŠ¸ ë¼ë²¨ ë“±)"
    }
]

ë¬¸ì œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ []ì„ ë°˜í™˜í•˜ì„¸ìš”."""
    
    async def analyze_document(self, document_agent, doc_id: str) -> List[Issue]:
        """
        DocumentAgentì—ì„œ ì²˜ë¦¬ëœ ë¬¸ì„œ ì „ì²´ì˜ í’ˆì§ˆ ê²€ìˆ˜
        
        Args:
            document_agent: DocumentAgent ì¸ìŠ¤í„´ìŠ¤
            doc_id: ë¬¸ì„œ ID
            
        Returns:
            List[Issue]: ë°œê²¬ëœ í’ˆì§ˆ ì´ìŠˆë“¤
        """
        logger.info(f"í’ˆì§ˆ ê²€ìˆ˜ ì‹œì‘: {doc_id}")
        
        # DocumentAgentì—ì„œ ìŠ¬ë¼ì´ë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        doc_meta = document_agent.get_document(doc_id)
        slide_data_list = document_agent.get_slide_data(doc_id)
        
        if not doc_meta or not slide_data_list:
            logger.warning(f"ë¬¸ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
            return []
        
        all_issues = []
        
        # ìŠ¬ë¼ì´ë“œë³„ ë¶„ì„
        for slide_data in slide_data_list:
            try:
                logger.info(f"ìŠ¬ë¼ì´ë“œ ë¶„ì„: {slide_data['page_id']}")
                
                slide_issues = await self._analyze_slide(slide_data, doc_meta)
                all_issues.extend(slide_issues)
                
                # API ë ˆì´íŠ¸ ì œí•œ ê³ ë ¤
                await asyncio.sleep(0.2)
                
            except Exception as e:
                logger.error(f"ìŠ¬ë¼ì´ë“œ ë¶„ì„ ì‹¤íŒ¨ {slide_data['page_id']}: {str(e)}")
                continue
        
        # ë¬¸ì„œ ë ˆë²¨ ì¼ê´€ì„± ê²€ì‚¬
        # document_issues = await self._analyze_document_consistency(slide_data_list, doc_meta)
        # all_issues.extend(document_issues)
        
        # ìµœì¢… í•„í„°ë§
        filtered_issues = self._filter_issues(all_issues)
        
        logger.info(f"í’ˆì§ˆ ê²€ìˆ˜ ì™„ë£Œ: {len(filtered_issues)}/{len(all_issues)}ê°œ ì´ìŠˆ ì„ ë³„")
        return filtered_issues
    
    async def _analyze_slide(self, slide_data: Dict[str, Any], doc_meta: DocumentMeta) -> List[Issue]:
        """ë‹¨ì¼ ìŠ¬ë¼ì´ë“œ í’ˆì§ˆ ë¶„ì„ - ì´ë¯¸ì§€ + ìº¡ì…˜ í†µí•© ë¶„ì„"""
        
        # ì´ë¯¸ì§€ê°€ í•„ìˆ˜
        if not slide_data.get("image_base64"):
            logger.warning(f"ìŠ¬ë¼ì´ë“œ {slide_data['page_id']}ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            # Vision LLMìœ¼ë¡œ ì´ë¯¸ì§€ + ìº¡ì…˜ í†µí•© ë¶„ì„
            issues_data = await self._analyze_slide_with_vision(slide_data)
            
            # Issue ê°ì²´ë¡œ ë³€í™˜
            issues = self._convert_to_issues(issues_data, slide_data, doc_meta)
            
            return issues
            
        except Exception as e:
            logger.error(f"ìŠ¬ë¼ì´ë“œ ë¶„ì„ ì‹¤íŒ¨ {slide_data['page_id']}: {str(e)}")
            return []
    
    async def _analyze_slide_with_vision(self, slide_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Vision ëª¨ë¸ì„ í†µí•œ ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ + ìº¡ì…˜ í†µí•© ë¶„ì„"""
        
        # ìº¡ì…˜ ì •ë³´ ì¤€ë¹„
        caption_info = ""
        if slide_data.get("caption"):
            caption_info += f"\n\n[AI ìƒì„± ìº¡ì…˜ (ì°¸ê³ ìš©)]\n{slide_data['caption']}"
        
        if slide_data.get("slide_text"):
            caption_info += f"\n\n[ìŠ¬ë¼ì´ë“œ ì›ë³¸ í…ìŠ¤íŠ¸ (ì°¸ê³ ìš©)]\n{slide_data['slide_text']}"
        
        analysis_prompt = f"""ì´ êµìœ¡ìë£Œ ìŠ¬ë¼ì´ë“œë¥¼ ë©´ë°€íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ë°©ë²•:**
1. ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë³´ê³  ë¶„ì„í•˜ì„¸ìš” (ì£¼ìš” ì†ŒìŠ¤)
2. ì•„ë˜ ìº¡ì…˜ ì •ë³´ëŠ” ì°¸ê³ ë§Œ í•˜ì„¸ìš” (ë³´ì¡° ì •ë³´)

**ì¤‘ì  ê²€ì‚¬ í•­ëª©:**
1. ì´ë¯¸ì§€ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ì˜ ì˜¤íƒˆì, ë¬¸ë²• ì˜¤ë¥˜
2. ì°¨íŠ¸, í‘œ, ë‹¤ì´ì–´ê·¸ë¨ì˜ ì •í™•ì„±
3. í…ìŠ¤íŠ¸ ê°€ë…ì„± (í¬ê¸°, ëŒ€ë¹„, ìœ„ì¹˜)
4. ì •ë³´ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±
5. ì‹œê°ì  ìš”ì†Œê°€ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥

**íŠ¹íˆ ì£¼ì˜ê¹Šê²Œ í™•ì¸í•  ê²ƒ:**
- ì œëª©, í—¤ë”©ì˜ ì˜¤íƒˆì
- ë³¸ë¬¸ í…ìŠ¤íŠ¸ì˜ ë¬¸ë²• ì˜¤ë¥˜
- ì°¨íŠ¸ë‚˜ í‘œì˜ ë¼ë²¨, ìˆ˜ì¹˜ ì˜¤ë¥˜
- ìš©ì–´ ì‚¬ìš©ì˜ ì¼ê´€ì„±
- í…ìŠ¤íŠ¸ì™€ ì‹œê° ìš”ì†Œ ê°„ì˜ ë¶ˆì¼ì¹˜

ìœ„ì˜ ì‹œìŠ¤í…œ ì§€ì¹¨ì— ë”°ë¼ ì‹¤ì œ í•™ìŠµì— ë°©í•´ê°€ ë  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ ë¬¸ì œë§Œ ì°¾ì•„ì„œ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.{caption_info}"""

        try:
            response = await self.client.chat.completions.create(
                model=self.vision_model,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": analysis_prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{slide_data['image_base64']}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
            )
            
            response_text = response.choices[0].message.content.strip()

            
            # JSON íŒŒì‹±
            if response_text.startswith("```json"):
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif response_text.startswith("```"):
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            issues_data = json.loads(response_text)
            
            # ì´ìŠˆ ê°œìˆ˜ ì œí•œ
            if len(issues_data) > self.config.max_issues_per_slide:
                # ì‹¬ê°ë„ì™€ ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ Nê°œ ì„ íƒ
                issues_data = sorted(
                    issues_data, 
                    key=lambda x: (
                        x.get("severity", "medium") == "high",
                        x.get("confidence", 0.5)
                    ),
                    reverse=True
                )[:self.config.max_issues_per_slide]
            
            return issues_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"Vision LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:100]}... - {str(e)}")
            return []
        except Exception as e:
            logger.error(f"Vision ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _prepare_slide_text(self, slide_data: Dict[str, Any]) -> str:
        """ìŠ¬ë¼ì´ë“œ ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        text_parts = []
        
        # ìº¡ì…˜ (ì£¼ìš” ë¶„ì„ ëŒ€ìƒ)
        if slide_data.get("caption"):
            text_parts.append(f"[ìŠ¬ë¼ì´ë“œ ì„¤ëª…]\n{slide_data['caption']}")
        
        # ì›ë³¸ ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
        if slide_data.get("slide_text"):
            text_parts.append(f"[ìŠ¬ë¼ì´ë“œ ì›ë³¸ í…ìŠ¤íŠ¸]\n{slide_data['slide_text']}")
        
        return "\n\n".join(text_parts)
    
    async def _request_llm_analysis(self, text: str) -> List[Dict[str, Any]]:
        """LLMì— í’ˆì§ˆ ë¶„ì„ ìš”ì²­"""
        
        user_prompt = f"""ë‹¤ìŒ êµìœ¡ìë£Œ ìŠ¬ë¼ì´ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

{text}

ìœ„ì˜ ì‹œìŠ¤í…œ ì§€ì¹¨ì— ë”°ë¼ ì‹¤ì œ í•™ìŠµì— ë°©í•´ê°€ ë  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ ë¬¸ì œë§Œ ì°¾ì•„ì„œ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_prompt}
            ],
        )
        
        response_text = response.choices[0].message.content.strip()
        
        try:
            # JSON íŒŒì‹±
            if response_text.startswith("```json"):
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif response_text.startswith("```"):
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            issues_data = json.loads(response_text)
            
            # ì´ìŠˆ ê°œìˆ˜ ì œí•œ
            if len(issues_data) > self.config.max_issues_per_slide:
                # ì‹¬ê°ë„ì™€ ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ Nê°œ ì„ íƒ
                issues_data = sorted(
                    issues_data, 
                    key=lambda x: (
                        x.get("severity", "medium") == "high",
                        x.get("confidence", 0.5)
                    ),
                    reverse=True
                )[:self.config.max_issues_per_slide]
            
            return issues_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text[:100]}... - {str(e)}")
            return []
    
    def _convert_to_issues(
        self, 
        issues_data: List[Dict[str, Any]], 
        slide_data: Dict[str, Any], 
        doc_meta: DocumentMeta
    ) -> List[Issue]:
        """Vision LLM ì‘ë‹µì„ Issue ê°ì²´ë¡œ ë³€í™˜"""
        
        issues = []
        
        for issue_data in issues_data:
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if not all(key in issue_data for key in ["issue_type", "message"]):
                    logger.warning(f"ì´ìŠˆ ë°ì´í„° í•„ë“œ ëˆ„ë½: {issue_data}")
                    continue
                
                # IssueType ê²€ì¦ (layout ì¶”ê°€ ì²˜ë¦¬)
                issue_type_str = issue_data["issue_type"]
                if issue_type_str == "layout":
                    issue_type_str = "image_quality"  # ê¸°ì¡´ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘
                
                try:
                    issue_type = IssueType(issue_type_str)
                except ValueError:
                    logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ìŠˆ íƒ€ì…: {issue_data['issue_type']}")
                    continue
                
                # ì‹ ë¢°ë„ í•„í„°ë§
                confidence = issue_data.get("confidence", 0.8)
                if confidence < self.config.confidence_threshold:
                    continue
                
                # ì‹¬ê°ë„ í•„í„°ë§
                severity = issue_data.get("severity", "medium")
                if not self._passes_severity_filter(severity):
                    continue
                
                # ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ìœ„ì¹˜ ì •ë³´
                original_text = issue_data.get("original_text", "")
                location_desc = issue_data.get("location", "")
                
                # ì´ë¯¸ì§€ ê¸°ë°˜ ë¶„ì„ì´ë¯€ë¡œ text_locationì€ Noneìœ¼ë¡œ ì„¤ì •
                text_location = None
                
                # Issue ê°ì²´ ìƒì„±
                issue = Issue(
                    issue_id=generate_issue_id(
                        doc_meta.doc_id,
                        slide_data["page_id"],
                        TextLocation(start=0, end=1),  # ë”ë¯¸ ìœ„ì¹˜
                        issue_type
                    ),
                    doc_id=doc_meta.doc_id,
                    page_id=slide_data["page_id"],
                    issue_type=issue_type,
                    text_location=text_location,
                    bbox_location=None,
                    element_id=None,
                    original_text=original_text,
                    message=f"[{location_desc}] {issue_data['message']}" if location_desc else issue_data["message"],
                    suggestion=issue_data.get("suggestion", ""),
                    confidence=confidence,
                    confidence_level="high",  # Pydanticì´ ìë™ ê³„ì‚°
                    agent_name="quality_agent_vision"
                )
                
                issues.append(issue)
                
            except Exception as e:
                logger.warning(f"ì´ìŠˆ ê°ì²´ ìƒì„± ì‹¤íŒ¨: {str(e)} - {issue_data}")
                continue
        
        return issues
    
    def _find_text_location(self, slide_data: Dict[str, Any], original_text: str) -> Optional[TextLocation]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì›ë³¸ ìœ„ì¹˜ ì°¾ê¸°"""
        if not original_text:
            return None
        
        # ìº¡ì…˜ì—ì„œ ì°¾ê¸°
        caption = slide_data.get("caption", "")
        if original_text in caption:
            start = caption.find(original_text)
            return TextLocation(start=start, end=start + len(original_text))
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
        slide_text = slide_data.get("slide_text", "")
        if original_text in slide_text:
            start = slide_text.find(original_text)
            return TextLocation(start=start, end=start + len(original_text))
        
        return None
    
    
#     async def _analyze_document_consistency(self, slide_data_list: List[Dict[str, Any]], doc_meta: DocumentMeta) -> List[Issue]:
#         """ë¬¸ì„œ ì „ì²´ ì¼ê´€ì„± ë¶„ì„ - ìº¡ì…˜ ê¸°ë°˜"""
#         if len(slide_data_list) < 2:
#             return []
        
#         try:
#             # ëª¨ë“  ìº¡ì…˜ ê²°í•© (ì´ë¯¸ì§€ ë¶„ì„ì€ ê°œë³„ ìŠ¬ë¼ì´ë“œì—ì„œ ìˆ˜í–‰)
#             all_captions = []
#             for slide in slide_data_list:
#                 if slide.get("caption"):
#                     all_captions.append(f"ìŠ¬ë¼ì´ë“œ {slide['page_number']}: {slide['caption']}")
            
#             if len(all_captions) < 2:
#                 return []
            
#             combined_text = "\n\n".join(all_captions)
            
#             consistency_prompt = f"""ë‹¤ìŒì€ êµìœ¡ìë£Œì˜ ëª¨ë“  ìŠ¬ë¼ì´ë“œ AI ìƒì„± ìº¡ì…˜ì…ë‹ˆë‹¤:

# {combined_text}

# ë¬¸ì„œ ì „ì²´ì—ì„œ ë‹¤ìŒ ì¼ê´€ì„± ë¬¸ì œë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:
# 1. ë™ì¼í•œ ê°œë…ì— ëŒ€í•œ ë‹¤ë¥¸ ìš©ì–´ ì‚¬ìš© (ì˜ˆ: "ë¨¸ì‹ ëŸ¬ë‹" vs "ê¸°ê³„í•™ìŠµ")
# 2. ì„¤ëª… ìŠ¤íƒ€ì¼ì˜ ì‹¬ê°í•œ ë¶ˆì¼ì¹˜
# 3. ë…¼ë¦¬ì  ìˆœì„œë‚˜ êµ¬ì¡°ì˜ ë¬¸ì œ
# 4. ì „ì²´ì ì¸ êµìœ¡ íë¦„ì˜ ë¬¸ì œ

# ì¤‘ìš”í•œ ì¼ê´€ì„± ë¬¸ì œë§Œ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
# [{{"issue_type": "consistency", "message": "...", "suggestion": "...", "affected_slides": [1, 2, 3], "confidence": 0.0-1.0}}]

# ë¬¸ì œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ []ì„ ë°˜í™˜í•˜ì„¸ìš”."""

#             response = await self.client.chat.completions.create(
#                 model=self.model,
#                 messages=[
#                     {"role": "system", "content": "ë‹¹ì‹ ì€ êµìœ¡ìë£Œ ì¼ê´€ì„± ê²€ìˆ˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
#                     {"role": "user", "content": consistency_prompt}
#                 ],
#             )
            
#             response_text = response.choices[0].message.content.strip()
            
#             # JSON íŒŒì‹±
#             if response_text.startswith("```json"):
#                 response_text = response_text.split("```json")[1].split("```")[0].strip()
#             elif response_text.startswith("```"):
#                 response_text = response_text.split("```")[1].split("```")[0].strip()
            
#             consistency_data = json.loads(response_text)
            
#             # ë¬¸ì„œ ë ˆë²¨ ì´ìŠˆë¡œ ë³€í™˜
#             document_issues = []
#             for issue_data in consistency_data:
#                 if issue_data.get("confidence", 0.8) >= self.config.confidence_threshold:
#                     # ì²« ë²ˆì§¸ ìŠ¬ë¼ì´ë“œì— ì´ìŠˆ í• ë‹¹
#                     first_slide = slide_data_list[0]
                    
#                     issue = Issue(
#                         issue_id=generate_issue_id(
#                             doc_meta.doc_id,
#                             first_slide["page_id"],
#                             TextLocation(start=0, end=1),
#                             IssueType.CONSISTENCY
#                         ),
#                         doc_id=doc_meta.doc_id,
#                         page_id=first_slide["page_id"],
#                         issue_type=IssueType.CONSISTENCY,
#                         text_location=None,
#                         bbox_location=None,
#                         element_id=None,
#                         original_text="ë¬¸ì„œ ì „ì²´",
#                         message=f"[ë¬¸ì„œ ì¼ê´€ì„±] {issue_data['message']}",
#                         suggestion=issue_data.get("suggestion", ""),
#                         confidence=issue_data.get("confidence", 0.8),
#                         confidence_level="medium",
#                         agent_name="quality_agent_consistency"
#                     )
                    
#                     document_issues.append(issue)
            
#             return document_issues
            
#         except Exception as e:
#             logger.warning(f"ë¬¸ì„œ ì¼ê´€ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
#             return []
    
    def _filter_issues(self, issues: List[Issue]) -> List[Issue]:
        """ìµœì¢… ì´ìŠˆ í•„í„°ë§"""
        filtered = []
        
        for issue in issues:
            # ì‹ ë¢°ë„ í•„í„°
            if issue.confidence < self.config.confidence_threshold:
                continue
            
            # ì¤‘ë³µ ì œê±° (ê°™ì€ í˜ì´ì§€, ê°™ì€ íƒ€ì…, ë¹„ìŠ·í•œ ë©”ì‹œì§€)
            is_duplicate = False
            for existing in filtered:
                if (existing.page_id == issue.page_id and 
                    existing.issue_type == issue.issue_type and
                    self._similar_messages(existing.message, issue.message)):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered.append(issue)
        
        # ì‹¬ê°ë„ ê¸°ì¤€ ì •ë ¬
        filtered.sort(key=lambda x: (
            x.issue_type == IssueType.FACT,  # ì‚¬ì‹¤ ì˜¤ë¥˜ ìš°ì„ 
            x.confidence
        ), reverse=True)
        
        return filtered
    
    def _similar_messages(self, msg1: str, msg2: str, threshold: float = 0.8) -> bool:
        """ë©”ì‹œì§€ ìœ ì‚¬ë„ í™•ì¸ (ê°„ë‹¨í•œ ë¬¸ìì—´ ë¹„êµ)"""
        if not msg1 or not msg2:
            return False
        
        # ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„
        words1 = set(msg1.lower().split())
        words2 = set(msg2.lower().split())
        
        if not words1 or not words2:
            return False
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return (intersection / union) > threshold
    
    def get_quality_summary(self, issues: List[Issue]) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²€ìˆ˜ ê²°ê³¼ ìš”ì•½"""
        if not issues:
            return {
                "total_issues": 0,
                "quality_score": 1.0,
                "by_type": {},
                "by_severity": {},
                "recommendations": ["ë¬¸ì„œ í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤."]
            }
        
        # íƒ€ì…ë³„ ë¶„ë¥˜
        by_type = {}
        by_severity = {"high": 0, "medium": 0, "low": 0}
        
        for issue in issues:
            issue_type = issue.issue_type.value
            by_type[issue_type] = by_type.get(issue_type, 0) + 1
            
            if issue.confidence >= 0.9:
                by_severity["high"] += 1
            elif issue.confidence >= 0.7:
                by_severity["medium"] += 1
            else:
                by_severity["low"] += 1
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        quality_score = max(0.0, 1.0 - (len(issues) * 0.1))
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        if by_type.get("fact", 0) > 0:
            recommendations.append("ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•œ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤.")
        if by_type.get("typo", 0) > 0:
            recommendations.append("ì˜¤íƒˆì êµì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        if by_type.get("consistency", 0) > 0:
            recommendations.append("ìš©ì–´ ì‚¬ìš©ì˜ ì¼ê´€ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        if by_type.get("image_quality", 0) > 0:
            recommendations.append("ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”.")
        
        return {
            "total_issues": len(issues),
            "quality_score": quality_score,
            "by_type": by_type,
            "by_severity": by_severity,
            "recommendations": recommendations
        }

async def test_quality_agent_e2e():
    """QualityAgent E2E í…ŒìŠ¤íŠ¸ - DocumentAgentì™€ ì—°ë™"""
    print("ğŸ§ª QualityAgent E2E í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    import os
    from dotenv import load_dotenv
    from pathlib import Path
    
    env_path = Path(__file__).resolve().parents[2] / '.env.dev'
    load_dotenv(env_path)
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ì°¾ê¸°
    test_files = [
        "sample_docs/sample.pdf"
    ]
    
    test_file = None
    for file_name in test_files:
        if Path(file_name).exists():
            test_file = file_name
            break
    
    if not test_file:
        print("âŒ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ë‹¤ìŒ íŒŒì¼ ì¤‘ í•˜ë‚˜ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”: {', '.join(test_files)}")
        return
    
    print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_file}")
    
    try:
        # 1. DocumentAgent ì´ˆê¸°í™” ë° ë¬¸ì„œ ì²˜ë¦¬
        print("\nğŸ”§ DocumentAgent ì´ˆê¸°í™” ì¤‘...")
        from src.agents.document_agent import DocumentAgent  # ì‹¤ì œ ì„í¬íŠ¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •
        
        document_agent = DocumentAgent(
            openai_api_key=api_key,
            vision_model="gpt-5-nano",
            embedding_model="text-embedding-3-small"
        )
        
        print(f"ğŸ“– ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {test_file}")
        doc_meta = await document_agent.process_document(test_file)
        
        print(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ë¬¸ì„œ ID: {doc_meta.doc_id}")
        print(f"   ì œëª©: {doc_meta.title}")
        print(f"   ìŠ¬ë¼ì´ë“œ ìˆ˜: {doc_meta.total_pages}")
        
        # 2. ìŠ¬ë¼ì´ë“œ ë°ì´í„° í™•ì¸
        slide_data_list = document_agent.get_slide_data(doc_meta.doc_id)
        print(f"   ìƒì„±ëœ ìŠ¬ë¼ì´ë“œ ë°ì´í„°: {len(slide_data_list)}ê°œ")
        
        # ì²« ë²ˆì§¸ ìŠ¬ë¼ì´ë“œ ì •ë³´ ì¶œë ¥
        if slide_data_list:
            first_slide = slide_data_list[0]
            print(f"   ì²« ìŠ¬ë¼ì´ë“œ ìº¡ì…˜: {first_slide.get('caption', 'None')[:100]}...")
            print(f"   ì´ë¯¸ì§€ ë°ì´í„°: {'ìˆìŒ' if first_slide.get('image_base64') else 'ì—†ìŒ'}")
        
        # 3. QualityAgent ì´ˆê¸°í™” ë° í’ˆì§ˆ ê²€ìˆ˜
        print("\nğŸ” QualityAgent ì´ˆê¸°í™” ì¤‘...")
        config = QualityConfig(
            max_issues_per_slide=3,
            confidence_threshold=0.7,
            issue_severity_filter="medium"
        )
        
        quality_agent = QualityAgent(
            openai_api_key=api_key,
            vision_model="gpt-5-nano",
            config=config
        )
        
        print("ğŸ“‹ í’ˆì§ˆ ê²€ìˆ˜ ì‹¤í–‰ ì¤‘... (Vision ëª¨ë¸ ì‚¬ìš©)")
        print("   â³ ì´ ê³¼ì •ì€ ëª‡ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        issues = await quality_agent.analyze_document(document_agent, doc_meta.doc_id)
        
        # 4. ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ ë°œê²¬ëœ ì´ìŠˆë“¤ ({len(issues)}ê°œ):")
        
        if not issues:
            print("   ğŸ‰ ë°œê²¬ëœ í’ˆì§ˆ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤!")
        else:
            # ì´ìŠˆ ì¶œë ¥
            for i, issue in enumerate(issues, 1):
                print(f"\n{i}. [{issue.issue_type.value.upper()}] {issue.page_id}")
                print(f"   ì›ë³¸: {issue.original_text[:50]}{'...' if len(issue.original_text) > 50 else ''}")
                print(f"   ë¬¸ì œ: {issue.message}")
                print(f"   ì œì•ˆ: {issue.suggestion}")
                print(f"   ì‹ ë¢°ë„: {issue.confidence:.2f}")
                print(f"   ì—ì´ì „íŠ¸: {issue.agent_name}")
        
        # 5. í’ˆì§ˆ ìš”ì•½
        summary = quality_agent.get_quality_summary(issues)
        print(f"\nğŸ“Š í’ˆì§ˆ ìš”ì•½:")
        print(f"   ì „ì²´ ì´ìŠˆ: {summary['total_issues']}ê°œ")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {summary['quality_score']:.2f}/1.0")
        
        if summary['by_type']:
            print(f"\nğŸ“ˆ ì´ìŠˆ ìœ í˜•ë³„ ë¶„í¬:")
            for issue_type, count in summary['by_type'].items():
                print(f"   {issue_type}: {count}ê°œ")
        
        if summary['by_severity']:
            print(f"\nâš–ï¸ ì‹¬ê°ë„ë³„ ë¶„í¬:")
            for severity, count in summary['by_severity'].items():
                print(f"   {severity}: {count}ê°œ")
        
        print(f"\nğŸ¯ ê¶Œì¥ì‚¬í•­:")
        for rec in summary['recommendations']:
            print(f"   - {rec}")
        
        # 6. ì¶”ê°€ ì •ë³´
        stats = document_agent.get_document_stats(doc_meta.doc_id)
        print(f"\nğŸ“ˆ ë¬¸ì„œ í†µê³„:")
        print(f"   ìº¡ì…˜ ìƒì„±ë¥ : {stats['caption_coverage']:.1%}")
        print(f"   í‰ê·  ìº¡ì…˜ ê¸¸ì´: {stats['avg_caption_length']:.0f}ì")
        print(f"   ì´ ì´ë¯¸ì§€ í¬ê¸°: {stats['total_image_size_mb']:.1f}MB")
        
        print("\nğŸ‰ E2E í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print("   DocumentAgent í´ë˜ìŠ¤ì˜ ì„í¬íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {str(e)}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_quality_agent_e2e())