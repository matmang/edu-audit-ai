"""
EDU-Audit Multimodal Quality Agent
ì˜¤íƒˆì, ë¬¸ë²•, í‘œí˜„ ì¼ê´€ì„± + ë©€í‹°ëª¨ë‹¬ í’ˆì§ˆ ê²€ì¶œ ì—ì´ì „íŠ¸
"""

import asyncio
import logging
import re
from collections import defaultdict, Counter
from typing import List, Dict, Set, Optional, Any
from dataclasses import dataclass

from llama_index.llms.openai import OpenAI

from src.core.models import (
    DocumentMeta, PageInfo, PageElement, ElementType, Issue, IssueType, 
    TextLocation, BoundingBox, ImageElement, TableElement, ChartElement,
    generate_issue_id
)

logger = logging.getLogger(__name__)


@dataclass
class TypoPattern:
    """ì˜¤íƒˆì íŒ¨í„´ ì •ì˜"""
    pattern: str          # ì •ê·œì‹ íŒ¨í„´
    correction: str       # ì˜¬ë°”ë¥¸ í‘œí˜„
    confidence: float     # ì‹ ë¢°ë„
    description: str      # ì„¤ëª…


@dataclass
class ConsistencyRule:
    """ì¼ê´€ì„± ê·œì¹™ ì •ì˜"""
    terms: List[str]      # ë™ì¼ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ë“¤
    preferred: str        # ê¶Œì¥ í‘œí˜„
    description: str      # ì„¤ëª…
    domain: str          # ì ìš© ë„ë©”ì¸


@dataclass
class ImageQualityRule:
    """ì´ë¯¸ì§€ í’ˆì§ˆ ê·œì¹™"""
    min_width: int = 300
    min_height: int = 200
    max_file_size_mb: float = 5.0
    required_dpi: int = 150
    avoid_formats: List[str] = None
    
    def __post_init__(self):
        if self.avoid_formats is None:
            self.avoid_formats = ["bmp", "tiff"]


@dataclass
class TableQualityRule:
    """í‘œ í’ˆì§ˆ ê·œì¹™"""
    min_rows: int = 2
    min_cols: int = 2
    max_empty_cells_ratio: float = 0.3
    require_headers: bool = True
    consistent_column_count: bool = True


@dataclass
class ChartQualityRule:
    """ì°¨íŠ¸ í’ˆì§ˆ ê·œì¹™"""
    require_title: bool = True
    require_axis_labels: bool = True
    require_legend: bool = False
    min_data_points: int = 2
    readable_text_size: bool = True


class MultimodalQualityAgent:
    """ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ í’ˆì§ˆ ê²€ì‚¬ ì—ì´ì „íŠ¸"""
    
    def __init__(
        self, 
        openai_api_key: Optional[str] = None, 
        llm_model: str = "gpt-5-nano",
        vision_model: str = "gpt-4-vision-preview",
        enable_vision_analysis: bool = True
    ):
        self.openai_api_key = openai_api_key
        self.enable_vision_analysis = enable_vision_analysis and openai_api_key
        
        # LLM ì´ˆê¸°í™” (API í‚¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        self.llm = None
        self.vision_llm = None
        
        if openai_api_key:
            self.llm = OpenAI(
                model=llm_model,
                temperature=0.1,
                api_key=openai_api_key
            )
            
            if self.enable_vision_analysis:
                self.vision_llm = OpenAI(
                    model=vision_model,
                    temperature=0.1,
                    api_key=openai_api_key
                )
        
        # ê¸°ì¡´ íŒ¨í„´ê³¼ ê·œì¹™ ë¡œë“œ
        self._load_typo_patterns()
        self._load_consistency_rules()
        
        # ë©€í‹°ëª¨ë‹¬ í’ˆì§ˆ ê·œì¹™ ë¡œë“œ
        self._load_multimodal_rules()
        
        logger.info("MultimodalQualityAgent ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  Vision ë¶„ì„: {'í™œì„±í™”' if self.enable_vision_analysis else 'ë¹„í™œì„±í™”'}")
    
    def _load_typo_patterns(self):
        """ì¼ë°˜ì ì¸ ì˜¤íƒˆì íŒ¨í„´ ë¡œë“œ"""
        self.typo_patterns = [
            # í•œêµ­ì–´ ì˜¤íƒˆì
            TypoPattern(
                pattern=r"ì•Œê³ ë¦¬ë“¬",
                correction="ì•Œê³ ë¦¬ì¦˜",
                confidence=0.95,
                description="ì•Œê³ ë¦¬ë“¬ â†’ ì•Œê³ ë¦¬ì¦˜"
            ),
            TypoPattern(
                pattern=r"ë°ì´íƒ€",
                correction="ë°ì´í„°",
                confidence=0.98,
                description="ë°ì´íƒ€ â†’ ë°ì´í„°"
            ),
            TypoPattern(
                pattern=r"ì»´í“¨íƒ€",
                correction="ì»´í“¨í„°",
                confidence=0.98,
                description="ì»´í“¨íƒ€ â†’ ì»´í“¨í„°"
            ),
            TypoPattern(
                pattern=r"ì•¨ê³ ë¦¬ì¦˜",
                correction="ì•Œê³ ë¦¬ì¦˜",
                confidence=0.90,
                description="ì•¨ê³ ë¦¬ì¦˜ â†’ ì•Œê³ ë¦¬ì¦˜"
            ),
            
            # ì˜ì–´ ì˜¤íƒˆì
            TypoPattern(
                pattern=r"\bteh\b",
                correction="the",
                confidence=0.99,
                description="teh â†’ the"
            ),
            TypoPattern(
                pattern=r"\baccomodate\b",
                correction="accommodate",
                confidence=0.95,
                description="accomodate â†’ accommodate"
            ),
            TypoPattern(
                pattern=r"\brecieve\b",
                correction="receive",
                confidence=0.95,
                description="recieve â†’ receive"
            ),
        ]
    
    def _load_consistency_rules(self):
        """ìš©ì–´ ì¼ê´€ì„± ê·œì¹™ ë¡œë“œ"""
        self.consistency_rules = [
            ConsistencyRule(
                terms=["í•™ìŠµë¥ ", "ëŸ¬ë‹ë ˆì´íŠ¸", "ëŸ¬ë‹ ë ˆì´íŠ¸", "í•™ìŠµì†ë„", "learning rate"],
                preferred="í•™ìŠµë¥ ",
                description="í•™ìŠµë¥  ê´€ë ¨ ìš©ì–´ í†µì¼",
                domain="machine_learning"
            ),
            ConsistencyRule(
                terms=["ë”¥ëŸ¬ë‹", "ì‹¬ì¸µí•™ìŠµ", "ê¹Šì€í•™ìŠµ", "deep learning"],
                preferred="ë”¥ëŸ¬ë‹",
                description="ë”¥ëŸ¬ë‹ ê´€ë ¨ ìš©ì–´ í†µì¼",
                domain="machine_learning"
            ),
            ConsistencyRule(
                terms=["ë°ì´í„°ì…‹", "ë°ì´í„°ì„¸íŠ¸", "ë°ì´í„° ì…‹", "dataset"],
                preferred="ë°ì´í„°ì…‹",
                description="ë°ì´í„°ì…‹ ê´€ë ¨ ìš©ì–´ í†µì¼",
                domain="data_science"
            ),
            ConsistencyRule(
                terms=["ì•Œê³ ë¦¬ì¦˜", "ì•Œê³ ë¦¬ë“¬", "ì•¨ê³ ë¦¬ì¦˜", "algorithm"],
                preferred="ì•Œê³ ë¦¬ì¦˜",
                description="ì•Œê³ ë¦¬ì¦˜ ê´€ë ¨ ìš©ì–´ í†µì¼",
                domain="computer_science"
            ),
            ConsistencyRule(
                terms=["ë¨¸ì‹ ëŸ¬ë‹", "ê¸°ê³„í•™ìŠµ", "ë¨¸ì‹  ëŸ¬ë‹", "machine learning"],
                preferred="ë¨¸ì‹ ëŸ¬ë‹",
                description="ë¨¸ì‹ ëŸ¬ë‹ ê´€ë ¨ ìš©ì–´ í†µì¼",
                domain="machine_learning"
            ),
        ]
    
    def _load_multimodal_rules(self):
        """ë©€í‹°ëª¨ë‹¬ í’ˆì§ˆ ê·œì¹™ ë¡œë“œ"""
        self.image_quality_rules = ImageQualityRule()
        self.table_quality_rules = TableQualityRule()
        self.chart_quality_rules = ChartQualityRule()
    
    async def check_document(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """
        ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì „ì²´ í’ˆì§ˆ ê²€ì‚¬
        
        Args:
            doc_meta: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            pages: í˜ì´ì§€ ëª©ë¡ (ë©€í‹°ëª¨ë‹¬ ìš”ì†Œ í¬í•¨)
            
        Returns:
            List[Issue]: ë°œê²¬ëœ ì´ìŠˆë“¤
        """
        logger.info(f"ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘: {doc_meta.doc_id}")
        
        all_issues = []
        
        # 1. ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ì‚¬
        text_issues = await self._check_text_quality(doc_meta, pages)
        all_issues.extend(text_issues)
        
        # 2. ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì‚¬
        image_issues = await self._check_image_quality(doc_meta, pages)
        all_issues.extend(image_issues)
        
        # 3. í‘œ í’ˆì§ˆ ê²€ì‚¬
        table_issues = await self._check_table_quality(doc_meta, pages)
        all_issues.extend(table_issues)
        
        # 4. ì°¨íŠ¸ í’ˆì§ˆ ê²€ì‚¬
        chart_issues = await self._check_chart_quality(doc_meta, pages)
        all_issues.extend(chart_issues)
        
        # 5. ë©€í‹°ëª¨ë‹¬ ì¼ê´€ì„± ê²€ì‚¬ (OCR í…ìŠ¤íŠ¸ í¬í•¨)
        multimodal_consistency_issues = await self._check_multimodal_consistency(doc_meta, pages)
        all_issues.extend(multimodal_consistency_issues)
        
        logger.info(f"ë©€í‹°ëª¨ë‹¬ í’ˆì§ˆ ê²€ì‚¬ ì™„ë£Œ: {len(all_issues)}ê°œ ì´ìŠˆ ë°œê²¬")
        return all_issues
    
    async def _check_text_quality(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """ê¸°ì¡´ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬"""
        all_issues = []
        
        # 1. íŒ¨í„´ ê¸°ë°˜ ì˜¤íƒˆì ê²€ì‚¬
        typo_issues = await self._check_typos_pattern_based(doc_meta, pages)
        all_issues.extend(typo_issues)
        
        # 2. ìš©ì–´ ì¼ê´€ì„± ê²€ì‚¬
        consistency_issues = await self._check_consistency(doc_meta, pages)
        all_issues.extend(consistency_issues)
        
        # 3. LLM ê¸°ë°˜ ë¬¸ë²• ê²€ì‚¬
        if self.llm:
            grammar_issues = await self._check_grammar_llm(doc_meta, pages)
            all_issues.extend(grammar_issues)
        
        return all_issues
    
    async def _check_image_quality(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì‚¬"""
        issues = []
        
        for page in pages:
            for element in page.elements:
                if element.element_type != ElementType.IMAGE or not element.image_data:
                    continue
                
                image_data = element.image_data
                
                # 1. ì´ë¯¸ì§€ í¬ê¸° ê²€ì‚¬
                if (image_data.dimensions[0] < self.image_quality_rules.min_width or 
                    image_data.dimensions[1] < self.image_quality_rules.min_height):
                    
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.IMAGE_QUALITY,
                        f"ì´ë¯¸ì§€ í•´ìƒë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({image_data.dimensions[0]}x{image_data.dimensions[1]})",
                        f"ìµœì†Œ {self.image_quality_rules.min_width}x{self.image_quality_rules.min_height} ê¶Œì¥",
                        0.8
                    )
                    issues.append(issue)
                
                # 2. íŒŒì¼ í¬ê¸° ê²€ì‚¬
                size_mb = image_data.size_bytes / (1024 * 1024)
                if size_mb > self.image_quality_rules.max_file_size_mb:
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.IMAGE_QUALITY,
                        f"ì´ë¯¸ì§€ íŒŒì¼ í¬ê¸°ê°€ í½ë‹ˆë‹¤ ({size_mb:.1f}MB)",
                        f"ìµœëŒ€ {self.image_quality_rules.max_file_size_mb}MB ê¶Œì¥",
                        0.7
                    )
                    issues.append(issue)
                
                # 3. íŒŒì¼ í˜•ì‹ ê²€ì‚¬
                if image_data.format.lower() in self.image_quality_rules.avoid_formats:
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.IMAGE_QUALITY,
                        f"ê¶Œì¥í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤ ({image_data.format})",
                        "JPG, PNG í˜•ì‹ ì‚¬ìš© ê¶Œì¥",
                        0.6
                    )
                    issues.append(issue)
                
                # 4. Vision LLMì„ í†µí•œ í’ˆì§ˆ ë¶„ì„
                if self.enable_vision_analysis and self.vision_llm:
                    vision_issues = await self._analyze_image_quality_with_vision(
                        doc_meta, page, element
                    )
                    issues.extend(vision_issues)
        
        logger.info(f"ì´ë¯¸ì§€ í’ˆì§ˆ ê²€ì‚¬: {len(issues)}ê°œ ë°œê²¬")
        return issues
    
    async def _check_table_quality(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """í‘œ í’ˆì§ˆ ê²€ì‚¬"""
        issues = []
        
        for page in pages:
            for element in page.elements:
                if element.element_type != ElementType.TABLE or not element.table_data:
                    continue
                
                table_data = element.table_data
                
                # 1. ìµœì†Œ í¬ê¸° ê²€ì‚¬
                if (table_data.row_count < self.table_quality_rules.min_rows or 
                    table_data.col_count < self.table_quality_rules.min_cols):
                    
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.TABLE_FORMAT,
                        f"í‘œ í¬ê¸°ê°€ ì‘ìŠµë‹ˆë‹¤ ({table_data.row_count}x{table_data.col_count})",
                        f"ìµœì†Œ {self.table_quality_rules.min_rows}x{self.table_quality_rules.min_cols} ê¶Œì¥",
                        0.7
                    )
                    issues.append(issue)
                
                # 2. í—¤ë” ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬
                if self.table_quality_rules.require_headers and not table_data.has_headers:
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.TABLE_FORMAT,
                        "í‘œì— í—¤ë”ê°€ ì—†ìŠµë‹ˆë‹¤",
                        "ëª…í™•í•œ ì—´ ì œëª© ì¶”ê°€ ê¶Œì¥",
                        0.8
                    )
                    issues.append(issue)
                
                # 3. ë¹ˆ ì…€ ë¹„ìœ¨ ê²€ì‚¬
                empty_cells = self._count_empty_cells(table_data)
                total_cells = table_data.row_count * table_data.col_count
                empty_ratio = empty_cells / total_cells if total_cells > 0 else 0
                
                if empty_ratio > self.table_quality_rules.max_empty_cells_ratio:
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.TABLE_FORMAT,
                        f"í‘œì— ë¹ˆ ì…€ì´ ë§ìŠµë‹ˆë‹¤ ({empty_ratio:.1%})",
                        "ë¶ˆí•„ìš”í•œ ë¹ˆ ì…€ ì œê±° ë˜ëŠ” ë°ì´í„° ë³´ì™„ ê¶Œì¥",
                        0.6
                    )
                    issues.append(issue)
                
                # 4. ì—´ ì¼ê´€ì„± ê²€ì‚¬
                if self.table_quality_rules.consistent_column_count:
                    inconsistent_rows = self._check_column_consistency(table_data)
                    if inconsistent_rows:
                        issue = self._create_multimodal_issue(
                            doc_meta, page, element,
                            IssueType.TABLE_FORMAT,
                            f"í‘œì˜ ì—´ ê°œìˆ˜ê°€ ì¼ê´€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ ({len(inconsistent_rows)}ê°œ í–‰)",
                            "ëª¨ë“  í–‰ì˜ ì—´ ê°œìˆ˜ í†µì¼ ê¶Œì¥",
                            0.9
                        )
                        issues.append(issue)
        
        logger.info(f"í‘œ í’ˆì§ˆ ê²€ì‚¬: {len(issues)}ê°œ ë°œê²¬")
        return issues
    
    async def _check_chart_quality(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """ì°¨íŠ¸ í’ˆì§ˆ ê²€ì‚¬"""
        issues = []
        
        for page in pages:
            for element in page.elements:
                if element.element_type != ElementType.CHART or not element.chart_data:
                    continue
                
                chart_data = element.chart_data
                
                # 1. ì œëª© ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬
                if self.chart_quality_rules.require_title and not chart_data.title:
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.CHART_READABILITY,
                        "ì°¨íŠ¸ì— ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤",
                        "ì°¨íŠ¸ ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” ì œëª© ì¶”ê°€ ê¶Œì¥",
                        0.8
                    )
                    issues.append(issue)
                
                # 2. ì¶• ë¼ë²¨ ê²€ì‚¬
                if (self.chart_quality_rules.require_axis_labels and 
                    (not chart_data.x_label or not chart_data.y_label)):
                    
                    missing_labels = []
                    if not chart_data.x_label:
                        missing_labels.append("Xì¶•")
                    if not chart_data.y_label:
                        missing_labels.append("Yì¶•")
                    
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.CHART_READABILITY,
                        f"ì°¨íŠ¸ì— {', '.join(missing_labels)} ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤",
                        "ì¶•ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ëŠ” ë¼ë²¨ ì¶”ê°€ ê¶Œì¥",
                        0.7
                    )
                    issues.append(issue)
                
                # 3. ë²”ë¡€ ê²€ì‚¬
                if (self.chart_quality_rules.require_legend and 
                    not chart_data.legend):
                    
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.CHART_READABILITY,
                        "ì°¨íŠ¸ì— ë²”ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤",
                        "ë°ì´í„° ì‹œë¦¬ì¦ˆ êµ¬ë¶„ì„ ìœ„í•œ ë²”ë¡€ ì¶”ê°€ ê¶Œì¥",
                        0.6
                    )
                    issues.append(issue)
                
                # 4. ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ ê²€ì‚¬
                if len(chart_data.data_points) < self.chart_quality_rules.min_data_points:
                    issue = self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.CHART_READABILITY,
                        f"ì°¨íŠ¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(chart_data.data_points)}ê°œ)",
                        f"ìµœì†Œ {self.chart_quality_rules.min_data_points}ê°œ ë°ì´í„° í¬ì¸íŠ¸ ê¶Œì¥",
                        0.7
                    )
                    issues.append(issue)
                
                # 5. Vision LLMì„ í†µí•œ ê°€ë…ì„± ë¶„ì„
                if self.enable_vision_analysis and self.vision_llm:
                    readability_issues = await self._analyze_chart_readability_with_vision(
                        doc_meta, page, element
                    )
                    issues.extend(readability_issues)
        
        logger.info(f"ì°¨íŠ¸ í’ˆì§ˆ ê²€ì‚¬: {len(issues)}ê°œ ë°œê²¬")
        return issues
    
    async def _check_multimodal_consistency(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """ë©€í‹°ëª¨ë‹¬ ìš”ì†Œê°„ ì¼ê´€ì„± ê²€ì‚¬ (OCR í…ìŠ¤íŠ¸ í¬í•¨)"""
        issues = []
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (raw_text + OCR í…ìŠ¤íŠ¸)
        all_texts = []
        
        for page in pages:
            # í˜ì´ì§€ í…ìŠ¤íŠ¸
            if page.raw_text.strip():
                all_texts.append(page.raw_text)
            
            # OCR í…ìŠ¤íŠ¸
            for element in page.elements:
                if (element.element_type == ElementType.IMAGE and 
                    element.image_data and 
                    element.image_data.ocr_text):
                    all_texts.append(element.image_data.ocr_text)
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¼ê´€ì„± ê²€ì‚¬
        full_text = "\n".join(all_texts)
        
        for rule in self.consistency_rules:
            found_terms = self._find_terms_in_text(full_text, rule.terms)
            
            if len(found_terms) > 1:
                # OCR í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ëœ ë¶ˆì¼ì¹˜ ìš©ì–´ì— ëŒ€í•œ ì´ìŠˆ ìƒì„±
                for term, positions in found_terms.items():
                    if term != rule.preferred:
                        # OCR í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ëœ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                        ocr_issue = self._find_ocr_inconsistency(
                            doc_meta, pages, term, rule
                        )
                        if ocr_issue:
                            issues.append(ocr_issue)
        
        logger.info(f"ë©€í‹°ëª¨ë‹¬ ì¼ê´€ì„± ê²€ì‚¬: {len(issues)}ê°œ ë°œê²¬")
        return issues
    
    def _create_multimodal_issue(
        self, 
        doc_meta: DocumentMeta, 
        page: PageInfo, 
        element: PageElement,
        issue_type: IssueType,
        message: str,
        suggestion: str,
        confidence: float
    ) -> Issue:
        """ë©€í‹°ëª¨ë‹¬ ì´ìŠˆ ìƒì„±"""
        
        issue_id = generate_issue_id(
            doc_meta.doc_id,
            page.page_id,
            element.bbox or BoundingBox(x=0, y=0, width=100, height=100),
            issue_type
        )
        
        return Issue(
            issue_id=issue_id,
            doc_id=doc_meta.doc_id,
            page_id=page.page_id,
            issue_type=issue_type,
            text_location=None,  # ë©€í‹°ëª¨ë‹¬ ìš”ì†ŒëŠ” bbox ì‚¬ìš©
            bbox_location=element.bbox,
            element_id=element.element_id,
            original_text=self._get_element_description(element),
            message=message,
            suggestion=suggestion,
            confidence=confidence,
            confidence_level="high",  # Pydanticì´ ìë™ ê³„ì‚°
            agent_name="multimodal_quality_agent"
        )
    
    def _get_element_description(self, element: PageElement) -> str:
        """ìš”ì†Œ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„±"""
        if element.element_type == ElementType.IMAGE and element.image_data:
            return element.image_data.description or f"ì´ë¯¸ì§€ ({element.image_data.format})"
        elif element.element_type == ElementType.TABLE and element.table_data:
            return f"í‘œ ({element.table_data.row_count}x{element.table_data.col_count})"
        elif element.element_type == ElementType.CHART and element.chart_data:
            return f"{element.chart_data.chart_type} ì°¨íŠ¸"
        else:
            return f"{element.element_type.value} ìš”ì†Œ"
    
    def _count_empty_cells(self, table_data: TableElement) -> int:
        """í‘œì˜ ë¹ˆ ì…€ ê°œìˆ˜ ê³„ì‚°"""
        empty_count = 0
        
        for row in table_data.rows:
            for cell in row:
                if not cell or cell.strip() == "":
                    empty_count += 1
        
        return empty_count
    
    def _check_column_consistency(self, table_data: TableElement) -> List[int]:
        """í‘œì˜ ì—´ ì¼ê´€ì„± ê²€ì‚¬"""
        inconsistent_rows = []
        expected_cols = len(table_data.headers) if table_data.headers else None
        
        for i, row in enumerate(table_data.rows):
            if expected_cols is None:
                expected_cols = len(row)
            elif len(row) != expected_cols:
                inconsistent_rows.append(i)
        
        return inconsistent_rows
    
    def _find_ocr_inconsistency(
        self, 
        doc_meta: DocumentMeta, 
        pages: List[PageInfo], 
        term: str, 
        rule: ConsistencyRule
    ) -> Optional[Issue]:
        """OCR í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ ë¶ˆì¼ì¹˜ ì°¾ê¸°"""
        
        for page in pages:
            for element in page.elements:
                if (element.element_type == ElementType.IMAGE and 
                    element.image_data and 
                    element.image_data.ocr_text and 
                    term.lower() in element.image_data.ocr_text.lower()):
                    
                    return self._create_multimodal_issue(
                        doc_meta, page, element,
                        IssueType.CONSISTENCY,
                        f"ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ ë¶ˆì¼ì¹˜: '{term}'",
                        f"'{rule.preferred}' ìš©ì–´ë¡œ í†µì¼ ê¶Œì¥",
                        0.8
                    )
        
        return None
    
    async def _analyze_image_quality_with_vision(
        self, 
        doc_meta: DocumentMeta, 
        page: PageInfo, 
        element: PageElement
    ) -> List[Issue]:
        """Vision LLMì„ í†µí•œ ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„"""
        if not self.vision_llm or not element.image_data or not element.image_data.image_data:
            return []
        
        try:
            prompt = """ì´ êµìœ¡ìš© ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê´€ì ì—ì„œ ë¬¸ì œì ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:
1. í…ìŠ¤íŠ¸ ê°€ë…ì„± (íë¦¿í•¨, ì‘ì€ ê¸€ì”¨)
2. ì´ë¯¸ì§€ ì„ ëª…ë„ (í”½ì…€í™”, ì••ì¶• í’ˆì§ˆ)
3. ìƒ‰ìƒ ëŒ€ë¹„ (êµ¬ë¶„í•˜ê¸° ì–´ë ¤ìš´ ìƒ‰ìƒ)
4. ì „ì²´ì ì¸ ì‹œê°ì  í’ˆì§ˆ

ë¬¸ì œê°€ ìˆìœ¼ë©´ "ë¬¸ì œë°œê²¬: [êµ¬ì²´ì  ë¬¸ì œ]"ë¡œ ì‹œì‘í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë¬¸ì œê°€ ì—†ìœ¼ë©´ "í’ˆì§ˆì–‘í˜¸"ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”."""

            # ì‹¤ì œ Vision API í˜¸ì¶œì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            response = await self._call_vision_api_for_quality(prompt, element.image_data.image_data)
            
            if response and "ë¬¸ì œë°œê²¬:" in response:
                issue = self._create_multimodal_issue(
                    doc_meta, page, element,
                    IssueType.IMAGE_QUALITY,
                    f"Vision ë¶„ì„: {response.split('ë¬¸ì œë°œê²¬:')[1].strip()}",
                    "ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  ê¶Œì¥",
                    0.7
                )
                return [issue]
        
        except Exception as e:
            logger.warning(f"Vision ê¸°ë°˜ ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        return []
    
    async def _analyze_chart_readability_with_vision(
        self, 
        doc_meta: DocumentMeta, 
        page: PageInfo, 
        element: PageElement
    ) -> List[Issue]:
        """Vision LLMì„ í†µí•œ ì°¨íŠ¸ ê°€ë…ì„± ë¶„ì„"""
        # ì°¨íŠ¸ëŠ” ë³´í†µ ì´ë¯¸ì§€ í˜•íƒœë¡œë„ ì¡´ì¬í•˜ë¯€ë¡œ, ê´€ë ¨ ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°
        if not self.vision_llm:
            return []
        
        # ì°¨íŠ¸ì™€ ê´€ë ¨ëœ ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸° (ê°™ì€ ìœ„ì¹˜ ë˜ëŠ” ìœ ì‚¬í•œ ì„¤ëª…)
        related_image = None
        for other_element in [e for e in element.__dict__.get('page_elements', [])]:
            if (other_element.element_type == ElementType.IMAGE and 
                other_element.image_data and 
                "ì°¨íŠ¸" in (other_element.image_data.description or "")):
                related_image = other_element
                break
        
        if not related_image or not related_image.image_data.image_data:
            return []
        
        try:
            prompt = """ì´ ì°¨íŠ¸ì˜ ê°€ë…ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê´€ì ì—ì„œ ë¬¸ì œì ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
1. í…ìŠ¤íŠ¸ í¬ê¸° (ë„ˆë¬´ ì‘ê±°ë‚˜ ì½ê¸° ì–´ë ¤ì›€)
2. ìƒ‰ìƒ êµ¬ë¶„ (ë¹„ìŠ·í•œ ìƒ‰ìƒìœ¼ë¡œ êµ¬ë¶„ ì–´ë ¤ì›€)
3. ì¶• ë¼ë²¨ê³¼ ì œëª©ì˜ ëª…í™•ì„±
4. ë²”ë¡€ì˜ ìœ„ì¹˜ì™€ ê°€ë…ì„±
5. ì „ì²´ì ì¸ ë ˆì´ì•„ì›ƒ

ë¬¸ì œê°€ ìˆìœ¼ë©´ "ê°€ë…ì„±ë¬¸ì œ: [êµ¬ì²´ì  ë¬¸ì œ]"ë¡œ ì‹œì‘í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ë¬¸ì œê°€ ì—†ìœ¼ë©´ "ê°€ë…ì„±ì–‘í˜¸"ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”."""

            response = await self._call_vision_api_for_quality(prompt, related_image.image_data.image_data)
            
            if response and "ê°€ë…ì„±ë¬¸ì œ:" in response:
                issue = self._create_multimodal_issue(
                    doc_meta, page, element,
                    IssueType.CHART_READABILITY,
                    f"Vision ë¶„ì„: {response.split('ê°€ë…ì„±ë¬¸ì œ:')[1].strip()}",
                    "ì°¨íŠ¸ ê°€ë…ì„± ê°œì„  ê¶Œì¥",
                    0.7
                )
                return [issue]
        
        except Exception as e:
            logger.warning(f"Vision ê¸°ë°˜ ì°¨íŠ¸ ê°€ë…ì„± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        
        return []
    
    async def _call_vision_api_for_quality(self, prompt: str, image_data: str) -> Optional[str]:
        """Vision API í˜¸ì¶œ (í’ˆì§ˆ ë¶„ì„ìš©)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” OpenAI Vision API í˜¸ì¶œ
        # í˜„ì¬ëŠ” ë”ë¯¸ ì‘ë‹µ
        await asyncio.sleep(0.1)
        return "í’ˆì§ˆì–‘í˜¸"  # ë˜ëŠ” "ë¬¸ì œë°œê²¬: í…ìŠ¤íŠ¸ê°€ íë¦¿í•¨"
    
    # ê¸°ì¡´ í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬ ë©”ì„œë“œë“¤ (ê·¸ëŒ€ë¡œ ìœ ì§€)
    async def _check_typos_pattern_based(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """íŒ¨í„´ ê¸°ë°˜ ì˜¤íƒˆì ê²€ì‚¬ - ë©€í‹°ëª¨ë‹¬ í…ìŠ¤íŠ¸ í¬í•¨"""
        issues = []
        
        for page in pages:
            # 1. í˜ì´ì§€ raw_text ê²€ì‚¬
            text_issues = await self._check_text_patterns(doc_meta, page, page.raw_text, page.raw_text)
            issues.extend(text_issues)
            
            # 2. OCR í…ìŠ¤íŠ¸ ê²€ì‚¬
            for element in page.elements:
                if (element.element_type == ElementType.IMAGE and 
                    element.image_data and 
                    element.image_data.ocr_text):
                    
                    ocr_issues = await self._check_text_patterns(
                        doc_meta, page, element.image_data.ocr_text, 
                        f"ì´ë¯¸ì§€ OCR: {element.image_data.ocr_text}"
                    )
                    # OCR ì´ìŠˆëŠ” element_id í¬í•¨
                    for issue in ocr_issues:
                        issue.element_id = element.element_id
                        issue.message = f"[OCR] {issue.message}"
                    issues.extend(ocr_issues)
        
        logger.info(f"íŒ¨í„´ ê¸°ë°˜ ì˜¤íƒˆì (ë©€í‹°ëª¨ë‹¬): {len(issues)}ê°œ ë°œê²¬")
        return issues
    
    async def _check_text_patterns(self, doc_meta: DocumentMeta, page: PageInfo, text: str, display_text: str) -> List[Issue]:
        """í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ê¸°ë°˜ ì˜¤íƒˆì ê²€ì‚¬"""
        issues = []
        
        if not text.strip():
            return issues
        
        for pattern in self.typo_patterns:
            matches = re.finditer(pattern.pattern, text, re.IGNORECASE)
            
            for match in matches:
                location = TextLocation(
                    start=match.start(),
                    end=match.end()
                )
                
                issue_id = generate_issue_id(
                    doc_meta.doc_id,
                    page.page_id,
                    location,
                    IssueType.TYPO
                )
                
                issue = Issue(
                    issue_id=issue_id,
                    doc_id=doc_meta.doc_id,
                    page_id=page.page_id,
                    issue_type=IssueType.TYPO,
                    text_location=location,
                    original_text=match.group(),
                    message=pattern.description,
                    suggestion=pattern.correction,
                    confidence=pattern.confidence,
                    confidence_level="high",
                    agent_name="multimodal_quality_agent"
                )
                
                issues.append(issue)
        
        return issues
    
    async def _check_consistency(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """ìš©ì–´ ì¼ê´€ì„± ê²€ì‚¬ - ë©€í‹°ëª¨ë‹¬ í…ìŠ¤íŠ¸ í¬í•¨"""
        issues = []
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (í˜ì´ì§€ í…ìŠ¤íŠ¸ + OCR í…ìŠ¤íŠ¸)
        all_texts = []
        for page in pages:
            all_texts.append(page.raw_text)
            
            # OCR í…ìŠ¤íŠ¸ë„ í¬í•¨
            for element in page.elements:
                if (element.element_type == ElementType.IMAGE and 
                    element.image_data and 
                    element.image_data.ocr_text):
                    all_texts.append(element.image_data.ocr_text)
        
        full_text = "\n".join(all_texts)
        
        for rule in self.consistency_rules:
            found_terms = self._find_terms_in_text(full_text, rule.terms)
            
            if len(found_terms) > 1:
                most_common = max(found_terms.items(), key=lambda x: len(x[1]))[0]
                
                for term, positions in found_terms.items():
                    if term != rule.preferred and term != most_common:
                        first_pos = positions[0]
                        page_info = self._find_page_by_position(pages, first_pos)
                        
                        if page_info:
                            page, relative_pos = page_info
                            
                            location = TextLocation(
                                start=relative_pos,
                                end=relative_pos + len(term)
                            )
                            
                            issue_id = generate_issue_id(
                                doc_meta.doc_id,
                                page.page_id,
                                location,
                                IssueType.CONSISTENCY
                            )
                            
                            issue = Issue(
                                issue_id=issue_id,
                                doc_id=doc_meta.doc_id,
                                page_id=page.page_id,
                                issue_type=IssueType.CONSISTENCY,
                                text_location=location,
                                original_text=term,
                                message=f"ìš©ì–´ ì¼ê´€ì„±: {rule.description}",
                                suggestion=f"'{rule.preferred}' ìš©ì–´ë¡œ í†µì¼ ê¶Œì¥",
                                confidence=0.85,
                                confidence_level="medium",
                                agent_name="multimodal_quality_agent"
                            )
                            
                            issues.append(issue)
        
        logger.info(f"ì¼ê´€ì„± ê²€ì‚¬ (ë©€í‹°ëª¨ë‹¬): {len(issues)}ê°œ ë°œê²¬")
        return issues
    
    def _find_terms_in_text(self, text: str, terms: List[str]) -> Dict[str, List[int]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ë“¤ì˜ ìœ„ì¹˜ ì°¾ê¸°"""
        found = {}
        text_lower = text.lower()
        
        for term in terms:
            positions = []
            term_lower = term.lower()
            start = 0
            
            while True:
                pos = text_lower.find(term_lower, start)
                if pos == -1:
                    break
                
                if self._is_word_boundary(text, pos, len(term)):
                    positions.append(pos)
                
                start = pos + 1
            
            if positions:
                found[term] = positions
        
        return found
    
    def _is_word_boundary(self, text: str, pos: int, length: int) -> bool:
        """ë‹¨ì–´ ê²½ê³„ì¸ì§€ í™•ì¸"""
        before = text[pos-1] if pos > 0 else ' '
        after = text[pos + length] if pos + length < len(text) else ' '
        
        return not (before.isalnum() or after.isalnum())
    
    def _find_page_by_position(self, pages: List[PageInfo], position: int) -> Optional[tuple[PageInfo, int]]:
        """ì „ì²´ í…ìŠ¤íŠ¸ ìœ„ì¹˜ì—ì„œ í˜ì´ì§€ì™€ ìƒëŒ€ ìœ„ì¹˜ ì°¾ê¸°"""
        current_pos = 0
        
        for page in pages:
            page_length = len(page.raw_text) + 1
            
            if current_pos <= position < current_pos + page_length:
                relative_pos = position - current_pos
                return page, relative_pos
            
            current_pos += page_length
        
        return None
    
    async def _check_grammar_llm(self, doc_meta: DocumentMeta, pages: List[PageInfo]) -> List[Issue]:
        """LLM ê¸°ë°˜ ë¬¸ë²• ê²€ì‚¬ - ë©€í‹°ëª¨ë‹¬ í…ìŠ¤íŠ¸ í¬í•¨"""
        if not self.llm:
            return []
        
        issues = []
        
        for page in pages:
            # 1. í˜ì´ì§€ í…ìŠ¤íŠ¸ ê²€ì‚¬
            if page.raw_text.strip() and len(page.raw_text) <= 1000:
                korean_text = self._extract_korean_text(page.raw_text)
                if len(korean_text) >= 20:
                    try:
                        page_issues = await self._check_grammar_for_text(
                            doc_meta, page, korean_text, "í˜ì´ì§€ í…ìŠ¤íŠ¸"
                        )
                        issues.extend(page_issues)
                    except Exception as e:
                        logger.warning(f"í˜ì´ì§€ ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}")
            
            # 2. OCR í…ìŠ¤íŠ¸ ê²€ì‚¬
            for element in page.elements:
                if (element.element_type == ElementType.IMAGE and 
                    element.image_data and 
                    element.image_data.ocr_text):
                    
                    korean_text = self._extract_korean_text(element.image_data.ocr_text)
                    if len(korean_text) >= 20:
                        try:
                            ocr_issues = await self._check_grammar_for_text(
                                doc_meta, page, korean_text, f"OCR í…ìŠ¤íŠ¸ ({element.element_id})"
                            )
                            # OCR ì´ìŠˆëŠ” element_id í¬í•¨
                            for issue in ocr_issues:
                                issue.element_id = element.element_id
                                issue.message = f"[OCR] {issue.message}"
                            issues.extend(ocr_issues)
                        except Exception as e:
                            logger.warning(f"OCR ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}")
        
        logger.info(f"LLM ë¬¸ë²• ê²€ì‚¬ (ë©€í‹°ëª¨ë‹¬): {len(issues)}ê°œ ë°œê²¬")
        return issues
    
    async def _check_grammar_for_text(self, doc_meta: DocumentMeta, page: PageInfo, text: str, source: str) -> List[Issue]:
        """íŠ¹ì • í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë¬¸ë²• ê²€ì‚¬"""
        try:
            prompt = self._create_grammar_check_prompt(text)
            response = await self.llm.acomplete(prompt)
            
            page_issues = self._parse_grammar_response(response.text, doc_meta, page)
            
            # API í˜¸ì¶œ ì œí•œ
            await asyncio.sleep(0.5)
            
            return page_issues
            
        except Exception as e:
            logger.warning(f"{source} ë¬¸ë²• ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _extract_korean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ í•œêµ­ì–´ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        sentences = re.split(r'[.!?]\s*', text)
        korean_sentences = []
        
        for sentence in sentences:
            if re.search(r'[ê°€-í£]', sentence) and len(sentence.strip()) > 5:
                korean_sentences.append(sentence.strip())
        
        return ' '.join(korean_sentences)
    
    def _create_grammar_check_prompt(self, text: str) -> str:
        """ë¬¸ë²• ê²€ì‚¬ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¤ìŒ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë¬¸ë²•ê³¼ ë§ì¶¤ë²•ì„ ê²€ì‚¬í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: {text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:
- ë¬¸ì œê°€ ìˆìœ¼ë©´: "ì˜¤ë¥˜ë°œê²¬: [ë¬¸ì œìˆëŠ”ë¶€ë¶„] -> [ìˆ˜ì •ì œì•ˆ]"
- ë¬¸ì œê°€ ì—†ìœ¼ë©´: "ë¬¸ì œì—†ìŒ"

ì—¬ëŸ¬ ì˜¤ë¥˜ê°€ ìˆë‹¤ë©´ ê°ê° í•œ ì¤„ì”© ì‘ì„±í•´ì£¼ì„¸ìš”."""
    
    def _parse_grammar_response(self, response: str, doc_meta: DocumentMeta, page: PageInfo) -> List[Issue]:
        """LLM ì‘ë‹µì—ì„œ ë¬¸ë²• ì´ìŠˆ íŒŒì‹±"""
        issues = []
        
        if "ë¬¸ì œì—†ìŒ" in response or "ì˜¤ë¥˜ë°œê²¬" not in response:
            return issues
        
        lines = response.strip().split('\n')
        
        for line in lines:
            if "ì˜¤ë¥˜ë°œê²¬:" in line:
                try:
                    parts = line.split("ì˜¤ë¥˜ë°œê²¬:")[1].strip()
                    if "->" in parts:
                        original, suggestion = parts.split("->", 1)
                        original = original.strip()
                        suggestion = suggestion.strip()
                        
                        pos = page.raw_text.find(original)
                        if pos != -1:
                            location = TextLocation(
                                start=pos,
                                end=pos + len(original)
                            )
                            
                            issue_id = generate_issue_id(
                                doc_meta.doc_id,
                                page.page_id,
                                location,
                                IssueType.GRAMMAR
                            )
                            
                            issue = Issue(
                                issue_id=issue_id,
                                doc_id=doc_meta.doc_id,
                                page_id=page.page_id,
                                issue_type=IssueType.GRAMMAR,
                                text_location=location,
                                original_text=original,
                                message="LLMì´ ê°ì§€í•œ ë¬¸ë²•/ë§ì¶¤ë²• ì˜¤ë¥˜",
                                suggestion=suggestion,
                                confidence=0.75,
                                confidence_level="medium",
                                agent_name="multimodal_quality_agent"
                            )
                            
                            issues.append(issue)
                
                except Exception as e:
                    logger.warning(f"ë¬¸ë²• ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        
        return issues


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_multimodal_quality_agent():
    """MultimodalQualityAgent í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª MultimodalQualityAgent í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    import os
    from dotenv import load_dotenv
    from pathlib import Path
    env_path = Path(__file__).resolve().parents[2] / '.env.dev'
    load_dotenv(env_path)
    api_key = os.getenv("OPENAI_API_KEY")
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = MultimodalQualityAgent(
        openai_api_key=api_key,
        enable_vision_analysis=bool(api_key)
    )
    
    # í…ŒìŠ¤íŠ¸ìš© ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ìƒì„±
    from src.core.models import (
        DocumentMeta, PageInfo, PageElement, ElementType,
        ImageElement, TableElement, BoundingBox, generate_doc_id
    )
    
    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
    doc_meta = DocumentMeta(
        doc_id=generate_doc_id("test_multimodal.pdf"),
        title="ë©€í‹°ëª¨ë‹¬ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ",
        doc_type="pdf",
        total_pages=1,
        file_path="test_multimodal.pdf"
    )
    
    # í…ŒìŠ¤íŠ¸ í˜ì´ì§€ ìƒì„±
    test_elements = []
    
    # 1. í…ìŠ¤íŠ¸ ìš”ì†Œ (ì˜¤íƒˆì í¬í•¨)
    text_element = PageElement(
        element_id="p001_text_001",
        element_type=ElementType.TEXT,
        text_content="ë”¥ëŸ¬ë‹ê³¼ ì‹¬ì¸µí•™ìŠµì„ ì‚¬ìš©í•œ ì•Œê³ ë¦¬ë“¬ ì—°êµ¬ì…ë‹ˆë‹¤. ë°ì´íƒ€ ì „ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        confidence=1.0
    )
    test_elements.append(text_element)
    
    # 2. ì´ë¯¸ì§€ ìš”ì†Œ (OCR í…ìŠ¤íŠ¸ì— ì˜¤íƒˆì í¬í•¨)
    image_element = PageElement(
        element_id="p001_image_001",
        element_type=ElementType.IMAGE,
        bbox=BoundingBox(x=100, y=200, width=200, height=150),  # ì‘ì€ í¬ê¸°
        image_data=ImageElement(
            element_id="p001_image_001",
            bbox=BoundingBox(x=100, y=200, width=200, height=150),
            format="bmp",  # ê¶Œì¥í•˜ì§€ ì•ŠëŠ” í˜•ì‹
            size_bytes=8 * 1024 * 1024,  # 8MB (í° íŒŒì¼)
            dimensions=(200, 150),  # ì‘ì€ í•´ìƒë„
            ocr_text="ë¨¸ì‹ ëŸ¬ë‹ê³¼ ê¸°ê³„í•™ìŠµì€ ë™ì¼í•œ ê°œë…ì…ë‹ˆë‹¤",  # ìš©ì–´ ë¶ˆì¼ì¹˜
            description="ì‹ ê²½ë§ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨"
        ),
        confidence=0.8
    )
    test_elements.append(image_element)
    
    # 3. í‘œ ìš”ì†Œ (í’ˆì§ˆ ë¬¸ì œ í¬í•¨)
    table_element = PageElement(
        element_id="p001_table_001",
        element_type=ElementType.TABLE,
        bbox=BoundingBox(x=50, y=400, width=400, height=100),
        table_data=TableElement(
            element_id="p001_table_001",
            headers=[],  # í—¤ë” ì—†ìŒ
            rows=[
                ["", "0.95", "ë¹ ë¦„"],  # ë¹ˆ ì…€ í¬í•¨
                ["Random Forest", "", "ì¤‘ê°„"],  # ë¹ˆ ì…€ í¬í•¨
                ["Neural Network", "0.97"]  # ì—´ ê°œìˆ˜ ë¶ˆì¼ì¹˜
            ]
        ),
        confidence=0.9
    )
    test_elements.append(table_element)
    
    # 4. ì°¨íŠ¸ ìš”ì†Œ (ê°€ë…ì„± ë¬¸ì œ í¬í•¨)
    chart_element = PageElement(
        element_id="p001_chart_001",
        element_type=ElementType.CHART,
        bbox=BoundingBox(x=50, y=550, width=400, height=200),
        chart_data=ChartElement(
            element_id="p001_chart_001",
            chart_type="bar",
            title="",  # ì œëª© ì—†ìŒ
            x_label="",  # Xì¶• ë¼ë²¨ ì—†ìŒ
            y_label="",  # Yì¶• ë¼ë²¨ ì—†ìŒ
            data_points=[{"x": 1, "y": 0.95}],  # ë°ì´í„° í¬ì¸íŠ¸ ë¶€ì¡±
            legend=[]  # ë²”ë¡€ ì—†ìŒ
        ),
        confidence=0.7
    )
    test_elements.append(chart_element)
    
    # í˜ì´ì§€ ìƒì„±
    test_page = PageInfo(
        page_id="p001",
        page_number=1,
        raw_text="ë”¥ëŸ¬ë‹ê³¼ ì‹¬ì¸µí•™ìŠµì„ ì‚¬ìš©í•œ ì•Œê³ ë¦¬ë“¬ ì—°êµ¬ì…ë‹ˆë‹¤. ë°ì´íƒ€ ì „ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        word_count=12,
        elements=test_elements
    )
    
    # ë©€í‹°ëª¨ë‹¬ í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰
    print("\nğŸ” ë©€í‹°ëª¨ë‹¬ í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰ ì¤‘...")
    issues = await agent.check_document(doc_meta, [test_page])
    
    print(f"\nğŸ“‹ ë°œê²¬ëœ ì´ìŠˆë“¤ ({len(issues)}ê°œ):")
    
    # ì´ìŠˆ íƒ€ì…ë³„ ë¶„ë¥˜
    issue_by_type = {}
    for issue in issues:
        issue_type = issue.issue_type.value
        if issue_type not in issue_by_type:
            issue_by_type[issue_type] = []
        issue_by_type[issue_type].append(issue)
    
    for issue_type, type_issues in issue_by_type.items():
        print(f"\nğŸ“Œ {issue_type.upper()} ({len(type_issues)}ê°œ)")
        
        for issue in type_issues:
            print(f"   ğŸ” {issue.original_text[:30]}...")
            print(f"      ë©”ì‹œì§€: {issue.message}")
            print(f"      ì œì•ˆ: {issue.suggestion}")
            print(f"      ì‹ ë¢°ë„: {issue.confidence:.2f}")
            if issue.element_id:
                print(f"      ìš”ì†Œ ID: {issue.element_id}")
            print(f"      ìœ„ì¹˜: {issue.page_id}")
    
    print(f"\nğŸ“Š ì´ìŠˆ ìš”ì•½:")
    print(f"   í…ìŠ¤íŠ¸ ì´ìŠˆ: {len(issue_by_type.get('typo', []) + issue_by_type.get('consistency', []) + issue_by_type.get('grammar', []))}ê°œ")
    print(f"   ì´ë¯¸ì§€ í’ˆì§ˆ: {len(issue_by_type.get('image_quality', []))}ê°œ")
    print(f"   í‘œ í˜•ì‹: {len(issue_by_type.get('table_format', []))}ê°œ")
    print(f"   ì°¨íŠ¸ ê°€ë…ì„±: {len(issue_by_type.get('chart_readability', []))}ê°œ")
    
    print("\nğŸ‰ MultimodalQualityAgent í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(test_multimodal_quality_agent())