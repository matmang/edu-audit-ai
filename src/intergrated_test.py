"""
EDU-Audit ì‹¤ì œ íŒŒì¼ ê¸°ë°˜ í…ŒìŠ¤íŠ¸
sample_docs/Ch01_intro.pdfë¥¼ ì‚¬ìš©í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
"""

import asyncio
import logging
import os
import sys
import traceback
from pathlib import Path
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
from pathlib import Path
env_path = Path(__file__).resolve().parents[1] / '.env.dev'
load_dotenv(env_path)

class EDUAuditRealFileTest:
    """ì‹¤ì œ íŒŒì¼ì„ ì‚¬ìš©í•œ EDU-Audit ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.test_file = Path("sample_docs/Ch01_intro.pdf")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_API_KEY")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.test_results = {
            "environment": {},
            "agents": {},
            "processing": {},
            "errors": []
        }
    
    def check_prerequisites(self) -> bool:
        """ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        print("=== ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸ ===")
        
        # 1. íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not self.test_file.exists():
            print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.test_file}")
            print("   sample_docs í´ë”ì— Ch01_intro.pdf íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”")
            return False
        else:
            file_size = self.test_file.stat().st_size / (1024*1024)  # MB
            print(f"âœ… í…ŒìŠ¤íŠ¸ íŒŒì¼ í™•ì¸: {self.test_file} ({file_size:.1f}MB)")
            self.test_results["environment"]["file_path"] = str(self.test_file)
            self.test_results["environment"]["file_size_mb"] = file_size
        
        # 2. API í‚¤ í™•ì¸
        if not self.openai_api_key:
            print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        else:
            print(f"âœ… OpenAI API í‚¤ í™•ì¸: {self.openai_api_key[:8]}...")
            self.test_results["environment"]["has_openai_key"] = True
        
        if self.serpapi_key:
            print(f"âœ… SerpAPI í‚¤ í™•ì¸: {self.serpapi_key[:8]}...")
            self.test_results["environment"]["has_serpapi_key"] = True
        else:
            print("âš ï¸  SerpAPI í‚¤ ì—†ìŒ (ì‚¬ì‹¤ê²€ì¦ ì œí•œ)")
            self.test_results["environment"]["has_serpapi_key"] = False
        
        # 3. í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
        required_packages = [
            "pdfplumber", "llama_index", "openai", "pydantic", "aiohttp"
        ]
        
        missing_packages = []
        for package in required_packages:
            try:
                __import__(package.replace("-", "_"))
                print(f"âœ… {package}")
            except ImportError:
                print(f"âŒ {package} íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤")
                missing_packages.append(package)
        
        if missing_packages:
            print(f"ì„¤ì¹˜ ëª…ë ¹: pip install {' '.join(missing_packages)}")
            return False
        
        self.test_results["environment"]["packages_ok"] = True
        return True
    
    async def test_document_agent(self):
        """DocumentAgent í…ŒìŠ¤íŠ¸"""
        print("\n=== Document Agent í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œë„ (ê°„ë‹¨í•œ ë²„ì „)
            from src.core.models import DocumentMeta, PageInfo, generate_doc_id
            import pdfplumber
            
            print("ğŸ“„ PDF íŒŒì¼ ê¸°ë³¸ íŒŒì‹± í…ŒìŠ¤íŠ¸...")
            
            doc_id = generate_doc_id(str(self.test_file))
            pages_data = []
            
            # ê¸°ë³¸ PDF íŒŒì‹± (ë©€í‹°ëª¨ë‹¬ ì œì™¸)
            with pdfplumber.open(self.test_file) as pdf:
                print(f"   ì´ í˜ì´ì§€: {len(pdf.pages)}")
                
                for page_num, page in enumerate(pdf.pages[:3], 1):  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ
                    text = page.extract_text() or ""
                    word_count = len(text.split()) if text else 0
                    
                    page_info = PageInfo(
                        page_id=f"p{page_num:03d}",
                        page_number=page_num,
                        raw_text=text,
                        word_count=word_count,
                        elements=[]  # ì¼ë‹¨ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    )
                    
                    pages_data.append(page_info)
                    print(f"   í˜ì´ì§€ {page_num}: {word_count} ë‹¨ì–´")
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            doc_meta = DocumentMeta(
                doc_id=doc_id,
                title="Ch01_intro",
                doc_type="pdf",
                total_pages=len(pages_data),
                file_path=str(self.test_file)
            )
            
            # ê²°ê³¼ ì €ì¥
            self.test_results["agents"]["document_agent"] = {
                "status": "success",
                "doc_id": doc_id,
                "pages_parsed": len(pages_data),
                "total_words": sum(p.word_count for p in pages_data),
                "sample_text": pages_data[0].raw_text[:200] + "..." if pages_data else ""
            }
            
            print("âœ… Document Agent ê¸°ë³¸ íŒŒì‹± ì„±ê³µ")
            return doc_meta, pages_data
            
        except Exception as e:
            error_msg = f"Document Agent ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            self.test_results["errors"].append(error_msg)
            print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return None, []
    
    async def test_quality_agent(self, doc_meta, pages_data):
        """QualityAgent í…ŒìŠ¤íŠ¸"""
        print("\n=== Quality Agent í…ŒìŠ¤íŠ¸ ===")
        
        if not doc_meta or not pages_data:
            print("âŒ Document Agent ê²°ê³¼ê°€ ì—†ì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤")
            return []
        
        try:
            # ê°„ë‹¨í•œ ì˜¤íƒˆì íŒ¨í„´ ê²€ì‚¬ (LLM ì—†ì´)
            typo_patterns = [
                {"pattern": r"ì•Œê³ ë¦¬ë“¬", "correction": "ì•Œê³ ë¦¬ì¦˜", "description": "ì•Œê³ ë¦¬ë“¬ â†’ ì•Œê³ ë¦¬ì¦˜"},
                {"pattern": r"ë°ì´íƒ€", "correction": "ë°ì´í„°", "description": "ë°ì´íƒ€ â†’ ë°ì´í„°"},
                {"pattern": r"ì»´í“¨íƒ€", "correction": "ì»´í“¨í„°", "description": "ì»´í“¨íƒ€ â†’ ì»´í“¨í„°"},
                {"pattern": r"ì•¨ê³ ë¦¬ì¦˜", "correction": "ì•Œê³ ë¦¬ì¦˜", "description": "ì•¨ê³ ë¦¬ì¦˜ â†’ ì•Œê³ ë¦¬ì¦˜"},
            ]
            
            import re
            from src.core.models import Issue, IssueType, TextLocation, generate_issue_id
            
            issues_found = []
            
            print("ğŸ” ì˜¤íƒˆì íŒ¨í„´ ê²€ì‚¬ ì¤‘...")
            
            for page in pages_data:
                text = page.raw_text
                if not text:
                    continue
                
                for pattern_info in typo_patterns:
                    pattern = pattern_info["pattern"]
                    matches = re.finditer(pattern, text, re.IGNORECASE)
                    
                    for match in matches:
                        location = TextLocation(start=match.start(), end=match.end())
                        
                        issue_id = generate_issue_id(
                            doc_meta.doc_id, page.page_id, location, IssueType.TYPO
                        )
                        
                        issue = Issue(
                            issue_id=issue_id,
                            doc_id=doc_meta.doc_id,
                            page_id=page.page_id,
                            issue_type=IssueType.TYPO,
                            text_location=location,
                            original_text=match.group(),
                            message=pattern_info["description"],
                            suggestion=pattern_info["correction"],
                            confidence=0.95,
                            confidence_level="high",
                            agent_name="quality_agent_simple"
                        )
                        
                        issues_found.append(issue)
                        print(f"   ë°œê²¬: '{match.group()}' â†’ '{pattern_info['correction']}' (í˜ì´ì§€ {page.page_number})")
            
            # ê²°ê³¼ ì €ì¥
            self.test_results["agents"]["quality_agent"] = {
                "status": "success",
                "issues_found": len(issues_found),
                "issue_types": [issue.issue_type.value for issue in issues_found]
            }
            
            print(f"âœ… Quality Agent í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(issues_found)}ê°œ ì´ìŠˆ ë°œê²¬")
            return issues_found
            
        except Exception as e:
            error_msg = f"Quality Agent ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            self.test_results["errors"].append(error_msg)
            print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return []
    
    async def test_fact_check_agent(self, doc_meta, pages_data):
        """FactCheckAgent ê°„ë‹¨ í…ŒìŠ¤íŠ¸"""
        print("\n=== Fact Check Agent í…ŒìŠ¤íŠ¸ ===")
        
        if not doc_meta or not pages_data:
            print("âŒ Document Agent ê²°ê³¼ê°€ ì—†ì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤")
            return []
        
        try:
            # ì‚¬ì‹¤ ê²€ì¦ì´ í•„ìš”í•œ íŒ¨í„´ ì°¾ê¸° (ì‹¤ì œ ê²€ì¦ì€ skip)
            fact_patterns = [
                r"ì—°êµ¬ì— ë”°ë¥´ë©´",
                r"ìµœì‹  ì—°êµ¬",
                r"[0-9]{4}ë…„.*ì—°êµ¬",
                r"í†µê³„ì— ì˜í•˜ë©´",
                r"[0-9]+%"
            ]
            
            import re
            
            potential_claims = []
            
            print("ğŸ” ì‚¬ì‹¤ ê²€ì¦ ëŒ€ìƒ ë¬¸ì¥ íƒì§€ ì¤‘...")
            
            for page in pages_data:
                text = page.raw_text
                if not text:
                    continue
                
                sentences = re.split(r'[.!?]\s+', text)
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) < 10:
                        continue
                    
                    for pattern in fact_patterns:
                        if re.search(pattern, sentence, re.IGNORECASE):
                            potential_claims.append({
                                "page": page.page_number,
                                "sentence": sentence[:100] + "..." if len(sentence) > 100 else sentence,
                                "pattern": pattern
                            })
                            print(f"   í˜ì´ì§€ {page.page_number}: {sentence[:50]}...")
                            break
            
            # ê²°ê³¼ ì €ì¥
            self.test_results["agents"]["fact_check_agent"] = {
                "status": "success",
                "potential_claims": len(potential_claims),
                "note": "ì‹¤ì œ ê²€ì¦ì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ (API í˜¸ì¶œ ì œí•œ)"
            }
            
            print(f"âœ… Fact Check Agent í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(potential_claims)}ê°œ ê²€ì¦ ëŒ€ìƒ ë°œê²¬")
            return potential_claims
            
        except Exception as e:
            error_msg = f"Fact Check Agent ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            self.test_results["errors"].append(error_msg)
            print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            return []
    
    async def test_llm_connection(self):
        """OpenAI LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
        print("\n=== LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ===")
        
        try:
            from llama_index.llms.openai import OpenAI
            
            llm = OpenAI(
                model="gpt-3.5-turbo",
                temperature=0.1,
                api_key=self.openai_api_key
            )
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
            print("ğŸ”— OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. 'í…ŒìŠ¤íŠ¸ ì„±ê³µ'ì´ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”."
            
            response = await llm.acomplete(test_prompt)
            
            print(f"âœ… LLM ì—°ê²° ì„±ê³µ")
            print(f"   ì‘ë‹µ: {response.text[:100]}...")
            
            self.test_results["agents"]["llm_connection"] = {
                "status": "success",
                "model": "gpt-3.5-turbo",
                "response_preview": response.text[:50]
            }
            
            return True
            
        except Exception as e:
            error_msg = f"LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            self.test_results["errors"].append(error_msg)
            return False
    
    async def test_indexing(self, doc_meta, pages_data):
        """LlamaIndex ì¸ë±ì‹± í…ŒìŠ¤íŠ¸"""
        print("\n=== ì¸ë±ì‹± í…ŒìŠ¤íŠ¸ ===")
        
        if not doc_meta or not pages_data:
            print("âŒ Document Agent ê²°ê³¼ê°€ ì—†ì–´ì„œ ê±´ë„ˆëœë‹ˆë‹¤")
            return False
        
        try:
            from llama_index.core import Document, VectorStoreIndex
            from llama_index.embeddings.openai import OpenAIEmbedding
            
            print("ğŸ“Š ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            
            # Document ê°ì²´ë“¤ ìƒì„±
            documents = []
            for page in pages_data:
                if page.raw_text.strip():
                    doc = Document(
                        text=page.raw_text,
                        metadata={
                            "doc_id": doc_meta.doc_id,
                            "page_id": page.page_id,
                            "page_number": page.page_number
                        }
                    )
                    documents.append(doc)
            
            if not documents:
                print("âŒ ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            embeddings = OpenAIEmbedding(api_key=self.openai_api_key)
            
            # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
            index = VectorStoreIndex.from_documents(
                documents,
                embed_model=embeddings
            )
            
            # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            query_engine = index.as_query_engine(similarity_top_k=2)
            test_query = "ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            
            print("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘...")
            response = query_engine.query(test_query)
            
            print(f"âœ… ì¸ë±ì‹± ì„±ê³µ: {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹±")
            print(f"   ê²€ìƒ‰ ê²°ê³¼: {str(response)[:100]}...")
            
            self.test_results["agents"]["indexing"] = {
                "status": "success",
                "documents_indexed": len(documents),
                "search_test": str(response)[:100]
            }
            
            return True
            
        except Exception as e:
            error_msg = f"ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            self.test_results["errors"].append(error_msg)
            return False
    
    def generate_report(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“‹ EDU-Audit í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        print("="*60)
        
        # í™˜ê²½ ì„¤ì •
        print(f"\nğŸ”§ í™˜ê²½ ì„¤ì •:")
        env = self.test_results["environment"]
        print(f"   íŒŒì¼: {env.get('file_path', 'N/A')} ({env.get('file_size_mb', 0):.1f}MB)")
        print(f"   OpenAI API: {'âœ…' if env.get('has_openai_key') else 'âŒ'}")
        print(f"   SerpAPI: {'âœ…' if env.get('has_serpapi_key') else 'âŒ'}")
        print(f"   íŒ¨í‚¤ì§€: {'âœ…' if env.get('packages_ok') else 'âŒ'}")
        
        # ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        print(f"\nğŸ¤– ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        agents = self.test_results["agents"]
        
        for agent_name, result in agents.items():
            status = result.get("status", "unknown")
            emoji = "âœ…" if status == "success" else "âŒ"
            print(f"   {agent_name}: {emoji}")
            
            if agent_name == "document_agent" and status == "success":
                print(f"      - í˜ì´ì§€: {result['pages_parsed']}")
                print(f"      - ë‹¨ì–´ ìˆ˜: {result['total_words']:,}")
            
            elif agent_name == "quality_agent" and status == "success":
                print(f"      - ë°œê²¬ ì´ìŠˆ: {result['issues_found']}")
            
            elif agent_name == "fact_check_agent" and status == "success":
                print(f"      - ê²€ì¦ ëŒ€ìƒ: {result['potential_claims']}")
            
            elif agent_name == "indexing" and status == "success":
                print(f"      - ì¸ë±ìŠ¤ ë¬¸ì„œ: {result['documents_indexed']}")
        
        # ì˜¤ë¥˜ ëª©ë¡
        if self.test_results["errors"]:
            print(f"\nâŒ ë°œê²¬ëœ ì˜¤ë¥˜ë“¤:")
            for i, error in enumerate(self.test_results["errors"], 1):
                print(f"   {i}. {error}")
        
        # ê¶Œì¥ ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ ì‚¬í•­:")
        
        if not env.get('has_serpapi_key'):
            print("   - SerpAPI í‚¤ ì„¤ì •ìœ¼ë¡œ ì‚¬ì‹¤ê²€ì¦ ê¸°ëŠ¥ í™œì„±í™”")
        
        if self.test_results["errors"]:
            print("   - ì˜¤ë¥˜ í•´ê²° í›„ ì¬í…ŒìŠ¤íŠ¸ í•„ìš”")
        else:
            print("   - ê¸°ë³¸ ê¸°ëŠ¥ ì •ìƒ ì‘ë™, Streamlit ë°ëª¨ ê°œë°œ ê°€ëŠ¥")
        
        print("\n" + "="*60)
    
    async def run_full_test(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ EDU-Audit ì‹¤ì œ íŒŒì¼ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        start_time = datetime.now()
        
        # ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not self.check_prerequisites():
            print("\nâŒ ì‚¬ì „ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return
        
        # 1. Document Agent í…ŒìŠ¤íŠ¸
        doc_meta, pages_data = await self.test_document_agent()
        
        # 2. Quality Agent í…ŒìŠ¤íŠ¸
        quality_issues = await self.test_quality_agent(doc_meta, pages_data)
        
        # 3. LLM ì—°ê²° í…ŒìŠ¤íŠ¸
        llm_connected = await self.test_llm_connection()
        
        # 4. ì¸ë±ì‹± í…ŒìŠ¤íŠ¸ (LLM ì—°ê²°ì´ ì„±ê³µí•œ ê²½ìš°ë§Œ)
        if llm_connected:
            await self.test_indexing(doc_meta, pages_data)
        
        # 5. Fact Check Agent í…ŒìŠ¤íŠ¸
        fact_claims = await self.test_fact_check_agent(doc_meta, pages_data)
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        self.test_results["processing"] = {
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": processing_time
        }
        
        # ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±
        self.generate_report()
        
        print(f"\nâ±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        print(f"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: Streamlit ë°ëª¨ ê°œë°œ ì¤€ë¹„ ì™„ë£Œ")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = EDUAuditRealFileTest()
    await tester.run_full_test()


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")